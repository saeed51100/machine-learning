{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-02 (Sequence Models + Plot)\n",
    "\n",
    "What's new:\n",
    "\n",
    "1- Add ta-lib"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:26:41.587656Z",
     "start_time": "2025-06-05T16:26:39.112742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import talib"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 19:56:39.897663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749140799.912509   13215 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749140799.917031   13215 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-05 19:56:39.930202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:26:44.164909Z",
     "start_time": "2025-06-05T16:26:44.118680Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('../datasets/XAGUSD-H1-rates.csv', sep='\\t').dropna()",
   "id": "27be8804efc0b24c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:26:45.693058Z",
     "start_time": "2025-06-05T16:26:45.672320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering (TA-Lib indicators)\n",
    "def add_ta_features(df):\n",
    "    df['rsi'] = talib.RSI(df['<CLOSE>'], timeperiod=14)\n",
    "    df['macd'], df['MACD_signal'], df['MACD_hist'] = talib.MACD(df['<CLOSE>'])\n",
    "    df['ema_10'] = talib.EMA(df['<CLOSE>'], timeperiod=10)\n",
    "    df['ema_50'] = talib.EMA(df['<CLOSE>'], timeperiod=50)\n",
    "    df['atr'] = talib.ATR(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    df['adx'] = talib.ADX(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = add_ta_features(df)"
   ],
   "id": "d44ff48485f8bbc3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:26:47.600253Z",
     "start_time": "2025-06-05T16:26:47.586295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scaling Features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>', 'rsi', 'macd', 'ema_10', 'ema_50', 'atr', 'adx']])"
   ],
   "id": "3813efa7b7b96154",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:27:11.349473Z",
     "start_time": "2025-06-05T16:26:58.977580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare sequences\n",
    "def create_sequences(features, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features) - horizon):\n",
    "        X.append(features[i - window:i])\n",
    "        y.append(target[i:i + horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "WINDOW_SIZE = 30\n",
    "FORECAST_HORIZON = 10\n",
    "X, y = create_sequences(scaled, df['<CLOSE>'].values, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "# Build basic LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(FORECAST_HORIZON)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=2, verbose=1)"
   ],
   "id": "cb7841301ce30d5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749140819.351624   13215 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2413 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001B[1m   1/1375\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m34:52\u001B[0m 2s/step - loss: 375.2793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749140821.518801   13273 cuda_dnn.cc:529] Loaded cuDNN version 90800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1375/1375\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 4ms/step - loss: 129.8795\n",
      "Epoch 2/2\n",
      "\u001B[1m1375/1375\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 3ms/step - loss: 3.2112\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:45:31.217758Z",
     "start_time": "2025-06-05T16:45:30.985752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load the last 30 rows from a separate CSV file for prediction ===\n",
    "input_df = pd.read_csv('../datasets/new-data-for-test/rows-30-from-20240503/rows-30-from-20240503.csv', sep='\\t').dropna()\n",
    "input_df = add_ta_features(input_df)\n",
    "input_scaled = scaler.transform(input_df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>', 'rsi', 'macd', 'ema_10', 'ema_50', 'atr', 'adx']])\n",
    "input_sequence = np.expand_dims(input_scaled, axis=0)  # shape: (1, 30, 5)\n",
    "\n",
    "# === Predict the next 10 candles ===\n",
    "prediction = model.predict(input_sequence)"
   ],
   "id": "7ad768ead6620c73",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 11)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m input_df = pd.read_csv(\u001B[33m'\u001B[39m\u001B[33m../datasets/new-data-for-test/rows-30-from-20240503/rows-30-from-20240503.csv\u001B[39m\u001B[33m'\u001B[39m, sep=\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m'\u001B[39m).dropna()\n\u001B[32m      3\u001B[39m input_df = add_ta_features(input_df)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m input_scaled = \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m<OPEN>\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m<HIGH>\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m<LOW>\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m<CLOSE>\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m<TICKVOL>\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrsi\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmacd\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mema_10\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mema_50\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43matr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43madx\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m input_sequence = np.expand_dims(input_scaled, axis=\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# shape: (1, 30, 5)\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# === Predict the next 10 candles ===\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:534\u001B[39m, in \u001B[36mMinMaxScaler.transform\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    530\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    532\u001B[39m xp, _ = get_namespace(X)\n\u001B[32m--> \u001B[39m\u001B[32m534\u001B[39m X = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    536\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_array_api\u001B[49m\u001B[43m.\u001B[49m\u001B[43msupported_float_dtypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    539\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    540\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    541\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    543\u001B[39m X *= \u001B[38;5;28mself\u001B[39m.scale_\n\u001B[32m    544\u001B[39m X += \u001B[38;5;28mself\u001B[39m.min_\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/sklearn/base.py:633\u001B[39m, in \u001B[36mBaseEstimator._validate_data\u001B[39m\u001B[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[39m\n\u001B[32m    631\u001B[39m         out = X, y\n\u001B[32m    632\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m--> \u001B[39m\u001B[32m633\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m    635\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/sklearn/utils/validation.py:1087\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1085\u001B[39m     n_samples = _num_samples(array)\n\u001B[32m   1086\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_samples < ensure_min_samples:\n\u001B[32m-> \u001B[39m\u001B[32m1087\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1088\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m) while a\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1089\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1090\u001B[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001B[32m   1091\u001B[39m         )\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array.ndim == \u001B[32m2\u001B[39m:\n\u001B[32m   1094\u001B[39m     n_features = array.shape[\u001B[32m1\u001B[39m]\n",
      "\u001B[31mValueError\u001B[39m: Found array with 0 sample(s) (shape=(0, 11)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "6456b78cc1b88128"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:42:28.155287Z",
     "start_time": "2025-06-05T16:42:28.108096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "\n",
    "# PARAMETERS\n",
    "csv1_path = '../datasets/new-data-for-test/rows-30-from-20240503/latest-30-for-history.csv'\n",
    "csv3_path = '../datasets/new-data-for-test/rows-30-from-20240503/after.csv'\n",
    "plot_title = 'Actual vs Predicted Forex Closing Prices'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "\n",
    "# LOAD DATA FROM PREDICTION\n",
    "\n",
    "# Combine <DATE> and <TIME> columns into a datetime\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=len(prediction[0]), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "predicted_df = pd.DataFrame({'DATETIME': datetime_index, '<CLOSE>': prediction[0]})"
   ],
   "id": "d4d7751243eff18e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13215/563404433.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T16:42:38.659479Z",
     "start_time": "2025-06-05T16:42:38.538110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PLOT\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "a6e7b86736ad5b51",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DATETIME'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'DATETIME'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# PLOT\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mforex_plot_utils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mplot_all_series\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhistorical_df\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhistorical_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpredicted_df\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpredicted_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mactual_future_df\u001B[49m\u001B[43m=\u001B[49m\u001B[43mactual_future_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m=\u001B[49m\u001B[43mplot_title\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_plot_path\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/architecture-02/../utils/forex_plot_utils.py:39\u001B[39m, in \u001B[36mplot_all_series\u001B[39m\u001B[34m(historical_df, predicted_df, actual_future_df, title, output_path)\u001B[39m\n\u001B[32m     36\u001B[39m plotted = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m historical_df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     plt.plot(\u001B[43mhistorical_df\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDATETIME\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m, historical_df[\u001B[33m'\u001B[39m\u001B[33m<CLOSE>\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     40\u001B[39m              label=\u001B[33m'\u001B[39m\u001B[33mHistorical\u001B[39m\u001B[33m'\u001B[39m, color=colors[\u001B[33m'\u001B[39m\u001B[33mhistorical\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     41\u001B[39m     plotted = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m actual_future_df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/pandas/core/frame.py:4107\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4106\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4107\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4108\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4109\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n\u001B[32m   3818\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3819\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3820\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3821\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3822\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3823\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3824\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'DATETIME'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# === Save training loss plot ===\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# === Save model summary and final performance ===\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
