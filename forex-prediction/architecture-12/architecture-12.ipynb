{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-12 ( Basic Regression Model Copy From 05 )\n",
    "\n",
    "What's new:\n",
    "\n",
    "1- Add keras tuner"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('datasets-12/XAGUSD-H1-rates.csv', sep='\\t').dropna()",
   "id": "27be8804efc0b24c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:50:08.393854Z",
     "start_time": "2025-07-10T09:50:08.383409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Scaling Features ===\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(\n",
    "    df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']])"
   ],
   "id": "3813efa7b7b96154",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:50:10.054735Z",
     "start_time": "2025-07-10T09:50:09.929130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Prepare sequences ===\n",
    "def create_sequences(features, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features) - horizon):\n",
    "        X.append(features[i - window:i])\n",
    "        y.append(target[i:i + horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "\n",
    "\n",
    "close_scaler = MinMaxScaler()\n",
    "scaled_close = close_scaler.fit_transform(df[['<CLOSE>']])\n",
    "X, y = create_sequences(scaled, scaled_close, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n"
   ],
   "id": "cb7841301ce30d5b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:50:18.445509Z",
     "start_time": "2025-07-10T09:50:18.309413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Define the HyperModel Function ===\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Tune LSTM units\n",
    "        units = hp.Int('lstm_units', min_value=16, max_value=128, step=16)\n",
    "        model.add(LSTM(units, input_shape=self.input_shape))\n",
    "\n",
    "        # Optional extra dense layer\n",
    "        if hp.Boolean('use_dense'):\n",
    "            model.add(Dense(hp.Int('dense_units', 16, 128, step=16), activation='relu'))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(self.output_dim))\n",
    "\n",
    "        # Tune optimizer learning rate\n",
    "        lr = hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model"
   ],
   "id": "9a9fa54206fb95a7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:50:23.989530Z",
     "start_time": "2025-07-10T09:50:22.861336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Instantiate the Tuner ===\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    LSTMHyperModel(input_shape=(X_train.shape[1], X_train.shape[2]), output_dim=FORECAST_HORIZON),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='forex_lstm_tuning'\n",
    ")\n"
   ],
   "id": "10660f8355f692b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752141023.142930    8677 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2365 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# === Search the Best Hyperparameters ===\n",
    "es_tuner = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "mc_tuner = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',  # for val_loss -- max for val_accuracy auto for...\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es_tuner, mc_tuner],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "e6b6414bc8fb9009",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 Complete [00h 04m 19s]\n",
      "val_loss: 0.00013320977450348437\n",
      "\n",
      "Best val_loss So Far: 0.00013165002746973187\n",
      "Total elapsed time: 00h 45m 52s\n",
      "\n",
      "Search: Running Trial #15\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "80                |48                |lstm_units\n",
      "False             |False             |use_dense\n",
      "0.0014808         |0.0023197         |lr\n",
      "32                |32                |dense_units\n",
      "\n",
      "Epoch 1/50\n",
      "\u001B[1m1093/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0041 - mae: 0.0215\n",
      "Epoch 1: val_loss improved from inf to 0.00095, saving model to best_model.keras\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 7ms/step - loss: 0.0041 - mae: 0.0215 - val_loss: 9.5471e-04 - val_mae: 0.0278\n",
      "Epoch 2/50\n",
      "\u001B[1m1098/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.1520e-04 - mae: 0.0069\n",
      "Epoch 2: val_loss improved from 0.00095 to 0.00024, saving model to best_model.keras\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 7ms/step - loss: 1.1520e-04 - mae: 0.0069 - val_loss: 2.4313e-04 - val_mae: 0.0118\n",
      "Epoch 3/50\n",
      "\u001B[1m1098/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.0560e-04 - mae: 0.0065\n",
      "Epoch 3: val_loss improved from 0.00024 to 0.00020, saving model to best_model.keras\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 7ms/step - loss: 1.0559e-04 - mae: 0.0065 - val_loss: 1.9748e-04 - val_mae: 0.0102\n",
      "Epoch 4/50\n",
      "\u001B[1m1097/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 9.3335e-05 - mae: 0.0061\n",
      "Epoch 4: val_loss did not improve from 0.00020\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 6ms/step - loss: 9.3341e-05 - mae: 0.0061 - val_loss: 2.2302e-04 - val_mae: 0.0117\n",
      "Epoch 5/50\n",
      "\u001B[1m1093/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 9.8599e-05 - mae: 0.0063\n",
      "Epoch 5: val_loss did not improve from 0.00020\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 6ms/step - loss: 9.8571e-05 - mae: 0.0063 - val_loss: 3.5880e-04 - val_mae: 0.0153\n",
      "Epoch 6/50\n",
      "\u001B[1m1097/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 8.9106e-05 - mae: 0.0060\n",
      "Epoch 6: val_loss improved from 0.00020 to 0.00017, saving model to best_model.keras\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 6ms/step - loss: 8.9113e-05 - mae: 0.0060 - val_loss: 1.7222e-04 - val_mae: 0.0098\n",
      "Epoch 7/50\n",
      "\u001B[1m1095/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 8.2286e-05 - mae: 0.0057\n",
      "Epoch 7: val_loss did not improve from 0.00017\n",
      "\u001B[1m1101/1101\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 6ms/step - loss: 8.2305e-05 - mae: 0.0057 - val_loss: 2.1245e-04 - val_mae: 0.0116\n",
      "Epoch 8/50\n",
      "\u001B[1m 125/1101\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m4s\u001B[0m 5ms/step - loss: 7.9794e-05 - mae: 0.0057"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Retrieve and Train the Best Model ===\n",
    "es_final = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "mc_final = callbacks.ModelCheckpoint(filepath='best_final_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Optionally fine-tune more\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es_final, mc_final],  # or define separately callbacks\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "1506eb33c1bf0415",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load the last 60 rows from a separate CSV file for prediction ===\n",
    "input_df = pd.read_csv('datasets-12/new-data-for-test/rows-60-from-20240503/rows-60-from-20240503.csv',\n",
    "                       sep='\\t').dropna()\n",
    "input_scaled = scaler.transform(\n",
    "    input_df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']])\n",
    "input_sequence = np.expand_dims(input_scaled, axis=0)  # shape: (1, 60, 5)\n",
    "\n",
    "# === Predict the next 10 candles + Inverse scale ===\n",
    "pred = model.predict(input_sequence)\n",
    "prediction = close_scaler.inverse_transform(pred)"
   ],
   "id": "7ad768ead6620c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "6456b78cc1b88128"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "\n",
    "# PARAMETERS\n",
    "csv1_path = 'datasets-12/new-data-for-test/rows-60-from-20240503/latest-4-for-history.csv'\n",
    "csv3_path = 'datasets-12/new-data-for-test/rows-60-from-20240503/after.csv'\n",
    "plot_title = 'Actual vs Predicted Forex Closing Prices'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "\n",
    "# LOAD DATA FROM PREDICTION\n",
    "\n",
    "# Combine <DATE> and <TIME> columns into a datetime\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=len(prediction[0]), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "predicted_df = pd.DataFrame({'DATETIME': datetime_index, '<CLOSE>': prediction[0]})"
   ],
   "id": "d4d7751243eff18e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PLOT\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "a6e7b86736ad5b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# === Save training loss plot ===\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# === Save model summary and final performance ===\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss, final_test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test MAE : {final_test_mae:.6f}\\n')\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
