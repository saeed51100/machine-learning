{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-18 ( Basic Classification ) v-2\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- Add\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:19.236144Z",
     "start_time": "2025-07-31T07:00:19.232860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,Input, Reshape\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:20.997237Z",
     "start_time": "2025-07-31T07:00:20.929213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "# Load and preprocess\n",
    "df = pd.read_csv('datasets-18/XAGUSD-H1-rates.csv', sep='\\t')\n",
    "\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# Optional: Combine DATE and TIME into a single datetime column\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Select features to scale\n",
    "features = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[features])"
   ],
   "id": "4c9a5a36c8be002e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:27.386029Z",
     "start_time": "2025-07-31T07:00:26.895833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2- Label trend reversals (example: a simplistic method)\n",
    "\n",
    "def label_trend_reversals(prices, window=5, threshold=0.0005):\n",
    "    prices = np.asarray(prices)\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(prices) - window):\n",
    "        past = prices[i:i + window // 2]\n",
    "        future = prices[i + window // 2:i + window]\n",
    "\n",
    "        past_mean = np.mean(past)\n",
    "        future_mean = np.mean(future)\n",
    "\n",
    "        if future_mean > past_mean * (1 + threshold):\n",
    "            labels.append(1)  # potential buy\n",
    "        elif future_mean < past_mean * (1 - threshold):\n",
    "            labels.append(2)  # potential sell\n",
    "        else:\n",
    "            labels.append(0)  # no signal\n",
    "\n",
    "    # Padding the end with zeros\n",
    "    labels += [0] * window\n",
    "    return labels\n",
    "\n",
    "# Apply labeling on cleaned DataFrame\n",
    "df['Label'] = label_trend_reversals(df['CLOSE'].values, window=10, threshold=0.001)\n"
   ],
   "id": "ce5f9c1ecf96cced",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:31.721610Z",
     "start_time": "2025-07-31T07:00:30.822424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3- Prepare Sequences\n",
    "\n",
    "SEQUENCE_LENGTH = 60\n",
    "FORECAST_HORIZON = 10\n",
    "X, y = [], []\n",
    "for i in range(SEQUENCE_LENGTH, len(scaled_data) - FORECAST_HORIZON):\n",
    "    X.append(scaled_data[i-SEQUENCE_LENGTH:i])\n",
    "    y.append(df['Label'].iloc[i:i+FORECAST_HORIZON].values)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")  # Debug info"
   ],
   "id": "6dfd6176c12faccf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (55013, 60, 5), y shape: (55013, 10)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:34.434565Z",
     "start_time": "2025-07-31T07:00:34.430551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4- Split dataset\n",
    "\n",
    "# Reserve only the last sample for testing\n",
    "X_train = X[:-1]\n",
    "y_train = y[:-1]  # Shape: (samples, 10)\n",
    "\n",
    "X_test = X[-1:]   # Shape: (1, 60, 5)\n",
    "y_test = y[-1:]   # Shape: (1, 10), still in integer form for now\n",
    "\n",
    "print(f\"Train X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Test X: {X_test.shape}, y: {y_test.shape}\")"
   ],
   "id": "9b787dab0031e409",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X: (55012, 60, 5), y: (55012, 10)\n",
      "Test X: (1, 60, 5), y: (1, 10)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:41.476823Z",
     "start_time": "2025-07-31T07:00:40.585667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5- One-hot encode each timestep's class (3 classes → depth = 3)\n",
    "\n",
    "# One-hot encode training labels\n",
    "y_train_onehot = np.array([to_categorical(row, num_classes=3) for row in y_train])\n",
    "\n",
    "# One-hot encode test labels (for model.evaluate or model.predict accuracy)\n",
    "y_test_onehot = np.array([to_categorical(row, num_classes=3) for row in y_test])\n",
    "\n",
    "# Debug shapes\n",
    "print(f\"y_train_onehot shape: {y_train_onehot.shape}\")  # (samples, 10, 3)\n",
    "print(f\"y_test_onehot shape: {y_test_onehot.shape}\")    # (1, 10, 3)\n"
   ],
   "id": "ba0012e61d11b0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_onehot shape: (55012, 10, 3)\n",
      "y_test_onehot shape: (1, 10, 3)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:00:46.014374Z",
     "start_time": "2025-07-31T07:00:44.524976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6- Build LSTM Classification Model\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(SEQUENCE_LENGTH, X.shape[2])),  # (60, 5)\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(FORECAST_HORIZON * 3, activation='softmax'),\n",
    "    Reshape((FORECAST_HORIZON, 3))  # Final output: (10, 3)\n",
    "])\n"
   ],
   "id": "cf91bf473436a97e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753945244.771437    8210 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2403 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "W0000 00:00:1753945244.983429    8287 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "W0000 00:00:1753945245.400329    8210 gpu_backend_lib.cc:617] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-31 10:30:45.400568: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:228] INTERNAL: Generating device code failed.\n",
      "2025-07-31 10:30:45.401147: W tensorflow/core/framework/op_kernel.cc:1829] UNKNOWN: JIT compilation failed.\n",
      "2025-07-31 10:30:45.401166: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnknownError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 6- Build LSTM Classification Model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model = \u001B[43mSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mInput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSEQUENCE_LENGTH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# (60, 5)\u001B[39;49;00m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mLSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_sequences\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mDropout\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrelu\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mDropout\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFORECAST_HORIZON\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msoftmax\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mReshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFORECAST_HORIZON\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Final output: (10, 3)\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/models/sequential.py:76\u001B[39m, in \u001B[36mSequential.__init__\u001B[39m\u001B[34m(self, layers, trainable, name)\u001B[39m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m layers:\n\u001B[32m     75\u001B[39m     \u001B[38;5;28mself\u001B[39m.add(layer, rebuild=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_rebuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/models/sequential.py:149\u001B[39m, in \u001B[36mSequential._maybe_rebuild\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    147\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m._layers[\u001B[32m0\u001B[39m], InputLayer) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._layers) > \u001B[32m1\u001B[39m:\n\u001B[32m    148\u001B[39m     input_shape = \u001B[38;5;28mself\u001B[39m._layers[\u001B[32m0\u001B[39m].batch_shape\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    150\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m._layers[\u001B[32m0\u001B[39m], \u001B[33m\"\u001B[39m\u001B[33minput_shape\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._layers) > \u001B[32m1\u001B[39m:\n\u001B[32m    151\u001B[39m     \u001B[38;5;66;03m# We can build the Sequential model if the first layer has the\u001B[39;00m\n\u001B[32m    152\u001B[39m     \u001B[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001B[39;00m\n\u001B[32m    153\u001B[39m     \u001B[38;5;66;03m# model.\u001B[39;00m\n\u001B[32m    154\u001B[39m     input_shape = \u001B[38;5;28mself\u001B[39m._layers[\u001B[32m0\u001B[39m].input_shape\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/layer.py:232\u001B[39m, in \u001B[36mLayer.__new__.<locals>.build_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    230\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m obj._open_name_scope():\n\u001B[32m    231\u001B[39m     obj._path = current_path()\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m     \u001B[43moriginal_build_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;66;03m# Record build config.\u001B[39;00m\n\u001B[32m    234\u001B[39m signature = inspect.signature(original_build_method)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/models/sequential.py:195\u001B[39m, in \u001B[36mSequential.build\u001B[39m\u001B[34m(self, input_shape)\u001B[39m\n\u001B[32m    193\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._layers[\u001B[32m1\u001B[39m:]:\n\u001B[32m    194\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m195\u001B[39m         x = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[32m    197\u001B[39m         \u001B[38;5;66;03m# Can happen if shape inference is not implemented.\u001B[39;00m\n\u001B[32m    198\u001B[39m         \u001B[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001B[39;00m\n\u001B[32m    199\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py:2106\u001B[39m, in \u001B[36msign\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m   2104\u001B[39m     x = tf.cast(x, \u001B[33m\"\u001B[39m\u001B[33mint32\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   2105\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m tf.cast(tf.sign(x), ori_dtype)\n\u001B[32m-> \u001B[39m\u001B[32m2106\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43msign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mUnknownError\u001B[39m: {{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7- Compile model\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "id": "7b500e9a255ca6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8- Fit model\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_onehot,\n",
    "    epochs=2,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "f189dd95c6660684",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- Load the last 60 candles from a CSV file\n",
    "\n",
    "input_df = pd.read_csv(\n",
    "    'datasets-18/new-data-for-test/rows-60-from-20240503/rows-60-from-20240503.csv',\n",
    "    sep='\\t'\n",
    ").dropna()\n",
    "\n",
    "# 9-1 Combine <DATE> and <TIME> into a datetime column\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "# 9-2 Prepare features and scale\n",
    "input_features = input_df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']]\n",
    "input_scaled = scaler.transform(input_features)  # Use the same scaler from training\n",
    "\n",
    "# 9-3 Reshape for model: (1, 60, num_features)\n",
    "input_sequence = np.expand_dims(input_scaled, axis=0)\n",
    "\n",
    "# 9-4 Predict class probabilities\n",
    "pred_probs = model.predict(input_sequence)  # e.g. shape: (1, 10 * 3)\n",
    "forecast_horizon = 10\n",
    "pred_classes = np.argmax(pred_probs.reshape(forecast_horizon, 3), axis=1)\n",
    "print(\"Predicted Classes:\", pred_classes)  # 0=no signal, 1=buy, 2=sell\n",
    "\n",
    "# 9-5 Generate future timestamps\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "forecast_datetimes = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1),\n",
    "                                   periods=forecast_horizon, freq='h')\n",
    "\n",
    "# 9-6 Create predicted_df for plotting\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': pred_classes\n",
    "})\n",
    "\n",
    "# 9-7 Convert class → label\n",
    "def class_to_label(c):\n",
    "    return 'buy' if c == 1 else 'sell' if c == 2 else None\n",
    "\n",
    "predicted_df['label'] = predicted_df['forecast_class'].apply(class_to_label)\n"
   ],
   "id": "b27582fcf9337d79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- Plot section\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 10-1 PARAMETERS\n",
    "csv1_path = 'datasets-18/new-data-for-test/rows-60-from-20240503/latest-4-for-history.csv'\n",
    "csv3_path = 'datasets-18/new-data-for-test/rows-60-from-20240503/after.csv'\n",
    "plot_title = 'Write a suitable title.'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# 10-2 LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "# 10-3 Generate forecast timestamps ===\n",
    "# Start 1 hour after the last actual candle\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "forecast_datetimes = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=forecast_horizon, freq='h')\n",
    "\n",
    "# 10-4 Create predicted_df with forecasted trend reversals\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': pred_classes\n",
    "})\n",
    "\n",
    "# 10-5 Optional: Add labels for plotting\n",
    "def class_to_label(c):\n",
    "    if c == 1:\n",
    "        return 'buy'\n",
    "    elif c == 2:\n",
    "        return 'sell'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "predicted_df['label'] = predicted_df['forecast_class'].apply(class_to_label)\n"
   ],
   "id": "cf739a571793a98b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10-6 PLOT\n",
    "\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "49ac94a3b618c89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# 11-4 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# 11-5 Save training loss plot\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-6 Save model summary and final performance\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss, final_test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test MAE : {final_test_mae:.6f}\\n')\n"
   ],
   "id": "c60adc7617c16b75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
