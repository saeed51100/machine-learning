{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-16 ( Basic Regression )\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- Add unsupervised learning\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T10:08:13.930511Z",
     "start_time": "2025-07-20T10:08:13.924346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T10:08:16.425736Z",
     "start_time": "2025-07-20T10:08:16.353426Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('datasets-16/XAGUSD-H1-rates.csv', sep='\\t').dropna()",
   "id": "27be8804efc0b24c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T10:08:18.442220Z",
     "start_time": "2025-07-20T10:08:18.421678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scaling Features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']])\n",
    "\n",
    "close_scaler = MinMaxScaler()\n",
    "scaled_close = close_scaler.fit_transform(df[['<CLOSE>']])"
   ],
   "id": "3813efa7b7b96154",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T10:09:28.564417Z",
     "start_time": "2025-07-20T10:08:19.711186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare sequences\n",
    "def create_sequences(features, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features) - horizon):\n",
    "        X.append(features[i - window:i])\n",
    "        y.append(target[i:i + horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "\n",
    "X, y = create_sequences(scaled, scaled_close, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# === Phase 1: Unsupervised Pretraining using LSTM Autoencoder ===\n",
    "def build_lstm_autoencoder(timesteps, features, latent_dim):\n",
    "    inputs = Input(shape=(timesteps, features))\n",
    "    encoded = LSTM(latent_dim)(inputs)\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = LSTM(features, return_sequences=True)(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    encoder = Model(inputs, encoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "latent_dim = 32\n",
    "autoencoder, encoder = build_lstm_autoencoder(WINDOW_SIZE, X_train.shape[2], latent_dim)\n",
    "\n",
    "print(\"Pretraining autoencoder...\")\n",
    "autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Phase 2: Supervised Fine-tuning ===\n",
    "# Create LSTM model and initialize LSTM weights from encoder\n",
    "def build_forecasting_model(input_shape, encoder_model, forecast_horizon):\n",
    "    forecasting_model = Sequential()\n",
    "\n",
    "    # Add pre-trained encoder LSTM\n",
    "    pretrained_lstm_layer = LSTM(latent_dim, input_shape=input_shape)\n",
    "    pretrained_lstm_layer.build(input_shape=(None, *input_shape))\n",
    "    pretrained_lstm_layer.set_weights(encoder_model.layers[1].get_weights())  # Transfer encoder LSTM weights\n",
    "\n",
    "    forecasting_model.add(pretrained_lstm_layer)\n",
    "    forecasting_model.add(Dense(forecast_horizon))\n",
    "\n",
    "    forecasting_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return forecasting_model\n",
    "\n",
    "model = build_forecasting_model((X_train.shape[1], X_train.shape[2]), encoder, FORECAST_HORIZON)\n",
    "\n",
    "# === Train forecasting model ===\n",
    "mc = callbacks.ModelCheckpoint(filepath='mcp_saved_model.keras', monitor='val_loss', save_best_only=True)\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Fine-tuning with supervised learning...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[mc, es],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "6b5cdbf96eb6aa41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining autoencoder...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:38:20.179891: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 42249600 exceeds 10% of free system memory.\n",
      "2025-07-20 13:38:20.230713: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 42249600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m551/551\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 40ms/step - loss: 0.0106 - val_loss: 0.0015\n",
      "Epoch 2/3\n",
      "\u001B[1m551/551\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 38ms/step - loss: 8.3615e-04 - val_loss: 6.1431e-04\n",
      "Epoch 3/3\n",
      "\u001B[1m551/551\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 39ms/step - loss: 6.1395e-04 - val_loss: 4.8417e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/repozitories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RNN.build() got an unexpected keyword argument 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 63\u001B[39m\n\u001B[32m     60\u001B[39m     forecasting_model.compile(optimizer=\u001B[33m'\u001B[39m\u001B[33madam\u001B[39m\u001B[33m'\u001B[39m, loss=\u001B[33m'\u001B[39m\u001B[33mmse\u001B[39m\u001B[33m'\u001B[39m, metrics=[\u001B[33m'\u001B[39m\u001B[33mmae\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     61\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forecasting_model\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m model = \u001B[43mbuild_forecasting_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mFORECAST_HORIZON\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[38;5;66;03m# === Train forecasting model ===\u001B[39;00m\n\u001B[32m     66\u001B[39m mc = callbacks.ModelCheckpoint(filepath=\u001B[33m'\u001B[39m\u001B[33mmcp_saved_model.keras\u001B[39m\u001B[33m'\u001B[39m, monitor=\u001B[33m'\u001B[39m\u001B[33mval_loss\u001B[39m\u001B[33m'\u001B[39m, save_best_only=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 54\u001B[39m, in \u001B[36mbuild_forecasting_model\u001B[39m\u001B[34m(input_shape, encoder_model, forecast_horizon)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;66;03m# Add pre-trained encoder LSTM\u001B[39;00m\n\u001B[32m     53\u001B[39m pretrained_lstm_layer = LSTM(latent_dim, input_shape=input_shape)\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m \u001B[43mpretrained_lstm_layer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m pretrained_lstm_layer.set_weights(encoder_model.layers[\u001B[32m1\u001B[39m].get_weights())  \u001B[38;5;66;03m# Transfer encoder LSTM weights\u001B[39;00m\n\u001B[32m     57\u001B[39m forecasting_model.add(pretrained_lstm_layer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repozitories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/layer.py:232\u001B[39m, in \u001B[36mLayer.__new__.<locals>.build_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    230\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m obj._open_name_scope():\n\u001B[32m    231\u001B[39m     obj._path = current_path()\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m     \u001B[43moriginal_build_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[38;5;66;03m# Record build config.\u001B[39;00m\n\u001B[32m    234\u001B[39m signature = inspect.signature(original_build_method)\n",
      "\u001B[31mTypeError\u001B[39m: RNN.build() got an unexpected keyword argument 'input_shape'"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load the last 60 rows from a separate CSV file for prediction ===\n",
    "input_df = pd.read_csv('datasets-16/new-data-for-test/rows-60-from-20240503/rows-60-from-20240503.csv',\n",
    "                       sep='\\t').dropna()\n",
    "input_scaled = scaler.transform(\n",
    "    input_df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']])\n",
    "input_sequence = np.expand_dims(input_scaled, axis=0)  # shape: (1, 60, 5)\n",
    "\n",
    "# === Predict the next 10 candles + Inverse scale ===\n",
    "pred = model.predict(input_sequence)\n",
    "prediction = close_scaler.inverse_transform(pred)"
   ],
   "id": "7ad768ead6620c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "6456b78cc1b88128"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "\n",
    "# PARAMETERS\n",
    "csv1_path = 'datasets-16/new-data-for-test/rows-60-from-20240503/latest-4-for-history.csv'\n",
    "csv3_path = 'datasets-16/new-data-for-test/rows-60-from-20240503/after.csv'\n",
    "plot_title = 'Actual vs Predicted Forex Closing Prices'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "\n",
    "# LOAD DATA FROM PREDICTION\n",
    "\n",
    "# Combine <DATE> and <TIME> columns into a datetime\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=len(prediction[0]), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "predicted_df = pd.DataFrame({'DATETIME': datetime_index, '<CLOSE>': prediction[0]})"
   ],
   "id": "d4d7751243eff18e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PLOT\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "a6e7b86736ad5b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# === Save training loss plot ===\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# === Save model summary and final performance ===\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss, final_test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test MAE : {final_test_mae:.6f}\\n')\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
