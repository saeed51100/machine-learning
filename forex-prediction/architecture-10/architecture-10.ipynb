{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-10 ( Basic Regression Wide And Deep Add TA-Lib )\n",
    "What's new:\n",
    "\n",
    "1- Add TA-Lib to wide section"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T03:22:04.894567Z",
     "start_time": "2025-06-29T03:22:04.885963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Concatenate, GlobalAveragePooling1D, LayerNormalization\n",
    "import talib"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T03:22:06.312576Z",
     "start_time": "2025-06-29T03:22:06.227342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load and preprocess ===\n",
    "df = pd.read_csv('datasets-10/XAGUSD-H1-rates.csv', sep='\\t')"
   ],
   "id": "57786783bc2c952d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T03:22:09.435756Z",
     "start_time": "2025-06-29T03:22:09.402163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_ta_features(df):\n",
    "    df['rsi'] = talib.RSI(df['<CLOSE>'], timeperiod=14)\n",
    "    df['macd'], _, df['MACD_hist'] = talib.MACD(df['<CLOSE>'])\n",
    "    df['ema_50'] = talib.EMA(df['<CLOSE>'], timeperiod=50)\n",
    "    df['atr'] = talib.ATR(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    df['adx'] = talib.ADX(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "df = add_ta_features(df)"
   ],
   "id": "5db6ae6495f0072c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T03:22:12.414041Z",
     "start_time": "2025-06-29T03:22:11.733722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale sequence features\n",
    "sequence_features = ['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>']\n",
    "wide_features = ['<SPREAD>', '<VOL>', 'adx', 'atr', 'ema_50', 'macd', 'MACD_hist', 'rsi']\n",
    "\n",
    "scaler_seq = MinMaxScaler()\n",
    "scaler_wide = MinMaxScaler()\n",
    "scaler_close = MinMaxScaler()\n",
    "\n",
    "scaled_seq = scaler_seq.fit_transform(df[sequence_features])\n",
    "scaled_wide = scaler_wide.fit_transform(df[wide_features])\n",
    "scaled_close = scaler_close.fit_transform(df[['<CLOSE>']])\n",
    "\n",
    "# === Create sequences ===\n",
    "def create_sequences(seq_data, wide_data, target_data, window, horizon):\n",
    "    X_seq, X_wide, y = [], [], []\n",
    "    for i in range(window, len(seq_data) - horizon):\n",
    "        X_seq.append(seq_data[i-window:i])\n",
    "        X_wide.append(wide_data[i-1])  # use last row of each window\n",
    "        y.append(target_data[i:i+horizon].flatten())\n",
    "    return np.array(X_seq), np.array(X_wide), np.array(y)\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "\n",
    "X_seq, X_wide, y = create_sequences(scaled_seq, scaled_wide, scaled_close, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "# === Train/Test split ===\n",
    "X_seq_train, X_seq_test, X_wide_train, X_wide_test, y_train, y_test = train_test_split(\n",
    "    X_seq, X_wide, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# === Build Wide & Deep model ===\n",
    "def build_wide_deep_model(seq_shape, wide_shape, horizon):\n",
    "    # Deep part\n",
    "    seq_input = Input(shape=seq_shape, name=\"sequence_input\")\n",
    "    x = LSTM(64, return_sequences=True)(seq_input)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Wide part\n",
    "    wide_input = Input(shape=wide_shape, name=\"wide_input\")\n",
    "\n",
    "    # Combine\n",
    "    combined = Concatenate()([x, wide_input])\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(horizon)(x)\n",
    "\n",
    "    model = Model(inputs=[seq_input, wide_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_wide_deep_model(\n",
    "    seq_shape=(X_seq_train.shape[1], X_seq_train.shape[2]),\n",
    "    wide_shape=(X_wide_train.shape[1],),\n",
    "    horizon=FORECAST_HORIZON\n",
    ")\n"
   ],
   "id": "d8b0cc8f13e7204",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T03:23:03.397590Z",
     "start_time": "2025-06-29T03:23:03.386629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "# Choose output path\n",
    "summary_path = 'saved_models/model_summary.csv'\n",
    "\n",
    "# Open CSV for writing\n",
    "with open(summary_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Header\n",
    "    writer.writerow(['Layer Name', 'Layer Type', 'Output Shape', 'Param #'])\n",
    "\n",
    "    # Iterate over model layers\n",
    "    for layer in model.layers:\n",
    "        layer_name = layer.name\n",
    "        layer_type = layer.__class__.__name__\n",
    "        output_shape = layer.output_shape if hasattr(layer, 'output_shape') else 'N/A'\n",
    "        param_count = layer.count_params()\n",
    "\n",
    "        writer.writerow([layer_name, layer_type, output_shape, param_count])\n",
    "\n",
    "print(f'Model summary saved to: {summary_path}')\n"
   ],
   "id": "38265bfaaf45d58b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary saved to: saved_models/model_summary.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Train ===\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_seq_train, X_wide_train],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "42abd6c2a65fca25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Load the last 110 rows from a separate CSV file for prediction ===\n",
    "input_df = pd.read_csv('datasets-10/new-data-for-test/rows-60-from-20240503/rows-110-from-20240503.csv', sep='\\t')\n",
    "\n",
    "input_df = add_ta_features(input_df)\n",
    "\n",
    "# Take only the last 60 usable rows after TA features\n",
    "input_df = input_df.tail(60)\n",
    "\n",
    "input_seq_scaled = scaler_seq.transform(input_df[sequence_features])\n",
    "input_wide_scaled = scaler_wide.transform(input_df[wide_features])\n",
    "\n",
    "input_seq = np.expand_dims(input_seq_scaled, axis=0)\n",
    "input_wide = np.expand_dims(input_wide_scaled[-1], axis=0)\n",
    "\n",
    "pred = model.predict([input_seq, input_wide])\n",
    "prediction = scaler_close.inverse_transform(pred)\n",
    "print(\"Next 10 predicted closing prices:\", prediction.flatten())"
   ],
   "id": "2254a37d9c9f3fc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "6456b78cc1b88128"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "\n",
    "# PARAMETERS\n",
    "csv1_path = 'datasets-10/new-data-for-test/rows-60-from-20240503/latest-4-for-history.csv'\n",
    "csv3_path = 'datasets-10/new-data-for-test/rows-60-from-20240503/after.csv'\n",
    "plot_title = 'Actual vs Predicted Forex Closing Prices'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "\n",
    "# LOAD DATA FROM PREDICTION\n",
    "\n",
    "# Combine <DATE> and <TIME> columns into a datetime\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=len(prediction[0]), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "predicted_df = pd.DataFrame({'DATETIME': datetime_index, '<CLOSE>': prediction[0]})"
   ],
   "id": "d4d7751243eff18e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PLOT\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "a6e7b86736ad5b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# === Save training loss plot ===\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# === Save model summary and final performance ===\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss, final_test_mae = model.evaluate([X_seq_test, X_wide_test], y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss : {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test MAE : {final_test_mae:.6f}\\n')\n",
    "\n",
    "\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8e2657f15fa4600",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
