{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-15\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- https://claude.ai/chat/ef207056-eb79-4bfc-a8d7-c1f4c5d1fe84\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, RepeatVector, Dropout, \\\n",
    "    BatchNormalization\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(\n",
    "        close,\n",
    "        high=None,\n",
    "        low=None,\n",
    "        smoothing_window=31,\n",
    "        polyorder=3,\n",
    "        base_prom_factor=0.02,\n",
    "        distance=3,\n",
    "        snap_window=5,\n",
    "        min_dev_pct=0.0015,  # 0.15% minimum leg size\n",
    "        min_dev_sigma=2.0,  # >= 2x local abs-return EMA\n",
    "        vol_window=100,  # EMA window for local volatility\n",
    "        verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Label reversal points with improved accuracy.\n",
    "\n",
    "    Returns labels array of length n where:\n",
    "    0 = none, 1 = valley, 2 = peak.\n",
    "\n",
    "    Tips:\n",
    "    - For best accuracy, pass high/low arrays from your OHLCV.\n",
    "      Example: label_reversal_points(df['CLOSE'], df['HIGH'], df['LOW'])\n",
    "    - Tune min_dev_pct / min_dev_sigma to be stricter or looser on swing size.\n",
    "    \"\"\"\n",
    "    close = np.asarray(close, dtype=float)\n",
    "    n = close.size\n",
    "    if n < 3:\n",
    "        return np.zeros(n, dtype=int)\n",
    "\n",
    "    # Interpolate NaNs if any\n",
    "    if np.isnan(close).any():\n",
    "        idx = np.arange(n)\n",
    "        good = ~np.isnan(close)\n",
    "        close = close.copy()\n",
    "        close[~good] = np.interp(idx[~good], idx[good], close[good])\n",
    "\n",
    "    # Helper: simple EMA for local abs-return volatility\n",
    "    def ema(x, span):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        alpha = 2.0 / (span + 1.0)\n",
    "        out = np.empty_like(x)\n",
    "        out[0] = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]\n",
    "        return out\n",
    "\n",
    "    # Local volatility in price terms via EMA of absolute returns\n",
    "    ret = np.zeros(n)\n",
    "    ret[1:] = np.abs(np.diff(close) / np.maximum(1e-12, close[:-1]))\n",
    "    vol_absret = ema(ret, vol_window)\n",
    "    local_vol_price = vol_absret * close  # convert to price units\n",
    "\n",
    "    # Smoothing to get robust candidates\n",
    "    win = smoothing_window\n",
    "    if win >= n:\n",
    "        win = n - 1 if (n - 1) % 2 == 1 else n - 2\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    smoothed = savgol_filter(close, win, polyorder)\n",
    "\n",
    "    # Base prominence threshold\n",
    "    global_std = np.std(close) or 1.0\n",
    "    prom = global_std * base_prom_factor\n",
    "\n",
    "    # Candidate peaks/valleys on smoothed\n",
    "    peak_idx, _ = find_peaks(smoothed, distance=distance, prominence=prom)\n",
    "    val_idx, _ = find_peaks(-smoothed, distance=distance, prominence=prom)\n",
    "\n",
    "    # Prominences for tie-breaking\n",
    "    peak_prom = peak_prominences(smoothed, peak_idx)[0] if peak_idx.size else np.array([])\n",
    "    val_prom = peak_prominences(-smoothed, val_idx)[0] if val_idx.size else np.array([])\n",
    "\n",
    "    # Combine\n",
    "    candidates = []\n",
    "    for i, p in enumerate(peak_idx):\n",
    "        candidates.append((int(p), 2, float(peak_prom[i]) if peak_prom.size else 0.0))\n",
    "    for i, v in enumerate(val_idx):\n",
    "        candidates.append((int(v), 1, float(val_prom[i]) if val_prom.size else 0.0))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    if not candidates:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "        # still mark edges for completeness\n",
    "        labels[0] = 1 if close[1] > close[0] else 2\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "        return labels\n",
    "\n",
    "    # Enforce alternation (remove weaker when two same-type neighbors)\n",
    "    def enforce_alternation(ext):\n",
    "        ext = ext[:]  # list of (idx, typ, prom)\n",
    "        while True:\n",
    "            removed = False\n",
    "            i = 0\n",
    "            while i < len(ext) - 1:\n",
    "                if ext[i][1] == ext[i + 1][1]:\n",
    "                    # drop the smaller prominence\n",
    "                    if ext[i][2] < ext[i + 1][2]:\n",
    "                        ext.pop(i)\n",
    "                    else:\n",
    "                        ext.pop(i + 1)\n",
    "                    removed = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            if not removed:\n",
    "                break\n",
    "        return ext\n",
    "\n",
    "    candidates = enforce_alternation(candidates)\n",
    "\n",
    "    # SNAP: move each extreme to the true local extremum on raw close (or HIGH/LOW)\n",
    "    def snap_index(idx, typ):\n",
    "        L = max(0, idx - snap_window)\n",
    "        R = min(n, idx + snap_window + 1)\n",
    "        if high is not None and low is not None:\n",
    "            if typ == 2:  # peak\n",
    "                j = np.argmax(np.asarray(high[L:R], dtype=float))\n",
    "            else:  # valley\n",
    "                j = np.argmin(np.asarray(low[L:R], dtype=float))\n",
    "        else:\n",
    "            if typ == 2:\n",
    "                j = np.argmax(close[L:R])\n",
    "            else:\n",
    "                j = np.argmin(close[L:R])\n",
    "        return L + int(j)\n",
    "\n",
    "    snapped = []\n",
    "    seen_at = {}  # avoid duplicate indices by keeping stronger prominence\n",
    "    for idx, typ, pr in candidates:\n",
    "        j = snap_index(idx, typ)\n",
    "        key = (j, typ)\n",
    "        if key not in seen_at or pr > seen_at[key][2]:\n",
    "            seen_at[key] = (j, typ, pr)\n",
    "    snapped = sorted(seen_at.values(), key=lambda x: x[0])\n",
    "\n",
    "    # Enforce alternation again after snapping\n",
    "    snapped = enforce_alternation(snapped)\n",
    "\n",
    "    # Filter micro-legs using adaptive threshold (min % move and sigma*local_vol)\n",
    "    pruned = []\n",
    "    for idx, typ, pr in snapped:\n",
    "        if not pruned:\n",
    "            pruned.append((idx, typ, pr))\n",
    "            continue\n",
    "        prev_idx, prev_typ, prev_pr = pruned[-1]\n",
    "        # time spacing\n",
    "        if idx - prev_idx < distance:\n",
    "            # keep the more prominent of the two\n",
    "            if pr > prev_pr:\n",
    "                pruned[-1] = (idx, typ, pr)\n",
    "            continue\n",
    "        leg = abs(close[idx] - close[prev_idx])\n",
    "        # thresholds at both ends\n",
    "        thr = max(min_dev_pct * close[prev_idx],\n",
    "                  min_dev_sigma * max(local_vol_price[prev_idx], 1e-12))\n",
    "        thr = max(thr, max(min_dev_pct * close[idx],\n",
    "                           min_dev_sigma * max(local_vol_price[idx], 1e-12)))\n",
    "        if leg >= thr:\n",
    "            pruned.append((idx, typ, pr))\n",
    "        else:\n",
    "            # too small swing â†’ drop the later point\n",
    "            continue\n",
    "\n",
    "    # One more alternation pass (paranoid) and spacing check\n",
    "    pruned = enforce_alternation(pruned)\n",
    "    final_ext = []\n",
    "    for idx, typ, pr in pruned:\n",
    "        if final_ext and idx - final_ext[-1][0] < distance:\n",
    "            # keep stronger\n",
    "            if pr > final_ext[-1][2]:\n",
    "                final_ext[-1] = (idx, typ, pr)\n",
    "        else:\n",
    "            final_ext.append((idx, typ, pr))\n",
    "\n",
    "    # Build labels\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    for idx, typ, _ in final_ext:\n",
    "        labels[idx] = typ\n",
    "\n",
    "    # Mark edges as trend boundaries for continuity\n",
    "    if labels[0] == 0:\n",
    "        labels[0] = 1 if close[min(1, n - 1)] > close[0] else 2\n",
    "    if labels[-1] == 0 and n >= 2:\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "\n",
    "    if verbose:\n",
    "        c0 = int((labels == 0).sum())\n",
    "        c1 = int((labels == 1).sum())\n",
    "        c2 = int((labels == 2).sum())\n",
    "        print(f\"labels -> 0:{c0}  1:{c1}  2:{c2}  (extrema kept: {len(final_ext)})\")\n",
    "\n",
    "    return labels\n"
   ],
   "id": "cf03646179e62d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "9d9e40e665da9f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "NUM_CLASSES = 3  # 0=no signal, 1=buy, 2=sell"
   ],
   "id": "437f948703b12210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"\\n[1/6] Loading and preparing data...\")\n",
    "\n",
    "# Assuming df_model is already loaded\n",
    "# df_model should have columns: DATETIME, OPEN, HIGH, LOW, CLOSE, TICKVOL, VOL, SPREAD, Label\n",
    "\n",
    "# Extract features and labels\n",
    "feature_data = df_model[FEATURES].values\n",
    "labels = df_model['Label'].values\n",
    "\n",
    "# Remove any rows with NaN in features\n",
    "valid_indices = ~np.isnan(feature_data).any(axis=1)\n",
    "feature_data = feature_data[valid_indices]\n",
    "labels = labels[valid_indices]\n",
    "datetime_index = df_model['DATETIME'].values[valid_indices]\n",
    "\n",
    "print(f\"Total valid samples: {len(feature_data)}\")\n",
    "print(f\"Label distribution: {np.bincount(labels.astype(int))}\")"
   ],
   "id": "82c264bb95c8759b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Scaling features...\")\n",
    "\n",
    "# Fit scaler on all data (we'll use this for prediction too)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_data)\n",
    "\n",
    "print(f\"Feature scaling complete. Shape: {scaled_features.shape}\")"
   ],
   "id": "6fa35f032ee62ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CREATE SEQUENCES FOR TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] Creating sequences...\")\n",
    "\n",
    "\n",
    "def create_sequences(features, labels, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences of (window_size) candles with (forecast_horizon) labels.\n",
    "\n",
    "    Returns:\n",
    "        X: shape (num_samples, window_size, num_features)\n",
    "        y: shape (num_samples, forecast_horizon) - raw class labels\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # We need at least window_size + forecast_horizon data points\n",
    "    for i in range(len(features) - window_size - forecast_horizon + 1):\n",
    "        # Input: 60 candles\n",
    "        X.append(features[i:i + window_size])\n",
    "\n",
    "        # Output: labels for next 10 hours\n",
    "        y.append(labels[i + window_size:i + window_size + forecast_horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = create_sequences(scaled_features, labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "print(f\"Sequences created:\")\n",
    "print(f\"  X shape: {X.shape} (samples, window_size, features)\")\n",
    "print(f\"  y shape: {y.shape} (samples, forecast_horizon)\")"
   ],
   "id": "844333ad6d5ca6dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Splitting data...\")\n",
    "\n",
    "# Use 80/20 split, but keep temporal order (no shuffle for time series)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ],
   "id": "d1587b5bb97b135a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# HANDLE CLASS IMBALANCE WITH SAMPLE WEIGHTS (CORRECTED)\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] Computing sample weights for class imbalance...\")\n",
    "\n",
    "# Create sample weights for training\n",
    "sample_weights = np.ones_like(y_train, dtype=np.float32)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    for j in range(FORECAST_HORIZON):\n",
    "        label = int(y_train[i, j])\n",
    "        if label == 0:\n",
    "            sample_weights[i, j] = 0.1  # Lower weight for no-signal\n",
    "        elif label == 1:\n",
    "            sample_weights[i, j] = 10.0  # Very high weight for buy signals\n",
    "        elif label == 2:\n",
    "            sample_weights[i, j] = 10.0  # Very high weight for sell signals\n",
    "\n",
    "# Convert labels to categorical for each timestep\n",
    "y_train_cat = np.array([to_categorical(y_train[:, i], num_classes=NUM_CLASSES)\n",
    "                         for i in range(FORECAST_HORIZON)])\n",
    "y_train_cat = np.transpose(y_train_cat, (1, 0, 2))  # (samples, horizon, classes)\n",
    "\n",
    "y_test_cat = np.array([to_categorical(y_test[:, i], num_classes=NUM_CLASSES)\n",
    "                        for i in range(FORECAST_HORIZON)])\n",
    "y_test_cat = np.transpose(y_test_cat, (1, 0, 2))\n",
    "\n",
    "print(f\"Sample weights shape: {sample_weights.shape}\")\n",
    "print(f\"y_train_cat shape: {y_train_cat.shape}\")\n",
    "\n",
    "# Count labels in training set\n",
    "train_labels_flat = y_train.flatten()\n",
    "train_counts = np.bincount(train_labels_flat.astype(int))\n",
    "print(f\"Training label distribution: {train_counts}\")\n",
    "print(f\"  Class 0 (No Signal): {train_counts[0]} ({100*train_counts[0]/len(train_labels_flat):.1f}%)\")\n",
    "print(f\"  Class 1 (Buy): {train_counts[1]} ({100*train_counts[1]/len(train_labels_flat):.1f}%)\")\n",
    "print(f\"  Class 2 (Sell): {train_counts[2]} ({100*train_counts[2]/len(train_labels_flat):.1f}%)\")\n",
    "\n"
   ],
   "id": "43ea7cfde5a0e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# BUILD MODEL WITH CUSTOM WEIGHTED LOSS\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Building and training model...\")\n",
    "\n",
    "def build_reversal_model(input_shape, forecast_horizon, num_classes):\n",
    "    \"\"\"\n",
    "    Build a deep LSTM model for multi-step classification.\n",
    "\n",
    "    Architecture:\n",
    "    - Bidirectional LSTM layers to capture forward and backward patterns\n",
    "    - Dropout for regularization\n",
    "    - TimeDistributed Dense layers for per-timestep classification\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        # First LSTM layer - bidirectional to capture context from both directions\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Second LSTM layer\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=False)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Dense layer to expand to forecast horizon\n",
    "        layers.RepeatVector(forecast_horizon),\n",
    "\n",
    "        # Third LSTM layer for sequence output\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Output layer: TimeDistributed for per-timestep classification\n",
    "        layers.TimeDistributed(layers.Dense(32, activation='relu')),\n",
    "        layers.TimeDistributed(layers.Dense(num_classes, activation='softmax'))\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_reversal_model(\n",
    "    input_shape=(WINDOW_SIZE, len(FEATURES)),\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Custom weighted loss function\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss that applies per-sample, per-timestep weights.\n",
    "    This will be used without sample_weight parameter in fit().\n",
    "    \"\"\"\n",
    "    # Standard categorical crossentropy\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Compile with categorical crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n"
   ],
   "id": "6f703a047cf37253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CUSTOM DATA GENERATOR WITH SAMPLE WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "class WeightedSequence(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator that properly handles sample weights for multi-output.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, sample_weights, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sample_weights = sample_weights\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "        w_batch = self.sample_weights[batch_indices]\n",
    "\n",
    "        # Return (X, y, sample_weight) tuple\n",
    "        return X_batch, y_batch, w_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ],
   "id": "5b80a0425a939590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# TRAINING WITH PROPER SAMPLE WEIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split training data into train and validation\n",
    "val_split = 0.15\n",
    "val_samples = int(len(X_train) * val_split)\n",
    "train_samples = len(X_train) - val_samples\n",
    "\n",
    "X_train_final = X_train[:train_samples]\n",
    "y_train_final = y_train_cat[:train_samples]\n",
    "w_train_final = sample_weights[:train_samples]\n",
    "\n",
    "X_val = X_train[train_samples:]\n",
    "y_val = y_train_cat[train_samples:]\n",
    "w_val = sample_weights[train_samples:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train_final)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Create data generators\n",
    "train_gen = WeightedSequence(X_train_final, y_train_final, w_train_final, batch_size=64, shuffle=True)\n",
    "val_gen = WeightedSequence(X_val, y_val, w_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=2,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)"
   ],
   "id": "798bd1531cf661d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ============================================================================\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "for step in range(FORECAST_HORIZON):\n",
    "    step_acc = np.mean(y_pred[:, step] == y_test[:, step])\n",
    "    print(f\"  Forecast Hour {step + 1} Accuracy: {step_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL TRAINING SECTION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "98611bfd1abe3203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4c07e72d91d6ab41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1368c058f8523db0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87e662f3d2138b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(4).copy()\n"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = idx + 1\n",
    "actual_future_end = idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. Create predicted_df (forecast for next 10 hours) ---\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(\n",
    "    start=last_timestamp + pd.Timedelta(hours=1),\n",
    "    periods=FORECAST_HORIZON,\n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "c27312e46eb98ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "640f86f86378887c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# 11-4 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# 11-5 Save training loss plot\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 4. Evaluate on validation set (since no X_test/y_test defined)\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_loss, final_val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# 5. Save model summary and performance metrics\n",
    "summary_path = os.path.join(log_dir, 'model_log.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write('\\n')\n",
    "    f.write(f'Final Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Training Accuracy: {final_train_acc:.6f}\\n')\n",
    "    f.write(f'Final Validation Loss: {final_val_loss:.6f}\\n')\n",
    "    f.write(f'Final Validation Accuracy: {final_val_acc:.6f}\\n')"
   ],
   "id": "c60adc7617c16b75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = 'saved_models/model_20251112_071605.keras'\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    safe_mode=False\n",
    ")"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
