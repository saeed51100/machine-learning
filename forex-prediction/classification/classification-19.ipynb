{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-18\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- https://claude.ai/chat/f641b5d3-0bf4-463f-92db-a02f95f027ef\n",
    "\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "2a4eff8556e58600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, Lambda\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models, callbacks, losses, optimizers, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import json\n",
    "import os\n"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(\n",
    "        close,\n",
    "        high=None,\n",
    "        low=None,\n",
    "        smoothing_window=31,\n",
    "        polyorder=3,\n",
    "        base_prom_factor=0.02,\n",
    "        distance=3,\n",
    "        snap_window=5,\n",
    "        min_dev_pct=0.0015,  # 0.15% minimum leg size\n",
    "        min_dev_sigma=2.0,  # >= 2x local abs-return EMA\n",
    "        vol_window=100,  # EMA window for local volatility\n",
    "        verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Label reversal points with improved accuracy.\n",
    "\n",
    "    Returns labels array of length n where:\n",
    "    0 = none, 1 = valley, 2 = peak.\n",
    "\n",
    "    Tips:\n",
    "    - For best accuracy, pass high/low arrays from your OHLCV.\n",
    "      Example: label_reversal_points(df['CLOSE'], df['HIGH'], df['LOW'])\n",
    "    - Tune min_dev_pct / min_dev_sigma to be stricter or looser on swing size.\n",
    "    \"\"\"\n",
    "    close = np.asarray(close, dtype=float)\n",
    "    n = close.size\n",
    "    if n < 3:\n",
    "        return np.zeros(n, dtype=int)\n",
    "\n",
    "    # Interpolate NaNs if any\n",
    "    if np.isnan(close).any():\n",
    "        idx = np.arange(n)\n",
    "        good = ~np.isnan(close)\n",
    "        close = close.copy()\n",
    "        close[~good] = np.interp(idx[~good], idx[good], close[good])\n",
    "\n",
    "    # Helper: simple EMA for local abs-return volatility\n",
    "    def ema(x, span):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        alpha = 2.0 / (span + 1.0)\n",
    "        out = np.empty_like(x)\n",
    "        out[0] = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]\n",
    "        return out\n",
    "\n",
    "    # Local volatility in price terms via EMA of absolute returns\n",
    "    ret = np.zeros(n)\n",
    "    ret[1:] = np.abs(np.diff(close) / np.maximum(1e-12, close[:-1]))\n",
    "    vol_absret = ema(ret, vol_window)\n",
    "    local_vol_price = vol_absret * close  # convert to price units\n",
    "\n",
    "    # Smoothing to get robust candidates\n",
    "    win = smoothing_window\n",
    "    if win >= n:\n",
    "        win = n - 1 if (n - 1) % 2 == 1 else n - 2\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    smoothed = savgol_filter(close, win, polyorder)\n",
    "\n",
    "    # Base prominence threshold\n",
    "    global_std = np.std(close) or 1.0\n",
    "    prom = global_std * base_prom_factor\n",
    "\n",
    "    # Candidate peaks/valleys on smoothed\n",
    "    peak_idx, _ = find_peaks(smoothed, distance=distance, prominence=prom)\n",
    "    val_idx, _ = find_peaks(-smoothed, distance=distance, prominence=prom)\n",
    "\n",
    "    # Prominences for tie-breaking\n",
    "    peak_prom = peak_prominences(smoothed, peak_idx)[0] if peak_idx.size else np.array([])\n",
    "    val_prom = peak_prominences(-smoothed, val_idx)[0] if val_idx.size else np.array([])\n",
    "\n",
    "    # Combine\n",
    "    candidates = []\n",
    "    for i, p in enumerate(peak_idx):\n",
    "        candidates.append((int(p), 2, float(peak_prom[i]) if peak_prom.size else 0.0))\n",
    "    for i, v in enumerate(val_idx):\n",
    "        candidates.append((int(v), 1, float(val_prom[i]) if val_prom.size else 0.0))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    if not candidates:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "        # still mark edges for completeness\n",
    "        labels[0] = 1 if close[1] > close[0] else 2\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "        return labels\n",
    "\n",
    "    # Enforce alternation (remove weaker when two same-type neighbors)\n",
    "    def enforce_alternation(ext):\n",
    "        ext = ext[:]  # list of (idx, typ, prom)\n",
    "        while True:\n",
    "            removed = False\n",
    "            i = 0\n",
    "            while i < len(ext) - 1:\n",
    "                if ext[i][1] == ext[i + 1][1]:\n",
    "                    # drop the smaller prominence\n",
    "                    if ext[i][2] < ext[i + 1][2]:\n",
    "                        ext.pop(i)\n",
    "                    else:\n",
    "                        ext.pop(i + 1)\n",
    "                    removed = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            if not removed:\n",
    "                break\n",
    "        return ext\n",
    "\n",
    "    candidates = enforce_alternation(candidates)\n",
    "\n",
    "    # SNAP: move each extreme to the true local extremum on raw close (or HIGH/LOW)\n",
    "    def snap_index(idx, typ):\n",
    "        L = max(0, idx - snap_window)\n",
    "        R = min(n, idx + snap_window + 1)\n",
    "        if high is not None and low is not None:\n",
    "            if typ == 2:  # peak\n",
    "                j = np.argmax(np.asarray(high[L:R], dtype=float))\n",
    "            else:  # valley\n",
    "                j = np.argmin(np.asarray(low[L:R], dtype=float))\n",
    "        else:\n",
    "            if typ == 2:\n",
    "                j = np.argmax(close[L:R])\n",
    "            else:\n",
    "                j = np.argmin(close[L:R])\n",
    "        return L + int(j)\n",
    "\n",
    "    snapped = []\n",
    "    seen_at = {}  # avoid duplicate indices by keeping stronger prominence\n",
    "    for idx, typ, pr in candidates:\n",
    "        j = snap_index(idx, typ)\n",
    "        key = (j, typ)\n",
    "        if key not in seen_at or pr > seen_at[key][2]:\n",
    "            seen_at[key] = (j, typ, pr)\n",
    "    snapped = sorted(seen_at.values(), key=lambda x: x[0])\n",
    "\n",
    "    # Enforce alternation again after snapping\n",
    "    snapped = enforce_alternation(snapped)\n",
    "\n",
    "    # Filter micro-legs using adaptive threshold (min % move and sigma*local_vol)\n",
    "    pruned = []\n",
    "    for idx, typ, pr in snapped:\n",
    "        if not pruned:\n",
    "            pruned.append((idx, typ, pr))\n",
    "            continue\n",
    "        prev_idx, prev_typ, prev_pr = pruned[-1]\n",
    "        # time spacing\n",
    "        if idx - prev_idx < distance:\n",
    "            # keep the more prominent of the two\n",
    "            if pr > prev_pr:\n",
    "                pruned[-1] = (idx, typ, pr)\n",
    "            continue\n",
    "        leg = abs(close[idx] - close[prev_idx])\n",
    "        # thresholds at both ends\n",
    "        thr = max(min_dev_pct * close[prev_idx],\n",
    "                  min_dev_sigma * max(local_vol_price[prev_idx], 1e-12))\n",
    "        thr = max(thr, max(min_dev_pct * close[idx],\n",
    "                           min_dev_sigma * max(local_vol_price[idx], 1e-12)))\n",
    "        if leg >= thr:\n",
    "            pruned.append((idx, typ, pr))\n",
    "        else:\n",
    "            # too small swing → drop the later point\n",
    "            continue\n",
    "\n",
    "    # One more alternation pass (paranoid) and spacing check\n",
    "    pruned = enforce_alternation(pruned)\n",
    "    final_ext = []\n",
    "    for idx, typ, pr in pruned:\n",
    "        if final_ext and idx - final_ext[-1][0] < distance:\n",
    "            # keep stronger\n",
    "            if pr > final_ext[-1][2]:\n",
    "                final_ext[-1] = (idx, typ, pr)\n",
    "        else:\n",
    "            final_ext.append((idx, typ, pr))\n",
    "\n",
    "    # Build labels\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    for idx, typ, _ in final_ext:\n",
    "        labels[idx] = typ\n",
    "\n",
    "    # Mark edges as trend boundaries for continuity\n",
    "    if labels[0] == 0:\n",
    "        labels[0] = 1 if close[min(1, n - 1)] > close[0] else 2\n",
    "    if labels[-1] == 0 and n >= 2:\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "\n",
    "    if verbose:\n",
    "        c0 = int((labels == 0).sum())\n",
    "        c1 = int((labels == 1).sum())\n",
    "        c2 = int((labels == 2).sum())\n",
    "        print(f\"labels -> 0:{c0}  1:{c1}  2:{c2}  (extrema kept: {len(final_ext)})\")\n",
    "\n",
    "    return labels\n"
   ],
   "id": "cf03646179e62d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display label distribution in df_model\n",
    "label_counts = df_model['Label'].value_counts().sort_index()\n",
    "label_percentages = (df_model['Label'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "print(\"Label Distribution in df_model:\")\n",
    "print(\"-\" * 40)\n",
    "for label in sorted(df_model['Label'].unique()):\n",
    "    count = label_counts[label]\n",
    "    percentage = label_percentages[label]\n",
    "    print(f\"Class {label}: {count:,} rows ({percentage:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total rows: {len(df_model):,}\")\n"
   ],
   "id": "8e1d8ac369134288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "f29bcb88e0fcade0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Configuration (from user)\n",
    "# ---------------------------\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "id": "30527aee33dd82a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Helper: build supervised samples\n",
    "# ---------------------------\n",
    "def make_samples_from_df(df_source, features, label_column='Label',\n",
    "                         window_size=WINDOW_SIZE, horizon=FORECAST_HORIZON):\n",
    "    \"\"\"\n",
    "    Produces X (num_samples, window_size, n_features) and y (num_samples, horizon)\n",
    "    using df_source which must contain the label_column for rows used to produce labels.\n",
    "    This function assumes df_source index is chronological.\n",
    "    \"\"\"\n",
    "    arr_X = []\n",
    "    arr_y = []\n",
    "    # we'll generate samples where i = index of the last input candle in sequence\n",
    "    max_i = len(df_source) - horizon  # because we need horizon labels after last input index\n",
    "    for end_idx in range(window_size - 1, max_i):\n",
    "        start_idx = end_idx - (window_size - 1)\n",
    "        x_window = df_source.iloc[start_idx:end_idx + 1][features].values\n",
    "        # labels are the next horizon rows' Label column\n",
    "        y_window = df_source.iloc[end_idx + 1:end_idx + 1 + horizon][label_column].values\n",
    "        # sanity: only include samples where y_window length == horizon\n",
    "        if len(y_window) != horizon:\n",
    "            continue\n",
    "        arr_X.append(x_window)\n",
    "        arr_y.append(y_window.astype(int))\n",
    "    X = np.array(arr_X)  # shape (N, window_size, n_features)\n",
    "    y = np.array(arr_y)  # shape (N, horizon)\n",
    "    return X, y\n"
   ],
   "id": "5f63cf37677bcb59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Prepare training/validation/test sets from df_model (chronological)\n",
    "# ---------------------------\n",
    "# NOTE: df_model must already exist in notebook (first 130,000 rows with 'Label' column)\n",
    "N_rows = len(df_model)\n",
    "print(f\"df_model rows: {N_rows}\")\n",
    "\n",
    "# Create supervised samples from df_model\n",
    "X_all, y_all = make_samples_from_df(df_model, FEATURES, label_column='Label',\n",
    "                                    window_size=WINDOW_SIZE, horizon=FORECAST_HORIZON)\n",
    "print(\"All samples created:\", X_all.shape, y_all.shape)\n",
    "\n",
    "# Chronological split into 70% train / 15% val / 15% test (on the samples)\n",
    "n_samples = X_all.shape[0]\n",
    "train_end = int(n_samples * 0.70)\n",
    "val_end = train_end + int(n_samples * 0.15)\n",
    "\n",
    "X_train, y_train = X_all[:train_end], y_all[:train_end]\n",
    "X_val, y_val = X_all[train_end:val_end], y_all[train_end:val_end]\n",
    "X_test, y_test = X_all[val_end:], y_all[val_end:]\n",
    "\n",
    "print(\"Train/Val/Test shapes:\", X_train.shape, X_val.shape, X_test.shape)"
   ],
   "id": "d7db09b255da359c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Scale features (fit scaler on training input windows flattened across time)\n",
    "# ---------------------------\n",
    "# We'll scale per-feature using training set statistics (fit on flattened time axis)\n",
    "scaler = StandardScaler()\n",
    "# concatenate all timesteps of training data for scaler fitting\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler.fit(X_train_flat)\n",
    "\n",
    "\n",
    "def scale_X(X):\n",
    "    shp = X.shape\n",
    "    X_flat = X.reshape(-1, shp[-1])\n",
    "    Xs = scaler.transform(X_flat).reshape(shp)\n",
    "    return Xs\n",
    "\n",
    "\n",
    "X_train_s = scale_X(X_train)\n",
    "X_val_s = scale_X(X_val)\n",
    "X_test_s = scale_X(X_test)"
   ],
   "id": "90ed2f297b1d553b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Address class imbalance: create sample_weight per-sample-per-timestep\n",
    "# (shape (num_samples, FORECAST_HORIZON))\n",
    "# ---------------------------\n",
    "# Compute class frequencies on training labels across all timesteps\n",
    "unique, counts = np.unique(y_train.flatten(), return_counts=True)\n",
    "freq = dict(zip(unique.tolist(), counts.tolist()))\n",
    "print(\"Train label frequencies:\", freq)\n",
    "\n",
    "# If a class is missing in training (rare), add small epsilon to avoid division by zero\n",
    "all_classes = [0, 1, 2]\n",
    "class_counts = np.array([freq.get(c, 0) + 1e-6 for c in all_classes], dtype=np.float64)\n",
    "class_freq = class_counts / class_counts.sum()\n",
    "inv_freq = 1.0 / class_counts  # inverse counts\n",
    "# normalize weights so that avg weight ~1.0 (optional)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "\n",
    "weight_map = {c: inv_freq[i] for i, c in enumerate(all_classes)}\n",
    "print(\"Class weight map (per class):\", weight_map)\n",
    "\n",
    "\n",
    "def make_sample_weights(y_array):\n",
    "    # y_array shape: (num_samples, horizon) with integer labels 0/1/2\n",
    "    sw = np.vectorize(lambda label: weight_map[int(label)])(y_array)\n",
    "    return sw.astype(np.float32)  # shape (num_samples, horizon)\n",
    "\n",
    "\n",
    "sample_weight_train = make_sample_weights(y_train)\n",
    "sample_weight_val = make_sample_weights(y_val)"
   ],
   "id": "18d16bba0a10289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Build model: Conv1D -> BiLSTM -> TimeDistributed Dense(softmax)\n",
    "# Output shape: (batch, FORECAST_HORIZON, 3)\n",
    "# ---------------------------\n",
    "n_features = len(FEATURES)\n",
    "n_classes = 3\n",
    "\n",
    "\n",
    "def build_model(window_size=WINDOW_SIZE, n_features=n_features, horizon=FORECAST_HORIZON, n_classes=n_classes):\n",
    "    inp = layers.Input(shape=(window_size, n_features), name='input_window')\n",
    "    # small feature extractor\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(inp)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool1D(pool_size=2)(x)\n",
    "    # BiLSTM to encode temporal features\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)\n",
    "    # Expand to horizon timesteps and decode per-future-step\n",
    "    x = layers.RepeatVector(horizon)(x)  # shape (batch, horizon, features)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x)\n",
    "    out = layers.TimeDistributed(layers.Dense(n_classes, activation='softmax'), name='out')(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ],
   "id": "3c091ad81f626b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Compile model\n",
    "# ---------------------------\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),  # expects integer labels shape (batch, horizon)\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(name='sparse_acc')]\n",
    ")"
   ],
   "id": "db60925388da7898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Fit model with sample_weight (per-timestep)\n",
    "# Note: Keras accepts sample_weight with same shape as labels (batch, time) for sequence losses.\n",
    "# ---------------------------\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "history = model.fit(\n",
    "    X_train_s,  # shape (N_train, 60, n_features)\n",
    "    y_train,  # shape (N_train, 10)\n",
    "    sample_weight=sample_weight_train,  # shape (N_train, 10)\n",
    "    validation_data=(X_val_s, y_val, sample_weight_val),\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    callbacks=[es, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "840d6f1a985cd32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:47:49.718718Z",
     "start_time": "2025-11-30T16:47:44.003324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Evaluate on test set\n",
    "# ---------------------------\n",
    "sample_weight_test = make_sample_weights(y_test)\n",
    "test_res = model.evaluate(X_test_s, y_test, sample_weight=sample_weight_test, verbose=2)\n",
    "print(\"Test loss/metrics:\", test_res)\n"
   ],
   "id": "65739861c423ced",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686/686 - 6s - 8ms/step - loss: 0.0372 - sparse_acc: 0.2253\n",
      "Test loss/metrics: [0.037219345569610596, 0.22532817721366882]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:50:19.611170Z",
     "start_time": "2025-11-30T16:50:19.488866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# PREDICTION SECTION (must use df and given_time)\n",
    "# - Use df (not df_model) to assemble the 60-candle input ending at given_time\n",
    "# - Output predicted_df with DATETIME (10 rows), forecast_class, prob_0, prob_1, prob_2\n",
    "# ---------------------------\n",
    "# Parse given_time; tolerant parse\n",
    "given_time = \"2025.08.13 21:00:00\"\n",
    "\n",
    "def parse_given_time(s):\n",
    "    # expected format \"YYYY.MM.DD HH:MM:SS\"\n",
    "    return datetime.strptime(s, \"%Y.%m.%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "gt = parse_given_time(given_time)\n",
    "\n",
    "# ensure df's DATETIME column is datetime dtype and sorted chronologically\n",
    "if not np.issubdtype(df['DATETIME'].dtype, np.datetime64):\n",
    "    df = df.copy()\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "df = df.sort_values('DATETIME').reset_index(drop=True)\n",
    "\n",
    "# find the row index in df where DATETIME == given_time (the last input candle timestamp)\n",
    "mask_idx = df.index[df['DATETIME'] == gt].tolist()\n",
    "if not mask_idx:\n",
    "    raise ValueError(f\"given_time {given_time} not found exactly in df['DATETIME']. Check format and presence.\")\n",
    "end_idx = mask_idx[0]\n",
    "\n",
    "# check we have at least WINDOW_SIZE candles ending at given_time\n",
    "start_idx = end_idx - (WINDOW_SIZE - 1)\n",
    "if start_idx < 0:\n",
    "    raise ValueError(f\"Not enough historical rows in df before given_time. Need {WINDOW_SIZE} candles.\")\n",
    "\n",
    "# Build input input_df (1, window_size, n_features)\n",
    "X_input_raw = df.iloc[start_idx:end_idx + 1][FEATURES].values  # shape (60, n_features)\n",
    "input_df = scale_X(X_input_raw.reshape(1, WINDOW_SIZE, len(FEATURES)))  # scaled\n",
    "\n",
    "# Predict probabilities\n",
    "probs = model.predict(input_df)  # shape (1, horizon, 3)\n",
    "probs = probs[0]  # shape (horizon, 3)\n",
    "pred_classes = probs.argmax(axis=1).astype(int)  # shape (horizon,)\n",
    "\n",
    "# Build DATETIME series for the forecast horizon: next 10 hours immediately after given_time\n",
    "forecast_datetimes = [gt + timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': pred_classes,\n",
    "    'prob_0': probs[:, 0],\n",
    "    'prob_1': probs[:, 1],\n",
    "    'prob_2': probs[:, 2]\n",
    "})\n",
    "\n",
    "# show predicted_df as final line per your requirement\n",
    "predicted_df"
   ],
   "id": "ef1e29ee5df85060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             DATETIME  forecast_class    prob_0    prob_1    prob_2\n",
       "0 2025-08-13 22:00:00               1  0.180694  0.455670  0.363636\n",
       "1 2025-08-13 23:00:00               1  0.170417  0.471633  0.357951\n",
       "2 2025-08-14 00:00:00               1  0.166322  0.476266  0.357412\n",
       "3 2025-08-14 01:00:00               1  0.164266  0.477913  0.357821\n",
       "4 2025-08-14 02:00:00               1  0.164229  0.477556  0.358215\n",
       "5 2025-08-14 03:00:00               1  0.166184  0.475201  0.358615\n",
       "6 2025-08-14 04:00:00               1  0.170719  0.470237  0.359044\n",
       "7 2025-08-14 05:00:00               1  0.179203  0.461314  0.359482\n",
       "8 2025-08-14 06:00:00               1  0.194210  0.446094  0.359697\n",
       "9 2025-08-14 07:00:00               1  0.220051  0.420973  0.358976"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>forecast_class</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>prob_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-13 22:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>0.455670</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-13 23:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170417</td>\n",
       "      <td>0.471633</td>\n",
       "      <td>0.357951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166322</td>\n",
       "      <td>0.476266</td>\n",
       "      <td>0.357412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-14 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164266</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.357821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-14 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164229</td>\n",
       "      <td>0.477556</td>\n",
       "      <td>0.358215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-14 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>0.475201</td>\n",
       "      <td>0.358615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-14 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170719</td>\n",
       "      <td>0.470237</td>\n",
       "      <td>0.359044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-14 05:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179203</td>\n",
       "      <td>0.461314</td>\n",
       "      <td>0.359482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-14 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194210</td>\n",
       "      <td>0.446094</td>\n",
       "      <td>0.359697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-14 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220051</td>\n",
       "      <td>0.420973</td>\n",
       "      <td>0.358976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cc120fdef943ba43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "40aae797f3b5e6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7780400407a52a90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9f1606cb3f9fe40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "39a175ca7c0b7cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e50622a911249a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d601f97a1ab03e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2793b3ddc3046352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:50:27.499993Z",
     "start_time": "2025-11-30T16:50:27.484301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vals = predicted_df[\"forecast_class\"].tolist()\n",
    "print(\"[\" + \" \".join(str(x) for x in vals) + \"]\")\n",
    "\n",
    "\n",
    "def filter_forecast_classes(fc):\n",
    "    fc = fc.copy()\n",
    "    n = len(fc)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Rule A: Keep only last in consecutive runs\n",
    "    # ------------------------------------------\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if fc[i] in [1, 2]:\n",
    "            val = fc[i]\n",
    "            start = i\n",
    "            end = i\n",
    "            while end + 1 < n and fc[end + 1] == val:\n",
    "                end += 1\n",
    "            # Keep only the last occurrence\n",
    "            for k in range(start, end):\n",
    "                fc[k] = 0\n",
    "            i = end\n",
    "        i += 1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Rule B & C: If val (1 or 2) appears, then zeros, then val\n",
    "    # ---------------------------------------------------------\n",
    "    for val in [1, 2]:\n",
    "        last_pos = None\n",
    "        for i in range(n):\n",
    "            if fc[i] == val:\n",
    "                if last_pos is not None:\n",
    "                    # Remove the old one (set to 0)\n",
    "                    fc[last_pos] = 0\n",
    "                last_pos = i\n",
    "            elif fc[i] != 0:\n",
    "                # reset tracking when 0-block is broken by other class\n",
    "                last_pos = None\n",
    "\n",
    "    return fc\n",
    "\n",
    "\n",
    "filtered = filter_forecast_classes(predicted_df[\"forecast_class\"].tolist())\n",
    "print(\"[\" + \" \".join(str(x) for x in filtered) + \"]\")"
   ],
   "id": "5a77e63e291e3f37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:50:31.461593Z",
     "start_time": "2025-11-30T16:50:31.446855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Part 3: Apply filtering\n",
    "# -----------------------------\n",
    "\n",
    "# Overwrite forecast_class (replace old with new)\n",
    "predicted_df[\"forecast_class\"] = filtered\n",
    "\n",
    "# Show full dataframe\n",
    "predicted_df"
   ],
   "id": "27d99deac3efd463",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             DATETIME  forecast_class    prob_0    prob_1    prob_2\n",
       "0 2025-08-13 22:00:00               0  0.180694  0.455670  0.363636\n",
       "1 2025-08-13 23:00:00               0  0.170417  0.471633  0.357951\n",
       "2 2025-08-14 00:00:00               0  0.166322  0.476266  0.357412\n",
       "3 2025-08-14 01:00:00               0  0.164266  0.477913  0.357821\n",
       "4 2025-08-14 02:00:00               0  0.164229  0.477556  0.358215\n",
       "5 2025-08-14 03:00:00               0  0.166184  0.475201  0.358615\n",
       "6 2025-08-14 04:00:00               0  0.170719  0.470237  0.359044\n",
       "7 2025-08-14 05:00:00               0  0.179203  0.461314  0.359482\n",
       "8 2025-08-14 06:00:00               0  0.194210  0.446094  0.359697\n",
       "9 2025-08-14 07:00:00               1  0.220051  0.420973  0.358976"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>forecast_class</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>prob_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-13 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>0.455670</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-13 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170417</td>\n",
       "      <td>0.471633</td>\n",
       "      <td>0.357951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-14 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166322</td>\n",
       "      <td>0.476266</td>\n",
       "      <td>0.357412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-14 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164266</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.357821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-14 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164229</td>\n",
       "      <td>0.477556</td>\n",
       "      <td>0.358215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-14 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>0.475201</td>\n",
       "      <td>0.358615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-14 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170719</td>\n",
       "      <td>0.470237</td>\n",
       "      <td>0.359044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-14 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179203</td>\n",
       "      <td>0.461314</td>\n",
       "      <td>0.359482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-14 06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194210</td>\n",
       "      <td>0.446094</td>\n",
       "      <td>0.359697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-14 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220051</td>\n",
       "      <td>0.420973</td>\n",
       "      <td>0.358976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "437f948703b12210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87e662f3d2138b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T16:50:35.607601Z",
     "start_time": "2025-11-30T16:50:35.572203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(4).copy()"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tail'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# --------------------------\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# === Visualization Block ===\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# --------------------------\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m historical_df = \u001B[43minput_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtail\u001B[49m(\u001B[32m4\u001B[39m).copy()\n",
      "\u001B[31mAttributeError\u001B[39m: 'numpy.ndarray' object has no attribute 'tail'"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "c27312e46eb98ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "640f86f86378887c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model with Comprehensive Report\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# 11-4 Save scaler (IMPORTANT - needed for predictions!)\n",
    "import joblib\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 11-5 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# 11-6 Save full history as JSON so it can be reloaded later\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "print(f\"Full history object saved to: {history_json_path}\")\n",
    "\n",
    "# 11-7 Save training loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-8 Save accuracy plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_accuracy.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-9 Evaluate on validation set (NO ONE-HOT)\n",
    "eval_results = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_loss = eval_results[0]\n",
    "final_val_acc = eval_results[1]\n",
    "\n",
    "# 11-10 Generate detailed predictions for per-class analysis\n",
    "print(\"\\n[GENERATING DETAILED METRICS]\")\n",
    "y_val_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_val_pred = np.argmax(y_val_pred_proba, axis=-1)\n",
    "\n",
    "# Flatten predictions and true labels for sklearn metrics\n",
    "y_val_pred_flat = y_val_pred.flatten()\n",
    "y_val_true_flat = y_val.flatten()\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(\n",
    "    y_val_true_flat,\n",
    "    y_val_pred_flat,\n",
    "    target_names=['Class 0 (No Signal)', 'Class 1 (Buy)', 'Class 2 (Sell)'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_val_true_flat, y_val_pred_flat)\n",
    "\n",
    "# Calculate class distributions\n",
    "train_class_dist = np.bincount(y_train.flatten()) / len(y_train.flatten()) * 100\n",
    "val_class_dist = np.bincount(y_val.flatten()) / len(y_val.flatten()) * 100\n",
    "test_class_dist = np.bincount(y_test.flatten()) / len(y_test.flatten()) * 100\n",
    "\n",
    "# Get class weights used during training\n",
    "labels_flat = y_train.flatten()\n",
    "classes = np.unique(labels_flat)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = compute_class_weight('balanced', classes=classes, y=labels_flat)\n",
    "class_weight_dict = {i: weights[i] for i in range(len(weights))}\n",
    "\n",
    "# Calculate training time from history\n",
    "epochs_trained = len(history.history['loss'])\n",
    "\n",
    "# 11-11 Save confusion matrix plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(log_dir, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 11-12 Save per-class performance plot\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_val_true_flat, y_val_pred_flat, average=None\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "metrics = [precision, recall, f1]\n",
    "metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    axes[idx].bar(['Class 0', 'Class 1', 'Class 2'], metric, color=colors_bar)\n",
    "    axes[idx].set_title(f'{name} by Class')\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].set_ylabel(name)\n",
    "    for i, v in enumerate(metric):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(log_dir, 'per_class_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 11-12 Create comprehensive report\n",
    "comprehensive_report = f\"\"\"\n",
    "{'=' * 80}\n",
    "MODEL TRAINING REPORT\n",
    "{'=' * 80}\n",
    "Timestamp: {timestamp}\n",
    "Model Path: {model_path}\n",
    "Scaler Path: {scaler_path}\n",
    "\n",
    "{'=' * 80}\n",
    "DATA CONFIGURATION\n",
    "{'=' * 80}\n",
    "Total Samples in df_model: {len(df_model):,}\n",
    "\n",
    "Training Samples: {len(X_train):,} ({len(X_train) / len(df_model) * 100:.1f}%)\n",
    "Validation Samples: {len(X_val):,} ({len(X_val) / len(df_model) * 100:.1f}%)\n",
    "Test Samples: {len(X_test):,} ({len(X_test) / len(df_model) * 100:.1f}%)\n",
    "\n",
    "CLASS DISTRIBUTION:\n",
    "Training Set:\n",
    "  - Class 0 (No Signal): {train_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {train_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {train_class_dist[2]:.2f}%\n",
    "\n",
    "Validation Set:\n",
    "  - Class 0 (No Signal): {val_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {val_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {val_class_dist[2]:.2f}%\n",
    "\n",
    "Test Set:\n",
    "  - Class 0 (No Signal): {test_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {test_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {test_class_dist[2]:.2f}%\n",
    "\n",
    "FEATURE CONFIGURATION:\n",
    "Features Used: {', '.join(FEATURES)}\n",
    "Window Size: {WINDOW_SIZE} hours\n",
    "Forecast Horizon: {FORECAST_HORIZON} hours\n",
    "\n",
    "{'=' * 80}\n",
    "TRAINING CONFIGURATION\n",
    "{'=' * 80}\n",
    "Optimizer: Adam (initial lr=0.001)\n",
    "Loss Function: Categorical Crossentropy\n",
    "Batch Size: 64\n",
    "Early Stopping: patience=15, monitor=val_loss\n",
    "Reduce LR: patience=7, factor=0.5\n",
    "\n",
    "CLASS WEIGHTS (for handling imbalance):\n",
    "  Class 0: {class_weight_dict[0]:.4f}\n",
    "  Class 1: {class_weight_dict[1]:.4f}\n",
    "  Class 2: {class_weight_dict[2]:.4f}\n",
    "\n",
    "TRAINING PROGRESS:\n",
    "Epochs Trained: {epochs_trained} / 100\n",
    "Best Validation Loss Epoch: {np.argmin(history.history['val_loss']) + 1}\n",
    "\n",
    "{'=' * 80}\n",
    "MODEL ARCHITECTURE\n",
    "{'=' * 80}\n",
    "Total Parameters: {model.count_params():,}\n",
    "Trainable Parameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}\n",
    "\n",
    "{'=' * 80}\n",
    "OVERALL METRICS\n",
    "{'=' * 80}\n",
    "Final Training Loss: {final_train_loss:.6f}\n",
    "Final Training Accuracy: {final_train_acc:.6f}\n",
    "Final Validation Loss: {final_val_loss:.6f}\n",
    "Final Validation Accuracy: {final_val_acc:.6f}\n",
    "\n",
    "{'=' * 80}\n",
    "PER-CLASS PERFORMANCE (Validation Set)\n",
    "{'=' * 80}\n",
    "{class_report}\n",
    "\n",
    "{'=' * 80}\n",
    "CONFUSION MATRIX (Validation Set)\n",
    "{'=' * 80}\n",
    "                Predicted\n",
    "              Class 0  Class 1  Class 2\n",
    "Actual Class 0  {cm[0][0]:6d}    {cm[0][1]:6d}    {cm[0][2]:6d}\n",
    "       Class 1  {cm[1][0]:6d}    {cm[1][1]:6d}    {cm[1][2]:6d}\n",
    "       Class 2  {cm[2][0]:6d}    {cm[2][1]:6d}    {cm[2][2]:6d}\n",
    "\n",
    "{'=' * 80}\n",
    "MINORITY CLASS ANALYSIS\n",
    "{'=' * 80}\n",
    "Class 1 (Buy Reversal):\n",
    "  Total Instances: {support[1]}\n",
    "  Correctly Predicted: {cm[1][1]}\n",
    "  Missed (False Negatives): {cm[1][0] + cm[1][2]}\n",
    "  False Positives: {cm[0][1] + cm[2][1]}\n",
    "\n",
    "Class 2 (Sell Reversal):\n",
    "  Total Instances: {support[2]}\n",
    "  Correctly Predicted: {cm[2][2]}\n",
    "  Missed (False Negatives): {cm[2][0] + cm[2][1]}\n",
    "  False Positives: {cm[0][2] + cm[1][2]}\n",
    "\n",
    "{'=' * 80}\n",
    "FILES SAVED\n",
    "{'=' * 80}\n",
    "- Model: {model_filename}\n",
    "- Scaler: scaler_{timestamp}.pkl\n",
    "- Training History: training_history.csv\n",
    "- Training Loss Plot: training_loss.png\n",
    "- Training Accuracy Plot: training_accuracy.png\n",
    "- Confusion Matrix: confusion_matrix.png\n",
    "- Per-Class Metrics: per_class_metrics.png\n",
    "- This Report: comprehensive_report.txt\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    "\n",
    "# 11-13 Save comprehensive report\n",
    "report_path = os.path.join(log_dir, 'comprehensive_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(comprehensive_report)\n",
    "\n",
    "# Also save model summary separately\n",
    "summary_path = os.path.join(log_dir, 'model_architecture.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Print the comprehensive report to console\n",
    "print(comprehensive_report)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"[SAVE COMPLETE]\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"All files saved in: {log_dir}\")\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - Comprehensive Report: {report_path}\")\n",
    "print(f\"  - Model Architecture: {summary_path}\")\n",
    "print(f\"  - Confusion Matrix: {os.path.join(log_dir, 'confusion_matrix.png')}\")\n",
    "print(f\"  - Per-Class Metrics: {os.path.join(log_dir, 'per_class_metrics.png')}\")"
   ],
   "id": "7673e991882f6874",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251128_141712.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251128_141712.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251128_141712_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3b67d57f5b5bf4bd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
