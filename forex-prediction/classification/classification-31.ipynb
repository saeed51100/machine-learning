{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-31 (copy from 28)\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- change sections\n",
    "\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "2a4eff8556e58600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, Dropout\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow import keras\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2- Find start of dataset\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3- Select the rows required for the model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def label_trends_zigzag(close, threshold=0.002):\n",
    "    \"\"\"\n",
    "    ZigZag-style trend labeling.\n",
    "\n",
    "    Parameters:\n",
    "    close : pd.Series\n",
    "        Close price series\n",
    "    threshold : float\n",
    "        Minimum relative price change to confirm trend (e.g. 0.002 = 0.2%)\n",
    "\n",
    "    Returns:\n",
    "    labels : np.ndarray\n",
    "        0 = flat, 1 = bullish, 2 = bearish\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(close)\n",
    "    labels = np.zeros(n, dtype=np.int8)\n",
    "\n",
    "    last_pivot_idx = 0\n",
    "    last_pivot_price = close.iloc[0]\n",
    "\n",
    "    trend = 0  # 0 = unknown, 1 = bullish, 2 = bearish\n",
    "\n",
    "    for i in range(1, n):\n",
    "        price = close.iloc[i]\n",
    "        change = (price - last_pivot_price) / last_pivot_price\n",
    "\n",
    "        # No trend yet → wait for confirmation\n",
    "        if trend == 0:\n",
    "            if change >= threshold:\n",
    "                trend = 1\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "            elif change <= -threshold:\n",
    "                trend = 2\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "\n",
    "        # Bullish trend\n",
    "        elif trend == 1:\n",
    "            if price > last_pivot_price:\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "            elif (last_pivot_price - price) / last_pivot_price >= threshold:\n",
    "                labels[last_pivot_idx:i] = 1\n",
    "                trend = 2\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "\n",
    "        # Bearish trend\n",
    "        elif trend == 2:\n",
    "            if price < last_pivot_price:\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "            elif (price - last_pivot_price) / last_pivot_price >= threshold:\n",
    "                labels[last_pivot_idx:i] = 2\n",
    "                trend = 1\n",
    "                last_pivot_idx = i\n",
    "                last_pivot_price = price\n",
    "\n",
    "    # Fill last segment\n",
    "    if trend == 1:\n",
    "        labels[last_pivot_idx:] = 1\n",
    "    elif trend == 2:\n",
    "        labels[last_pivot_idx:] = 2\n",
    "\n",
    "    return labels\n"
   ],
   "id": "aab524bdf48bf3c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5- baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6- Display label distribution in df_model\n",
    "label_counts = df_model['Label'].value_counts().sort_index()\n",
    "label_percentages = (df_model['Label'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "print(\"Label Distribution in df_model:\")\n",
    "print(\"-\" * 40)\n",
    "for label in sorted(df_model['Label'].unique()):\n",
    "    count = label_counts[label]\n",
    "    percentage = label_percentages[label]\n",
    "    print(f\"Class {label}: {count:,} rows ({percentage:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total rows: {len(df_model):,}\")\n"
   ],
   "id": "8e1d8ac369134288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7- Plot labels\n",
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with trend labels based on the 'Label' column.\n",
    "    Label meanings:\n",
    "        0 = flat / no trend (yellow)\n",
    "        1 = bullish (green)\n",
    "        2 = bearish (red)\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop NaN rows\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(\n",
    "        df_plot['DATETIME'],\n",
    "        df_plot['CLOSE'],\n",
    "        label='Close Price',\n",
    "        color='black',\n",
    "        linewidth=1.5\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "    alpha_level = .3\n",
    "\n",
    "    # === Plot Labels ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "\n",
    "        # Bullish\n",
    "        if row['Label'] == 1:\n",
    "            plt.axvline(\n",
    "                x=row['DATETIME'],\n",
    "                color='green',\n",
    "                alpha=alpha_level\n",
    "            )\n",
    "\n",
    "        # Bearish\n",
    "        elif row['Label'] == 2:\n",
    "            plt.axvline(\n",
    "                x=row['DATETIME'],\n",
    "                color='red',\n",
    "                alpha=alpha_level\n",
    "            )\n",
    "\n",
    "\n",
    "        # Flat / No trend\n",
    "        elif row['Label'] == 0:\n",
    "            plt.axvline(\n",
    "                x=row['DATETIME'],\n",
    "                color='blue',\n",
    "                alpha=alpha_level\n",
    "            )\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Regime Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "id": "c11782e0612674f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "7a73456c9db9c4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8- PART 2 — CHRONOLOGICAL SPLITTING\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "LABEL_COL = 'Label'\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6, \"Split ratios must sum to 1\"\n",
    "assert LABEL_COL in df_model.columns, \"Label column missing\"\n",
    "for f in FEATURES:\n",
    "    assert f in df_model.columns, f\"Feature '{f}' missing\"\n",
    "\n",
    "# Ensure strict chronological order\n",
    "df_model = df_model.sort_values('DATETIME').reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute split indices\n",
    "# ----------------------------\n",
    "n_total = len(df_model)\n",
    "\n",
    "train_end = int(n_total * TRAIN_RATIO)\n",
    "val_end = train_end + int(n_total * VAL_RATIO)\n",
    "\n",
    "# ----------------------------\n",
    "# Chronological split\n",
    "# ----------------------------\n",
    "df_train = df_model.iloc[:train_end].copy()\n",
    "df_val = df_model.iloc[train_end:val_end].copy()\n",
    "df_test = df_model.iloc[val_end:].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Separate features and labels\n",
    "# (to be used in PART 3: Scaling)\n",
    "# ----------------------------\n",
    "X_train_df = df_train[FEATURES].copy()\n",
    "y_train_df = df_train[LABEL_COL].copy()\n",
    "\n",
    "X_val_df = df_val[FEATURES].copy()\n",
    "y_val_df = df_val[LABEL_COL].copy()\n",
    "\n",
    "X_test_df = df_test[FEATURES].copy()\n",
    "y_test_df = df_test[LABEL_COL].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Print outputs required for PART 3\n",
    "# ----------------------------\n",
    "print(\"=== CHRONOLOGICAL SPLIT COMPLETED ===\\n\")\n",
    "\n",
    "print(\"Rows:\")\n",
    "print(f\"Train: {len(df_train)}\")\n",
    "print(f\"Val  : {len(df_val)}\")\n",
    "print(f\"Test : {len(df_test)}\\n\")\n",
    "\n",
    "print(\"Feature matrices (for scaling):\")\n",
    "print(f\"X_train_df shape: {X_train_df.shape}\")\n",
    "print(f\"X_val_df   shape: {X_val_df.shape}\")\n",
    "print(f\"X_test_df  shape: {X_test_df.shape}\\n\")\n",
    "\n",
    "print(\"Label vectors:\")\n",
    "print(f\"y_train_df shape: {y_train_df.shape}\")\n",
    "print(f\"y_val_df   shape: {y_val_df.shape}\")\n",
    "print(f\"y_test_df  shape: {y_test_df.shape}\\n\")\n",
    "\n",
    "print(\"Variables available for PART 3:\")\n",
    "print(\"X_train_df, X_val_df, X_test_df\")\n",
    "print(\"y_train_df, y_val_df, y_test_df\")\n"
   ],
   "id": "ecdd55b73493676b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- PART 3: FEATURE SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize scaler\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ----------------------------\n",
    "# Fit ONLY on training data\n",
    "# ----------------------------\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Transform validation & test\n",
    "# ----------------------------\n",
    "X_val_scaled = scaler.transform(X_val_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Convert labels to NumPy arrays\n",
    "# (important for sequence creation)\n",
    "# ----------------------------\n",
    "y_train = y_train_df.values\n",
    "y_val = y_val_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# ----------------------------\n",
    "# Convert scaled features to NumPy arrays\n",
    "# ----------------------------\n",
    "X_train = np.asarray(X_train_scaled, dtype=np.float32)\n",
    "X_val = np.asarray(X_val_scaled, dtype=np.float32)\n",
    "X_test = np.asarray(X_test_scaled, dtype=np.float32)\n",
    "\n",
    "# ============================\n",
    "# OUTPUT FOR PART 4\n",
    "# ============================\n",
    "print(\"\\n=== SCALING COMPLETED ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val   shape: {X_val.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val   shape: {y_val.shape}\")\n",
    "print(f\"y_test  shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nVariables ready for PART 4:\")\n",
    "print(\"X_train, y_train\")\n",
    "print(\"X_val, y_val\")\n",
    "print(\"X_test, y_test\")\n"
   ],
   "id": "53c784b9a8eb1763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- PART 4 — CREATE SEQUENCES (WINDOW_SIZE → X, FORECAST_HORIZON → y)\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sequence creation function (time-series safe, no shuffling)\n",
    "# ------------------------------------------------------------\n",
    "def create_sequences(X, y, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "    window_size : int\n",
    "    forecast_horizon : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_seq : np.ndarray, shape (n_sequences, window_size, n_features)\n",
    "    y_seq : np.ndarray, shape (n_sequences, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    max_start = len(X) - window_size - forecast_horizon + 1\n",
    "\n",
    "    for i in range(max_start):\n",
    "        X_seq.append(X[i: i + window_size])\n",
    "        y_seq.append(y[i + window_size: i + window_size + forecast_horizon])\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create sequences for train / validation / test\n",
    "# ------------------------------------------------------------\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    X_train, y_train, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_val_seq, y_val_seq = create_sequences(\n",
    "    X_val, y_val, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    X_test, y_test, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "assert X_train_seq.shape[0] == y_train_seq.shape[0]\n",
    "assert X_val_seq.shape[0] == y_val_seq.shape[0]\n",
    "assert X_test_seq.shape[0] == y_test_seq.shape[0]\n",
    "\n",
    "assert X_train_seq.shape[1] == WINDOW_SIZE\n",
    "assert y_train_seq.shape[1] == FORECAST_HORIZON\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT VARIABLES REQUIRED FOR PART 5 (IMBALANCE HANDLING)\n",
    "# ============================================================\n",
    "print(\"=== SEQUENCE DATASETS READY ===\")\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "\n",
    "print(f\"X_val_seq   shape: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq   shape: {y_val_seq.shape}\")\n",
    "\n",
    "print(f\"X_test_seq  shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq  shape: {y_test_seq.shape}\")\n",
    "\n",
    "# Variables exposed for PART 5:\n",
    "# X_train_seq, y_train_seq\n",
    "# X_val_seq, y_val_seq\n",
    "# X_test_seq, y_test_seq\n"
   ],
   "id": "eaf8da664ba1d14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- PART 5 — IMBALANCE HANDLING (Class-Weighted)\n",
    "\n",
    "# Rules enforced:\n",
    "#   - NO oversampling / undersampling / SMOTE\n",
    "#   - Class-weighted loss ONLY\n",
    "#   - Weights computed from y_train_seq ONLY\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# y_train_seq shape: (num_samples, FORECAST_HORIZON)\n",
    "# We must compute class weights from ALL future labels\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Flatten all forecast steps into one long label vector\n",
    "y_train_flat = y_train_seq.reshape(-1)\n",
    "\n",
    "# Unique classes (must be [0,1,2])\n",
    "classes = np.unique(y_train_flat)\n",
    "\n",
    "# Compute balanced class weights\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_flat\n",
    ")\n",
    "\n",
    "# Convert to Keras-compatible dict\n",
    "class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "print(\"=== CLASS DISTRIBUTION (TRAIN ONLY) ===\")\n",
    "unique, counts = np.unique(y_train_flat, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} samples\")\n",
    "\n",
    "print(\"\\n=== CLASS WEIGHTS (Keras compatible) ===\")\n",
    "for k, v in class_weights.items():\n",
    "    print(f\"Class {k}: {v:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# VARIABLES REQUIRED FOR PART 6\n",
    "# ============================================================\n",
    "print(\"\\n=== VARIABLES READY FOR PART 6 ===\")\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq   shape: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq   shape: {y_val_seq.shape}\")\n",
    "print(f\"X_test_seq  shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq  shape: {y_test_seq.shape}\")\n",
    "print(\"class_weights:\", class_weights)\n"
   ],
   "id": "dc90bc998db96a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 12- PART 6 — Build and Train the Model\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ============================================================\n",
    "# Class weights (from PART 5)\n",
    "# ============================================================\n",
    "# class_weights = {0: 0.3377, 1: 50.9841, 2: 50.9841}\n",
    "\n",
    "class_weight_tensor = tf.constant(\n",
    "    [class_weights[0], class_weights[1], class_weights[2]],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Custom weighted sparse categorical cross-entropy\n",
    "# (supports multi-step sequence targets)\n",
    "# ============================================================\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def weighted_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: (batch, horizon)\n",
    "    y_pred: (batch, horizon, num_classes)\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Standard sparse categorical cross-entropy per timestep\n",
    "    scce = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=False\n",
    "    )  # shape: (batch, horizon)\n",
    "\n",
    "    # Gather class weights for each true label\n",
    "    weights = tf.gather(class_weight_tensor, y_true)  # (batch, horizon)\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_loss = scce * weights\n",
    "\n",
    "    return tf.reduce_mean(weighted_loss)\n"
   ],
   "id": "63bb67d52f9f988b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Architecture (Encoder → Sequence Classifier)\n",
    "\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE, len(FEATURES)))\n",
    "\n",
    "x = layers.LSTM(\n",
    "    128,\n",
    "    return_sequences=True,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(inputs)\n",
    "\n",
    "x = layers.LSTM(\n",
    "    64,\n",
    "    return_sequences=False,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(x)\n",
    "\n",
    "# Project to forecast horizon\n",
    "x = layers.Dense(FORECAST_HORIZON * 64, activation=\"relu\")(x)\n",
    "x = layers.Reshape((FORECAST_HORIZON, 64))(x)\n",
    "\n",
    "# Time-distributed classification head\n",
    "outputs = layers.TimeDistributed(\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    ")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)"
   ],
   "id": "57f665c22c5fa494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=weighted_sparse_categorical_crossentropy,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "2e8b1496e3f75e32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Callbacks\n",
    "\n",
    "cb_early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cb_reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "7f55b73cdd29ee6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[cb_early_stop, cb_reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\nTRAINING COMPLETE!\")"
   ],
   "id": "5c9625e3414766fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6036017d7bf93a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4c5bd4c62b298e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ef84a945999a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "117abfffbeea38c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "17c4230e4f2e0191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab3d86bff603109c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Model, Scaler and Report section",
   "id": "72fcbced795489e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)"
   ],
   "id": "9b2b6c543138b4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2- Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ],
   "id": "8e0bee4c8bbf1dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3- Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ],
   "id": "635d0bfeb8035e5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4- Save scaler (IMPORTANT - needed for predictions!)\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")"
   ],
   "id": "6886886e3ea704a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5- Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")"
   ],
   "id": "ad82b1de48ba9ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6- Save full history as JSON so it can be reloaded later\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "print(f\"Full history object saved to: {history_json_path}\")"
   ],
   "id": "c765bc39be8e8e17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7— Save Training Loss Plot\n",
    "\n",
    "loss_plot_path = os.path.join(log_dir, \"training_loss.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Loss plot saved to: {loss_plot_path}\")"
   ],
   "id": "2f99439ae0f34e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8— Save Accuracy Plot\n",
    "acc_plot_path = os.path.join(log_dir, \"training_accuracy.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(acc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Accuracy plot saved to: {acc_plot_path}\")"
   ],
   "id": "8c19296e5f54a835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- Save model summary and final performance\n",
    "\n",
    "# ============================\n",
    "# Evaluate once\n",
    "# ============================\n",
    "final_test_loss, final_test_acc = model.evaluate(\n",
    "    X_test_seq,\n",
    "    y_test_seq,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "\n",
    "# ============================\n",
    "# Save model summary and metrics\n",
    "# ============================\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    f.write('\\n=== FINAL PERFORMANCE ===\\n')\n",
    "    f.write(f'Final Training Loss : {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss     : {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Accuracy : {final_test_acc:.6f}\\n')"
   ],
   "id": "bc121bc7af5c454b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- Confusion matrix\n",
    "\n",
    "# Flatten horizon dimension\n",
    "y_test_pred = model.predict(X_test_seq, batch_size=BATCH_SIZE)\n",
    "y_test_pred_labels = y_test_pred.argmax(axis=-1)\n",
    "\n",
    "y_true_flat = y_test_seq.reshape(-1)\n",
    "y_pred_flat = y_test_pred_labels.reshape(-1)\n",
    "\n",
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n",
    "\n",
    "# Path to save\n",
    "cm_plot_path = os.path.join(log_dir, \"confusion_matrix.png\")\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=['0', '1', '2'],\n",
    "    yticklabels=['0', '1', '2']\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Flattened Horizon)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {cm_plot_path}\")\n"
   ],
   "id": "4d91e31c46fae6d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8cc2aab515607b8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c55f4c7120d6daf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2b247122d25086c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "80933bab65ad4e7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85ecb8f663ff1108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2ae5b508d4edf0fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac82f551c9a4aa67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "953e5f181718ed35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "73251aaa8b60f009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "423d7f0d7a1845b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7f2c7956633811f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f1d4369c39cbe50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac0b89fd65463f7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "496994b268d0c1d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PREDICTION SECTION\n",
    "N_FEATURES = len(FEATURES)  # 5\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION ON UNSEEN DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# If given_time already exists, add 5 hours\n",
    "try:\n",
    "    dt = datetime.strptime(given_time, \"%Y.%m.%d %H:%M:%S\") + timedelta(hours=5)\n",
    "except NameError:\n",
    "    # First run: initialize given_time\n",
    "    dt = datetime.strptime(\"2025.08.13 21:00:00\", \"%Y.%m.%d %H:%M:%S\")\n",
    "\n",
    "# Store back as string\n",
    "given_time = dt.strftime(\"%Y.%m.%d %H:%M:%S\")\n",
    "print(f\"\\nGiven time: {given_time}\")"
   ],
   "id": "971f9353f0017d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the index of given_time in df (not df_model)\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "given_idx = df[df['DATETIME'] == given_time].index[0]\n",
    "\n",
    "print(f\"Given time index in df: {given_idx}\")\n",
    "\n",
    "# Extract 120 candles ending at given_time\n",
    "start_idx = given_idx - WINDOW_SIZE + 1\n",
    "end_idx = given_idx + 1\n",
    "\n",
    "input_df = df.iloc[start_idx:end_idx][['DATETIME'] + FEATURES].copy()\n",
    "print(f\"Input shape (before scaling): {input_df.shape}\")\n",
    "\n",
    "# Separate DATETIME from features for scaling\n",
    "input_candles = input_df.copy()  # Keep for visualization (has DATETIME)\n",
    "input_features_only = input_df[FEATURES]  # Only features for model\n",
    "\n",
    "# Scale using the same scaler from training (only the FEATURES columns)\n",
    "input_scaled = scaler.transform(input_features_only)\n",
    "input_scaled = input_scaled.reshape(1, WINDOW_SIZE, N_FEATURES)\n",
    "\n",
    "# Predict\n",
    "predictions_proba = model.predict(input_scaled, verbose=0)  # Shape: (1, 10, 3)\n",
    "predictions_proba = predictions_proba[0]  # Shape: (10, 3)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "# Create forecast datetimes (next 10 hours after given_time)\n",
    "given_datetime = pd.to_datetime(given_time)\n",
    "forecast_datetimes = [given_datetime + pd.Timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "# Create output DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': predicted_classes,\n",
    "    'prob_0': predictions_proba[:, 0],\n",
    "    'prob_1': predictions_proba[:, 1],\n",
    "    'prob_2': predictions_proba[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "predicted_df"
   ],
   "id": "d3c938f293d42d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d72e84c79eb4918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df018d2be311917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df = input_df.tail(2).copy()",
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Actual future 10 candles\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add text labels for clarity\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n"
   ],
   "id": "9c0c6eb2f4c47e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "d5c3eaaba6bb40e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c098664f61aed263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ae1fdfa3834b946",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60a253b3c79f35a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a7f36854d09b071b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6b7e3f56a71d16c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load section",
   "id": "ac749aa8ef827b96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251226_170316.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251226_170316.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251226_170316_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "9a85da10356d2b29",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
