{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-28\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- change sections\n",
    "\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "2a4eff8556e58600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, Dropout\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow import keras\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2- Find start of dataset\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3- Select the rows required for the model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4- label_reversal_points Function\n",
    "def label_reversal_points(\n",
    "        close,\n",
    "        high=None,\n",
    "        low=None,\n",
    "        smoothing_window=31,\n",
    "        polyorder=3,\n",
    "        base_prom_factor=0.02,\n",
    "        distance=3,\n",
    "        snap_window=5,\n",
    "        min_dev_pct=0.0015,  # 0.15% minimum leg size\n",
    "        min_dev_sigma=2.0,  # >= 2x local abs-return EMA\n",
    "        vol_window=100,  # EMA window for local volatility\n",
    "        verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Label reversal points with improved accuracy.\n",
    "\n",
    "    Returns labels array of length n where:\n",
    "    0 = none, 1 = valley, 2 = peak.\n",
    "\n",
    "    Tips:\n",
    "    - For best accuracy, pass high/low arrays from your OHLCV.\n",
    "      Example: label_reversal_points(df['CLOSE'], df['HIGH'], df['LOW'])\n",
    "    - Tune min_dev_pct / min_dev_sigma to be stricter or looser on swing size.\n",
    "    \"\"\"\n",
    "    close = np.asarray(close, dtype=float)\n",
    "    n = close.size\n",
    "    if n < 3:\n",
    "        return np.zeros(n, dtype=int)\n",
    "\n",
    "    # Interpolate NaNs if any\n",
    "    if np.isnan(close).any():\n",
    "        idx = np.arange(n)\n",
    "        good = ~np.isnan(close)\n",
    "        close = close.copy()\n",
    "        close[~good] = np.interp(idx[~good], idx[good], close[good])\n",
    "\n",
    "    # Helper: simple EMA for local abs-return volatility\n",
    "    def ema(x, span):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        alpha = 2.0 / (span + 1.0)\n",
    "        out = np.empty_like(x)\n",
    "        out[0] = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]\n",
    "        return out\n",
    "\n",
    "    # Local volatility in price terms via EMA of absolute returns\n",
    "    ret = np.zeros(n)\n",
    "    ret[1:] = np.abs(np.diff(close) / np.maximum(1e-12, close[:-1]))\n",
    "    vol_absret = ema(ret, vol_window)\n",
    "    local_vol_price = vol_absret * close  # convert to price units\n",
    "\n",
    "    # Smoothing to get robust candidates\n",
    "    win = smoothing_window\n",
    "    if win >= n:\n",
    "        win = n - 1 if (n - 1) % 2 == 1 else n - 2\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    smoothed = savgol_filter(close, win, polyorder)\n",
    "\n",
    "    # Base prominence threshold\n",
    "    global_std = np.std(close) or 1.0\n",
    "    prom = global_std * base_prom_factor\n",
    "\n",
    "    # Candidate peaks/valleys on smoothed\n",
    "    peak_idx, _ = find_peaks(smoothed, distance=distance, prominence=prom)\n",
    "    val_idx, _ = find_peaks(-smoothed, distance=distance, prominence=prom)\n",
    "\n",
    "    # Prominences for tie-breaking\n",
    "    peak_prom = peak_prominences(smoothed, peak_idx)[0] if peak_idx.size else np.array([])\n",
    "    val_prom = peak_prominences(-smoothed, val_idx)[0] if val_idx.size else np.array([])\n",
    "\n",
    "    # Combine\n",
    "    candidates = []\n",
    "    for i, p in enumerate(peak_idx):\n",
    "        candidates.append((int(p), 2, float(peak_prom[i]) if peak_prom.size else 0.0))\n",
    "    for i, v in enumerate(val_idx):\n",
    "        candidates.append((int(v), 1, float(val_prom[i]) if val_prom.size else 0.0))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    if not candidates:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "        # still mark edges for completeness\n",
    "        labels[0] = 1 if close[1] > close[0] else 2\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "        return labels\n",
    "\n",
    "    # Enforce alternation (remove weaker when two same-type neighbors)\n",
    "    def enforce_alternation(ext):\n",
    "        ext = ext[:]  # list of (idx, typ, prom)\n",
    "        while True:\n",
    "            removed = False\n",
    "            i = 0\n",
    "            while i < len(ext) - 1:\n",
    "                if ext[i][1] == ext[i + 1][1]:\n",
    "                    # drop the smaller prominence\n",
    "                    if ext[i][2] < ext[i + 1][2]:\n",
    "                        ext.pop(i)\n",
    "                    else:\n",
    "                        ext.pop(i + 1)\n",
    "                    removed = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            if not removed:\n",
    "                break\n",
    "        return ext\n",
    "\n",
    "    candidates = enforce_alternation(candidates)\n",
    "\n",
    "    # SNAP: move each extreme to the true local extremum on raw close (or HIGH/LOW)\n",
    "    def snap_index(idx, typ):\n",
    "        L = max(0, idx - snap_window)\n",
    "        R = min(n, idx + snap_window + 1)\n",
    "        if high is not None and low is not None:\n",
    "            if typ == 2:  # peak\n",
    "                j = np.argmax(np.asarray(high[L:R], dtype=float))\n",
    "            else:  # valley\n",
    "                j = np.argmin(np.asarray(low[L:R], dtype=float))\n",
    "        else:\n",
    "            if typ == 2:\n",
    "                j = np.argmax(close[L:R])\n",
    "            else:\n",
    "                j = np.argmin(close[L:R])\n",
    "        return L + int(j)\n",
    "\n",
    "    snapped = []\n",
    "    seen_at = {}  # avoid duplicate indices by keeping stronger prominence\n",
    "    for idx, typ, pr in candidates:\n",
    "        j = snap_index(idx, typ)\n",
    "        key = (j, typ)\n",
    "        if key not in seen_at or pr > seen_at[key][2]:\n",
    "            seen_at[key] = (j, typ, pr)\n",
    "    snapped = sorted(seen_at.values(), key=lambda x: x[0])\n",
    "\n",
    "    # Enforce alternation again after snapping\n",
    "    snapped = enforce_alternation(snapped)\n",
    "\n",
    "    # Filter micro-legs using adaptive threshold (min % move and sigma*local_vol)\n",
    "    pruned = []\n",
    "    for idx, typ, pr in snapped:\n",
    "        if not pruned:\n",
    "            pruned.append((idx, typ, pr))\n",
    "            continue\n",
    "        prev_idx, prev_typ, prev_pr = pruned[-1]\n",
    "        # time spacing\n",
    "        if idx - prev_idx < distance:\n",
    "            # keep the more prominent of the two\n",
    "            if pr > prev_pr:\n",
    "                pruned[-1] = (idx, typ, pr)\n",
    "            continue\n",
    "        leg = abs(close[idx] - close[prev_idx])\n",
    "        # thresholds at both ends\n",
    "        thr = max(min_dev_pct * close[prev_idx],\n",
    "                  min_dev_sigma * max(local_vol_price[prev_idx], 1e-12))\n",
    "        thr = max(thr, max(min_dev_pct * close[idx],\n",
    "                           min_dev_sigma * max(local_vol_price[idx], 1e-12)))\n",
    "        if leg >= thr:\n",
    "            pruned.append((idx, typ, pr))\n",
    "        else:\n",
    "            # too small swing → drop the later point\n",
    "            continue\n",
    "\n",
    "    # One more alternation pass (paranoid) and spacing check\n",
    "    pruned = enforce_alternation(pruned)\n",
    "    final_ext = []\n",
    "    for idx, typ, pr in pruned:\n",
    "        if final_ext and idx - final_ext[-1][0] < distance:\n",
    "            # keep stronger\n",
    "            if pr > final_ext[-1][2]:\n",
    "                final_ext[-1] = (idx, typ, pr)\n",
    "        else:\n",
    "            final_ext.append((idx, typ, pr))\n",
    "\n",
    "    # Build labels\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    for idx, typ, _ in final_ext:\n",
    "        labels[idx] = typ\n",
    "\n",
    "    # Mark edges as trend boundaries for continuity\n",
    "    if labels[0] == 0:\n",
    "        labels[0] = 1 if close[min(1, n - 1)] > close[0] else 2\n",
    "    if labels[-1] == 0 and n >= 2:\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "\n",
    "    if verbose:\n",
    "        c0 = int((labels == 0).sum())\n",
    "        c1 = int((labels == 1).sum())\n",
    "        c2 = int((labels == 2).sum())\n",
    "        print(f\"labels -> 0:{c0}  1:{c1}  2:{c2}  (extrema kept: {len(final_ext)})\")\n",
    "\n",
    "    return labels\n"
   ],
   "id": "cf03646179e62d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5- baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6- Display label distribution in df_model\n",
    "label_counts = df_model['Label'].value_counts().sort_index()\n",
    "label_percentages = (df_model['Label'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "print(\"Label Distribution in df_model:\")\n",
    "print(\"-\" * 40)\n",
    "for label in sorted(df_model['Label'].unique()):\n",
    "    count = label_counts[label]\n",
    "    percentage = label_percentages[label]\n",
    "    print(f\"Class {label}: {count:,} rows ({percentage:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total rows: {len(df_model):,}\")\n"
   ],
   "id": "8e1d8ac369134288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7- Plot labels\n",
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "7a73456c9db9c4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8- PART 2 — CHRONOLOGICAL SPLITTING\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "LABEL_COL = 'Label'\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6, \"Split ratios must sum to 1\"\n",
    "assert LABEL_COL in df_model.columns, \"Label column missing\"\n",
    "for f in FEATURES:\n",
    "    assert f in df_model.columns, f\"Feature '{f}' missing\"\n",
    "\n",
    "# Ensure strict chronological order\n",
    "df_model = df_model.sort_values('DATETIME').reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute split indices\n",
    "# ----------------------------\n",
    "n_total = len(df_model)\n",
    "\n",
    "train_end = int(n_total * TRAIN_RATIO)\n",
    "val_end = train_end + int(n_total * VAL_RATIO)\n",
    "\n",
    "# ----------------------------\n",
    "# Chronological split\n",
    "# ----------------------------\n",
    "df_train = df_model.iloc[:train_end].copy()\n",
    "df_val = df_model.iloc[train_end:val_end].copy()\n",
    "df_test = df_model.iloc[val_end:].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Separate features and labels\n",
    "# (to be used in PART 3: Scaling)\n",
    "# ----------------------------\n",
    "X_train_df = df_train[FEATURES].copy()\n",
    "y_train_df = df_train[LABEL_COL].copy()\n",
    "\n",
    "X_val_df = df_val[FEATURES].copy()\n",
    "y_val_df = df_val[LABEL_COL].copy()\n",
    "\n",
    "X_test_df = df_test[FEATURES].copy()\n",
    "y_test_df = df_test[LABEL_COL].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Print outputs required for PART 3\n",
    "# ----------------------------\n",
    "print(\"=== CHRONOLOGICAL SPLIT COMPLETED ===\\n\")\n",
    "\n",
    "print(\"Rows:\")\n",
    "print(f\"Train: {len(df_train)}\")\n",
    "print(f\"Val  : {len(df_val)}\")\n",
    "print(f\"Test : {len(df_test)}\\n\")\n",
    "\n",
    "print(\"Feature matrices (for scaling):\")\n",
    "print(f\"X_train_df shape: {X_train_df.shape}\")\n",
    "print(f\"X_val_df   shape: {X_val_df.shape}\")\n",
    "print(f\"X_test_df  shape: {X_test_df.shape}\\n\")\n",
    "\n",
    "print(\"Label vectors:\")\n",
    "print(f\"y_train_df shape: {y_train_df.shape}\")\n",
    "print(f\"y_val_df   shape: {y_val_df.shape}\")\n",
    "print(f\"y_test_df  shape: {y_test_df.shape}\\n\")\n",
    "\n",
    "print(\"Variables available for PART 3:\")\n",
    "print(\"X_train_df, X_val_df, X_test_df\")\n",
    "print(\"y_train_df, y_val_df, y_test_df\")\n"
   ],
   "id": "ecdd55b73493676b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- PART 3: FEATURE SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize scaler\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ----------------------------\n",
    "# Fit ONLY on training data\n",
    "# ----------------------------\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Transform validation & test\n",
    "# ----------------------------\n",
    "X_val_scaled = scaler.transform(X_val_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Convert labels to NumPy arrays\n",
    "# (important for sequence creation)\n",
    "# ----------------------------\n",
    "y_train = y_train_df.values\n",
    "y_val = y_val_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# ----------------------------\n",
    "# Convert scaled features to NumPy arrays\n",
    "# ----------------------------\n",
    "X_train = np.asarray(X_train_scaled, dtype=np.float32)\n",
    "X_val = np.asarray(X_val_scaled, dtype=np.float32)\n",
    "X_test = np.asarray(X_test_scaled, dtype=np.float32)\n",
    "\n",
    "# ============================\n",
    "# OUTPUT FOR PART 4\n",
    "# ============================\n",
    "print(\"\\n=== SCALING COMPLETED ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val   shape: {X_val.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val   shape: {y_val.shape}\")\n",
    "print(f\"y_test  shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nVariables ready for PART 4:\")\n",
    "print(\"X_train, y_train\")\n",
    "print(\"X_val, y_val\")\n",
    "print(\"X_test, y_test\")\n"
   ],
   "id": "53c784b9a8eb1763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- PART 4 — CREATE SEQUENCES (WINDOW_SIZE → X, FORECAST_HORIZON → y)\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sequence creation function (time-series safe, no shuffling)\n",
    "# ------------------------------------------------------------\n",
    "def create_sequences(X, y, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n_samples, n_features)\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "    window_size : int\n",
    "    forecast_horizon : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_seq : np.ndarray, shape (n_sequences, window_size, n_features)\n",
    "    y_seq : np.ndarray, shape (n_sequences, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    max_start = len(X) - window_size - forecast_horizon + 1\n",
    "\n",
    "    for i in range(max_start):\n",
    "        X_seq.append(X[i: i + window_size])\n",
    "        y_seq.append(y[i + window_size: i + window_size + forecast_horizon])\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create sequences for train / validation / test\n",
    "# ------------------------------------------------------------\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    X_train, y_train, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_val_seq, y_val_seq = create_sequences(\n",
    "    X_val, y_val, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    X_test, y_test, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "assert X_train_seq.shape[0] == y_train_seq.shape[0]\n",
    "assert X_val_seq.shape[0] == y_val_seq.shape[0]\n",
    "assert X_test_seq.shape[0] == y_test_seq.shape[0]\n",
    "\n",
    "assert X_train_seq.shape[1] == WINDOW_SIZE\n",
    "assert y_train_seq.shape[1] == FORECAST_HORIZON\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT VARIABLES REQUIRED FOR PART 5 (IMBALANCE HANDLING)\n",
    "# ============================================================\n",
    "print(\"=== SEQUENCE DATASETS READY ===\")\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "\n",
    "print(f\"X_val_seq   shape: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq   shape: {y_val_seq.shape}\")\n",
    "\n",
    "print(f\"X_test_seq  shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq  shape: {y_test_seq.shape}\")\n",
    "\n",
    "# Variables exposed for PART 5:\n",
    "# X_train_seq, y_train_seq\n",
    "# X_val_seq, y_val_seq\n",
    "# X_test_seq, y_test_seq\n"
   ],
   "id": "eaf8da664ba1d14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- PART 5 — IMBALANCE HANDLING (Class-Weighted)\n",
    "\n",
    "# Rules enforced:\n",
    "#   - NO oversampling / undersampling / SMOTE\n",
    "#   - Class-weighted loss ONLY\n",
    "#   - Weights computed from y_train_seq ONLY\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# y_train_seq shape: (num_samples, FORECAST_HORIZON)\n",
    "# We must compute class weights from ALL future labels\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Flatten all forecast steps into one long label vector\n",
    "y_train_flat = y_train_seq.reshape(-1)\n",
    "\n",
    "# Unique classes (must be [0,1,2])\n",
    "classes = np.unique(y_train_flat)\n",
    "\n",
    "# Compute balanced class weights\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_flat\n",
    ")\n",
    "\n",
    "# Convert to Keras-compatible dict\n",
    "class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "print(\"=== CLASS DISTRIBUTION (TRAIN ONLY) ===\")\n",
    "unique, counts = np.unique(y_train_flat, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} samples\")\n",
    "\n",
    "print(\"\\n=== CLASS WEIGHTS (Keras compatible) ===\")\n",
    "for k, v in class_weights.items():\n",
    "    print(f\"Class {k}: {v:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# VARIABLES REQUIRED FOR PART 6\n",
    "# ============================================================\n",
    "print(\"\\n=== VARIABLES READY FOR PART 6 ===\")\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq   shape: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq   shape: {y_val_seq.shape}\")\n",
    "print(f\"X_test_seq  shape: {X_test_seq.shape}\")\n",
    "print(f\"y_test_seq  shape: {y_test_seq.shape}\")\n",
    "print(\"class_weights:\", class_weights)\n"
   ],
   "id": "dc90bc998db96a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 12- PART 6 — Build and Train the Model\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# ============================================================\n",
    "# Class weights (from PART 5)\n",
    "# ============================================================\n",
    "# class_weights = {0: 0.3377, 1: 50.9841, 2: 50.9841}\n",
    "\n",
    "class_weight_tensor = tf.constant(\n",
    "    [class_weights[0], class_weights[1], class_weights[2]],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Custom weighted sparse categorical cross-entropy\n",
    "# (supports multi-step sequence targets)\n",
    "# ============================================================\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def weighted_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: (batch, horizon)\n",
    "    y_pred: (batch, horizon, num_classes)\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Standard sparse categorical cross-entropy per timestep\n",
    "    scce = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=False\n",
    "    )  # shape: (batch, horizon)\n",
    "\n",
    "    # Gather class weights for each true label\n",
    "    weights = tf.gather(class_weight_tensor, y_true)  # (batch, horizon)\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_loss = scce * weights\n",
    "\n",
    "    return tf.reduce_mean(weighted_loss)\n"
   ],
   "id": "63bb67d52f9f988b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Architecture (Encoder → Sequence Classifier)\n",
    "\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE, len(FEATURES)))\n",
    "\n",
    "x = layers.LSTM(\n",
    "    128,\n",
    "    return_sequences=True,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(inputs)\n",
    "\n",
    "x = layers.LSTM(\n",
    "    64,\n",
    "    return_sequences=False,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(x)\n",
    "\n",
    "# Project to forecast horizon\n",
    "x = layers.Dense(FORECAST_HORIZON * 64, activation=\"relu\")(x)\n",
    "x = layers.Reshape((FORECAST_HORIZON, 64))(x)\n",
    "\n",
    "# Time-distributed classification head\n",
    "outputs = layers.TimeDistributed(\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    ")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)"
   ],
   "id": "57f665c22c5fa494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=weighted_sparse_categorical_crossentropy,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "2e8b1496e3f75e32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Callbacks\n",
    "\n",
    "cb_early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cb_reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "7f55b73cdd29ee6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[cb_early_stop, cb_reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\nTRAINING COMPLETE!\")"
   ],
   "id": "5c9625e3414766fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6036017d7bf93a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4c5bd4c62b298e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ef84a945999a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "117abfffbeea38c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "17c4230e4f2e0191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab3d86bff603109c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save Model, Scaler and Report section",
   "id": "72fcbced795489e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1- Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)"
   ],
   "id": "9b2b6c543138b4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2- Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ],
   "id": "8e0bee4c8bbf1dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3- Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ],
   "id": "635d0bfeb8035e5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4- Save scaler (IMPORTANT - needed for predictions!)\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")"
   ],
   "id": "6886886e3ea704a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5- Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")"
   ],
   "id": "ad82b1de48ba9ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6- Save full history as JSON so it can be reloaded later\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "print(f\"Full history object saved to: {history_json_path}\")"
   ],
   "id": "c765bc39be8e8e17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7— Save Training Loss Plot\n",
    "\n",
    "loss_plot_path = os.path.join(log_dir, \"training_loss.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Loss plot saved to: {loss_plot_path}\")"
   ],
   "id": "2f99439ae0f34e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8— Save Accuracy Plot\n",
    "acc_plot_path = os.path.join(log_dir, \"training_accuracy.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(acc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Accuracy plot saved to: {acc_plot_path}\")"
   ],
   "id": "8c19296e5f54a835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- Save model summary and final performance\n",
    "\n",
    "# ============================\n",
    "# Evaluate once\n",
    "# ============================\n",
    "final_test_loss, final_test_acc = model.evaluate(\n",
    "    X_test_seq,\n",
    "    y_test_seq,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "\n",
    "# ============================\n",
    "# Save model summary and metrics\n",
    "# ============================\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    f.write('\\n=== FINAL PERFORMANCE ===\\n')\n",
    "    f.write(f'Final Training Loss : {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss     : {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Accuracy : {final_test_acc:.6f}\\n')"
   ],
   "id": "bc121bc7af5c454b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- Confusion matrix\n",
    "\n",
    "# Flatten horizon dimension\n",
    "y_test_pred = model.predict(X_test_seq, batch_size=BATCH_SIZE)\n",
    "y_test_pred_labels = y_test_pred.argmax(axis=-1)\n",
    "\n",
    "y_true_flat = y_test_seq.reshape(-1)\n",
    "y_pred_flat = y_test_pred_labels.reshape(-1)\n",
    "\n",
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n",
    "\n",
    "# Path to save\n",
    "cm_plot_path = os.path.join(log_dir, \"confusion_matrix.png\")\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=['0', '1', '2'],\n",
    "    yticklabels=['0', '1', '2']\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Flattened Horizon)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {cm_plot_path}\")\n"
   ],
   "id": "4d91e31c46fae6d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8cc2aab515607b8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c55f4c7120d6daf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2b247122d25086c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "80933bab65ad4e7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85ecb8f663ff1108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2ae5b508d4edf0fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac82f551c9a4aa67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "953e5f181718ed35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "73251aaa8b60f009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "423d7f0d7a1845b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7f2c7956633811f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f1d4369c39cbe50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac0b89fd65463f7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "496994b268d0c1d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "N_FEATURES = len(FEATURES)  # 5\n",
    "# ============================================================================\n",
    "# PREDICTION SECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION ON UNSEEN DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# If given_time already exists, add 5 hours\n",
    "try:\n",
    "    dt = datetime.strptime(given_time, \"%Y.%m.%d %H:%M:%S\") + timedelta(hours=5)\n",
    "except NameError:\n",
    "    # First run: initialize given_time\n",
    "    dt = datetime.strptime(\"2025.08.13 21:00:00\", \"%Y.%m.%d %H:%M:%S\")\n",
    "\n",
    "# Store back as string\n",
    "given_time = dt.strftime(\"%Y.%m.%d %H:%M:%S\")\n",
    "print(f\"\\nGiven time: {given_time}\")"
   ],
   "id": "971f9353f0017d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Find the index of given_time in df (not df_model)\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "given_idx = df[df['DATETIME'] == given_time].index[0]\n",
    "\n",
    "print(f\"Given time index in df: {given_idx}\")\n",
    "\n",
    "# Extract 120 candles ending at given_time\n",
    "start_idx = given_idx - WINDOW_SIZE + 1\n",
    "end_idx = given_idx + 1\n",
    "\n",
    "input_df = df.iloc[start_idx:end_idx][['DATETIME'] + FEATURES].copy()\n",
    "print(f\"Input shape (before scaling): {input_df.shape}\")\n",
    "\n",
    "# Separate DATETIME from features for scaling\n",
    "input_candles = input_df.copy()  # Keep for visualization (has DATETIME)\n",
    "input_features_only = input_df[FEATURES]  # Only features for model\n",
    "\n",
    "# Scale using the same scaler from training (only the FEATURES columns)\n",
    "input_scaled = scaler.transform(input_features_only)\n",
    "input_scaled = input_scaled.reshape(1, WINDOW_SIZE, N_FEATURES)\n",
    "\n",
    "# Predict\n",
    "predictions_proba = model.predict(input_scaled, verbose=0)  # Shape: (1, 10, 3)\n",
    "predictions_proba = predictions_proba[0]  # Shape: (10, 3)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "# Create forecast datetimes (next 10 hours after given_time)\n",
    "given_datetime = pd.to_datetime(given_time)\n",
    "forecast_datetimes = [given_datetime + pd.Timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "# Create output DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': predicted_classes,\n",
    "    'prob_0': predictions_proba[:, 0],\n",
    "    'prob_1': predictions_proba[:, 1],\n",
    "    'prob_2': predictions_proba[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "predicted_df"
   ],
   "id": "d3c938f293d42d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d72e84c79eb4918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df018d2be311917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(2).copy()"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "9c0c6eb2f4c47e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "d5c3eaaba6bb40e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c098664f61aed263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ae1fdfa3834b946",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60a253b3c79f35a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a7f36854d09b071b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6b7e3f56a71d16c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load section",
   "id": "ac749aa8ef827b96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251225_234944.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251225_234944.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251225_234944_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "9a85da10356d2b29",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
