{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Part 3 — Chronological splitting (70% train, 15% val, 15% test)\n",
    "# --------------------------\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Basic meta\n",
    "n_rows = len(df_model)\n",
    "print(f\"Total rows in df_model: {n_rows}\")\n",
    "\n",
    "# Chronological split boundaries\n",
    "train_end = int(np.floor(0.70 * n_rows))\n",
    "val_end = int(np.floor(0.85 * n_rows))  # 70% -> 85% -> 100%\n",
    "\n",
    "# Slice datasets (pure chronological, no shuffling)\n",
    "train_df = df_model.iloc[:train_end].reset_index(drop=True)\n",
    "val_df = df_model.iloc[train_end:val_end].reset_index(drop=True)\n",
    "test_df = df_model.iloc[val_end:].reset_index(drop=True)\n"
   ],
   "id": "ad510608238e2121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 4 — Scaling using only training data (StandardScaler)\n",
    "# --- Prepare arrays of features for scaler and later sequence building ---\n",
    "\n",
    "X_train_raw = train_df[FEATURES].astype(float).copy()  # DataFrame\n",
    "X_val_raw = val_df[FEATURES].astype(float).copy()\n",
    "X_test_raw = test_df[FEATURES].astype(float).copy()\n",
    "\n",
    "# --- Fit scaler on training features ONLY ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw.values)  # fit on numpy array from training set only\n",
    "\n",
    "# Transform all splits using the training-fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train_raw.values)  # numpy array shape (n_train, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_raw.values)\n",
    "X_test_scaled = scaler.transform(X_test_raw.values)\n",
    "\n",
    "# If user later wants DataFrames back (with same columns), create them:\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=FEATURES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=FEATURES)\n",
    "\n",
    "# --- Label arrays for imbalance handling (these are per-row labels) ---\n",
    "y_train_labels = train_df['Label'].astype(int).values\n",
    "y_val_labels = val_df['Label'].astype(int).values\n",
    "y_test_labels = test_df['Label'].astype(int).values\n",
    "\n",
    "# --- Basic class distribution info (useful for imbalance handling) ---\n",
    "unique_classes = np.array(sorted(df_model['Label'].dropna().unique())).astype(int)\n",
    "\n",
    "train_class_counts = {int(c): int((y_train_labels == c).sum()) for c in unique_classes}\n",
    "val_class_counts = {int(c): int((y_val_labels == c).sum()) for c in unique_classes}\n",
    "test_class_counts = {int(c): int((y_test_labels == c).sum()) for c in unique_classes}\n",
    "\n",
    "train_total = len(y_train_labels)\n",
    "val_total = len(y_val_labels)\n",
    "test_total = len(y_test_labels)\n",
    "\n",
    "train_class_percent = {c: (count / train_total) * 100.0 for c, count in train_class_counts.items()}\n",
    "\n",
    "# --- Compute initial class weights (sklearn balanced weighting) on the training labels\n",
    "# This gives a starting point; given extreme imbalance you will likely combine resampling + weights.\n",
    "classes_for_weights = np.array(sorted(np.unique(y_train_labels)))\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes_for_weights, y=y_train_labels)\n",
    "class_weight_dict = {int(cls): float(w) for cls, w in zip(classes_for_weights, class_weights)}\n",
    "\n",
    "# For convenience, also expose them in the current namespace (no-op if already present)\n",
    "_train_idx = (0, train_end)\n",
    "_val_idx = (train_end, val_end)\n",
    "_test_idx = (val_end, n_rows)\n",
    "\n",
    "# End of this code cell — ready for Part 5.\n"
   ],
   "id": "3afdc96547207ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 6 — Create sequences (WINDOW_SIZE -> X, FORECAST_HORIZON -> y)\n",
    "\n",
    "\n",
    "# --- Helper: sliding-window sequence creator for one split ---\n",
    "def create_sequences_from_split(X_split, y_split, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    X_split: 2D array (n_rows, n_features)\n",
    "    y_split: 1D array (n_rows,) integer labels\n",
    "    Returns:\n",
    "      X_seq: (n_sequences, window_size, n_features)\n",
    "      y_seq: (n_sequences, forecast_horizon) ints\n",
    "    \"\"\"\n",
    "    X_arr = np.asarray(X_split)\n",
    "    y_arr = np.asarray(y_split)\n",
    "\n",
    "    if X_arr.ndim != 2:\n",
    "        raise ValueError(f\"X_split must be 2D array, got shape {X_arr.shape}\")\n",
    "    if y_arr.ndim != 1:\n",
    "        raise ValueError(f\"y_split must be 1D array, got shape {y_arr.shape}\")\n",
    "    if X_arr.shape[0] != y_arr.shape[0]:\n",
    "        raise ValueError(f\"X and y must have same first-dimension length. X: {X_arr.shape[0]}, y: {y_arr.shape[0]}\")\n",
    "\n",
    "    n_rows = X_arr.shape[0]\n",
    "    last_start = n_rows - window_size - forecast_horizon  # inclusive max start index\n",
    "    if last_start < 0:\n",
    "        # Not enough rows to construct a single sequence\n",
    "        return np.empty((0, window_size, X_arr.shape[1])), np.empty((0, forecast_horizon), dtype=int)\n",
    "\n",
    "    n_sequences = last_start + 1\n",
    "    X_seq = np.empty((n_sequences, window_size, X_arr.shape[1]), dtype=X_arr.dtype)\n",
    "    y_seq = np.empty((n_sequences, forecast_horizon), dtype=int)\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        X_seq[i] = X_arr[i: i + window_size]\n",
    "        y_seq[i] = y_arr[i + window_size: i + window_size + forecast_horizon]\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "# --- Create sequences for each split ---\n",
    "X_train_seq, y_train_seq = create_sequences_from_split(X_train_scaled, y_train_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_val_seq, y_val_seq = create_sequences_from_split(X_val_scaled, y_val_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_test_seq, y_test_seq = create_sequences_from_split(X_test_scaled, y_test_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "\n",
    "# --- One-hot encode multi-step labels ---\n",
    "# Determine number of classes from training labels (safe default: 3 classes {0,1,2})\n",
    "classes_in_train = np.unique(y_train_seq) if y_train_seq.size > 0 else np.array([0, 1, 2])\n",
    "n_classes = int(max(classes_in_train.max(), 2) + 1)  # ensures at least 3 classes (0..2)\n",
    "\n",
    "\n",
    "# Convert to categorical: result shape (n_sequences, FORECAST_HORIZON, n_classes)\n",
    "def one_hot_multi_step(y_seq, n_classes):\n",
    "    if y_seq.size == 0:\n",
    "        return np.empty((0, y_seq.shape[1], n_classes), dtype=np.float32)\n",
    "    # to_categorical works on flattened array, then reshape\n",
    "    flat = to_categorical(y_seq.ravel(), num_classes=n_classes)\n",
    "    return flat.reshape((y_seq.shape[0], y_seq.shape[1], n_classes))\n",
    "\n",
    "\n",
    "y_train_seq_cat = one_hot_multi_step(y_train_seq, n_classes)\n",
    "y_val_seq_cat = one_hot_multi_step(y_val_seq, n_classes)\n",
    "y_test_seq_cat = one_hot_multi_step(y_test_seq, n_classes)\n"
   ],
   "id": "a3d60357fcba0b3a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
