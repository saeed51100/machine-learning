{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Part 3 — Chronological splitting (70% train, 15% val, 15% test)\n",
    "# --------------------------\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Basic meta\n",
    "n_rows = len(df_model)\n",
    "print(f\"Total rows in df_model: {n_rows}\")\n",
    "\n",
    "# Chronological split boundaries\n",
    "train_end = int(np.floor(0.70 * n_rows))\n",
    "val_end = int(np.floor(0.85 * n_rows))  # 70% -> 85% -> 100%\n",
    "\n",
    "# Slice datasets (pure chronological, no shuffling)\n",
    "train_df = df_model.iloc[:train_end].reset_index(drop=True)\n",
    "val_df = df_model.iloc[train_end:val_end].reset_index(drop=True)\n",
    "test_df = df_model.iloc[val_end:].reset_index(drop=True)\n"
   ],
   "id": "ad510608238e2121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 4 — Scaling using only training data (StandardScaler)\n",
    "# --- Prepare arrays of features for scaler and later sequence building ---\n",
    "\n",
    "X_train_raw = train_df[FEATURES].astype(float).copy()  # DataFrame\n",
    "X_val_raw = val_df[FEATURES].astype(float).copy()\n",
    "X_test_raw = test_df[FEATURES].astype(float).copy()\n",
    "\n",
    "# --- Fit scaler on training features ONLY ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw.values)  # fit on numpy array from training set only\n",
    "\n",
    "# Transform all splits using the training-fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train_raw.values)  # numpy array shape (n_train, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_raw.values)\n",
    "X_test_scaled = scaler.transform(X_test_raw.values)\n",
    "\n",
    "# If user later wants DataFrames back (with same columns), create them:\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=FEATURES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=FEATURES)\n",
    "\n",
    "# --- Label arrays for imbalance handling (these are per-row labels) ---\n",
    "y_train_labels = train_df['Label'].astype(int).values\n",
    "y_val_labels = val_df['Label'].astype(int).values\n",
    "y_test_labels = test_df['Label'].astype(int).values\n",
    "\n",
    "# --- Basic class distribution info (useful for imbalance handling) ---\n",
    "unique_classes = np.array(sorted(df_model['Label'].dropna().unique())).astype(int)\n",
    "\n",
    "train_class_counts = {int(c): int((y_train_labels == c).sum()) for c in unique_classes}\n",
    "val_class_counts = {int(c): int((y_val_labels == c).sum()) for c in unique_classes}\n",
    "test_class_counts = {int(c): int((y_test_labels == c).sum()) for c in unique_classes}\n",
    "\n",
    "train_total = len(y_train_labels)\n",
    "val_total = len(y_val_labels)\n",
    "test_total = len(y_test_labels)\n",
    "\n",
    "train_class_percent = {c: (count / train_total) * 100.0 for c, count in train_class_counts.items()}\n",
    "\n",
    "# --- Compute initial class weights (sklearn balanced weighting) on the training labels\n",
    "# This gives a starting point; given extreme imbalance you will likely combine resampling + weights.\n",
    "classes_for_weights = np.array(sorted(np.unique(y_train_labels)))\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes_for_weights, y=y_train_labels)\n",
    "class_weight_dict = {int(cls): float(w) for cls, w in zip(classes_for_weights, class_weights)}\n",
    "\n",
    "# For convenience, also expose them in the current namespace (no-op if already present)\n",
    "_train_idx = (0, train_end)\n",
    "_val_idx = (train_end, val_end)\n",
    "_test_idx = (val_end, n_rows)\n",
    "\n",
    "# End of this code cell — ready for Part 5.\n"
   ],
   "id": "3afdc96547207ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# PART 5 — IMBALANCE HANDLING\n",
    "# -----------------------------\n",
    "# Place parameters here so you can manage imbalance handling behavior easily.\n",
    "IMBALANCE_STRATEGY = \"combined\"  # options: \"none\", \"class_weight\", \"oversample\", \"undersample\", \"smote\", \"combined\"\n",
    "RANDOM_STATE = 42\n",
    "# Only used for undersampling the majority class (0). Fraction of majority to KEEP (0-1).\n",
    "MAJORITY_KEEP_FRAC = 0.5\n",
    "# After undersampling, SMOTE will upsample minority classes to reach this relative proportion of the\n",
    "# (new) majority class. e.g. minority_target_ratio = 0.5 -> each minority class will be ~50% of majority.\n",
    "MINORITY_TARGET_RATIO = 0.6\n",
    "\n",
    "# NOTE (important): resampling is applied ONLY to the TRAIN split. Validation and test splits remain unchanged.\n",
    "# Also: resampling breaks chronological ordering inside the training set. That is usually acceptable for models\n",
    "# that only learn patterns (not for models where the exact chronology of training samples matters). You already\n",
    "# performed chronological splitting; we keep val/test untouched.\n",
    "\n",
    "# Try to import imblearn; if not available, fallback to sklearn.utils.resample for simple oversampling/undersampling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare training arrays from existing variables\n",
    "# -----------------------------\n",
    "# Expectations (you said these exist earlier):\n",
    "# - train_df (pandas DataFrame) with a 'Label' column for training slice\n",
    "# - X_train_scaled, X_val_scaled, X_test_scaled: numpy arrays of scaled features (train/val/test)\n",
    "# - scaler: fitted scaler instance (kept for later parts)\n",
    "# If your variable names differ, adapt below accordingly.\n",
    "\n",
    "# create y_train_labels from train_df\n",
    "y_train_labels = train_df['Label'].astype(int).to_numpy()\n",
    "# make sure X_train_scaled is a numpy array\n",
    "X_train_scaled = np.asarray(X_train_scaled)\n",
    "X_val_scaled = np.asarray(X_val_scaled)\n",
    "X_test_scaled = np.asarray(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame view of scaled train features (useful later and requested)\n",
    "# FEATURES list is provided in your global hyperparameters\n",
    "try:\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "except Exception:\n",
    "    # if shape mismatch or column names not matching, fallback to generic column names\n",
    "    n_feats = X_train_scaled.shape[1]\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f\"f{i}\" for i in range(n_feats)])\n",
    "\n",
    "# Compute and print initial train class counts\n",
    "train_class_counts = dict(Counter(y_train_labels))\n",
    "print(\"Initial TRAIN class counts:\", train_class_counts)\n",
    "\n",
    "# Compute sklearn balanced class weights (reference)\n",
    "classes = np.unique(y_train_labels)\n",
    "class_weight_vals = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_labels)\n",
    "class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weight_vals)}\n",
    "print(\"Initial computed class_weight (sklearn 'balanced'):\", class_weight_dict)\n",
    "\n",
    "# Prepare placeholders for resampled outputs\n",
    "X_train_resampled = X_train_scaled.copy()\n",
    "y_train_resampled = y_train_labels.copy()\n",
    "sample_weight_train_resampled = None\n",
    "\n",
    "# -----------------------------\n",
    "# Implement strategies\n",
    "# -----------------------------\n",
    "if IMBALANCE_STRATEGY == \"none\":\n",
    "    # Do nothing — just use the original training arrays\n",
    "    print(\"IMBALANCE_STRATEGY = 'none' -> no resampling performed. Using original training data.\")\n",
    "\n",
    "elif IMBALANCE_STRATEGY == \"class_weight\":\n",
    "    # Do not change the training set; use class weights at training time.\n",
    "    print(\"IMBALANCE_STRATEGY = 'class_weight' -> no resampling performed. Use class weights during model.fit().\")\n",
    "\n",
    "elif IMBALANCE_STRATEGY in (\"oversample\", \"undersample\", \"smote\", \"combined\"):\n",
    "    print(f\"IMBALANCE_STRATEGY = '{IMBALANCE_STRATEGY}' -> performing resampling on TRAIN only.\")\n",
    "    # If imblearn available, use its resamplers for better behavior. Otherwise fall back to manual resampling.\n",
    "    if IMBLEARN_AVAILABLE:\n",
    "        # Build index array so we can easily map back if needed\n",
    "        train_idx = np.arange(len(y_train_labels))\n",
    "\n",
    "        if IMBALANCE_STRATEGY == \"oversample\":\n",
    "            ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = ros.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"RandomOverSampler completed.\")\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"undersample\":\n",
    "            # Reduce majority class (0) to MAJORITY_KEEP_FRAC of its original count while keeping minorities intact.\n",
    "            # Create sampling_strategy dict for RandomUnderSampler\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "            sampling_strategy = {0: target_maj}\n",
    "            # keep other classes as-is (imblearn will remove entries only for classes in sampling_strategy)\n",
    "            rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"RandomUnderSampler completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"smote\":\n",
    "            # SMOTE can be unstable with extremely imbalanced data; we will first undersample majority a little, then SMOTE.\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "            # Step 1: undersample majority to make SMOTE viable\n",
    "            rus = RandomUnderSampler(sampling_strategy={0: target_maj}, random_state=RANDOM_STATE)\n",
    "            X_tmp, y_tmp = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            # Step 2: SMOTE to balance minorities relative to the new majority\n",
    "            # Build sampling_strategy for SMOTE: set each minority to MINORITY_TARGET_RATIO * target_maj\n",
    "            sampling_strategy_smote = {}\n",
    "            for cls, _ in Counter(y_tmp).items():\n",
    "                if cls == 0:\n",
    "                    continue\n",
    "                sampling_strategy_smote[int(cls)] = int(max(1, target_maj * MINORITY_TARGET_RATIO))\n",
    "            sm = SMOTE(sampling_strategy=sampling_strategy_smote, random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = sm.fit_resample(X_tmp, y_tmp)\n",
    "            print(\"SMOTE (with initial undersample) completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"combined\":\n",
    "            # Combined strategy: undersample majority to MAJORITY_KEEP_FRAC, then SMOTE minority classes up to\n",
    "            # MINORITY_TARGET_RATIO of the new majority class.\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "\n",
    "            # 1) Undersample majority\n",
    "            rus = RandomUnderSampler(sampling_strategy={0: target_maj}, random_state=RANDOM_STATE)\n",
    "            X_tmp, y_tmp = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"Step 1: Undersample majority done. Counts:\", dict(Counter(y_tmp)))\n",
    "\n",
    "            # 2) SMOTE -> target minority counts relative to new majority\n",
    "            sampling_strategy_smote = {}\n",
    "            for cls in np.unique(y_tmp):\n",
    "                if int(cls) == 0:\n",
    "                    continue\n",
    "                sampling_strategy_smote[int(cls)] = int(max(1, target_maj * MINORITY_TARGET_RATIO))\n",
    "\n",
    "            # If sampling_strategy_smote would result in fewer minority samples than already present, set to 'auto'\n",
    "            # to avoid trying to downsample with SMOTE (which cannot downsample).\n",
    "            # Build final smote strategy only for classes where desired > present\n",
    "            final_smote_strategy = {}\n",
    "            for cls, desired in sampling_strategy_smote.items():\n",
    "                present = Counter(y_tmp).get(cls, 0)\n",
    "                if desired > present:\n",
    "                    final_smote_strategy[cls] = desired\n",
    "\n",
    "            if len(final_smote_strategy) == 0:\n",
    "                # Nothing to SMOTE; keep undersampled version\n",
    "                X_train_resampled, y_train_resampled = X_tmp, y_tmp\n",
    "                print(\"No SMOTE required after undersampling (minorities already at/above target).\")\n",
    "            else:\n",
    "                sm = SMOTE(sampling_strategy=final_smote_strategy, random_state=RANDOM_STATE)\n",
    "                X_train_resampled, y_train_resampled = sm.fit_resample(X_tmp, y_tmp)\n",
    "                print(\"Step 2: SMOTE completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported IMBALANCE_STRATEGY with imblearn.\")\n",
    "\n",
    "    else:\n",
    "        # imblearn not available — fallback to simpler resampling using sklearn.utils.resample\n",
    "        print(\"imblearn not installed. Falling back to simple resampling with sklearn.utils.resample. \"\n",
    "              \"This is less flexible for SMOTE, so only basic oversample/undersample are available.\")\n",
    "        if IMBALANCE_STRATEGY == \"oversample\":\n",
    "            # naive oversample minority classes to match majority count (not ideal but workable)\n",
    "            df_train_tmp = pd.DataFrame(X_train_scaled)\n",
    "            df_train_tmp['Label'] = y_train_labels\n",
    "            majority_class = df_train_tmp[df_train_tmp['Label'] == 0]\n",
    "            majority_count = len(majority_class)\n",
    "            resampled_parts = [majority_class]\n",
    "            for cls in np.unique(y_train_labels):\n",
    "                if cls == 0:\n",
    "                    continue\n",
    "                cls_df = df_train_tmp[df_train_tmp['Label'] == cls]\n",
    "                upsampled = resample(cls_df, replace=True, n_samples=majority_count, random_state=RANDOM_STATE)\n",
    "                resampled_parts.append(upsampled)\n",
    "            df_res = pd.concat(resampled_parts).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "            y_train_resampled = df_res['Label'].to_numpy()\n",
    "            X_train_resampled = df_res.drop(columns=['Label']).to_numpy()\n",
    "            print(\"Fallback oversample completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"undersample\":\n",
    "            df_train_tmp = pd.DataFrame(X_train_scaled)\n",
    "            df_train_tmp['Label'] = y_train_labels\n",
    "            majority_class = df_train_tmp[df_train_tmp['Label'] == 0]\n",
    "            target_maj = int(len(majority_class) * MAJORITY_KEEP_FRAC)\n",
    "            majority_down = resample(majority_class, replace=False, n_samples=target_maj, random_state=RANDOM_STATE)\n",
    "            others = df_train_tmp[df_train_tmp['Label'] != 0]\n",
    "            df_res = pd.concat([majority_down, others]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "            y_train_resampled = df_res['Label'].to_numpy()\n",
    "            X_train_resampled = df_res.drop(columns=['Label']).to_numpy()\n",
    "            print(\"Fallback undersample completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(\"Requested strategy requires imblearn but it's not installed. \"\n",
    "                               \"Install 'imblearn' to use SMOTE or combined strategies.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown IMBALANCE_STRATEGY: {IMBALANCE_STRATEGY}\")\n",
    "\n",
    "# -----------------------------\n",
    "# After resampling: compute sample weights (useful in model.fit if you want per-sample weighting)\n",
    "# -----------------------------\n",
    "# Compute class weights on the resampled training set (so the model still sees higher weight for minority if desired)\n",
    "resampled_classes = np.unique(y_train_resampled)\n",
    "resampled_class_weight_vals = compute_class_weight(class_weight=\"balanced\", classes=resampled_classes,\n",
    "                                                   y=y_train_resampled)\n",
    "resampled_class_weight_dict = {int(c): float(w) for c, w in zip(resampled_classes, resampled_class_weight_vals)}\n",
    "print(\"Resampled TRAIN class counts:\", dict(Counter(y_train_resampled)))\n",
    "print(\"Resampled computed class_weight (sklearn 'balanced'):\", resampled_class_weight_dict)\n",
    "\n",
    "# Build per-sample weights array for the resampled training set (useful if you want to pass sample_weight to model.fit)\n",
    "sample_weight_train_resampled = np.array([resampled_class_weight_dict[int(lbl)] for lbl in y_train_resampled],\n",
    "                                         dtype=float)\n",
    "\n"
   ],
   "id": "1ad0897eaa68dcc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 6 — Create sequences (WINDOW_SIZE -> X, FORECAST_HORIZON -> y)\n",
    "\n",
    "\n",
    "# --- Helper: sliding-window sequence creator for one split ---\n",
    "def create_sequences_from_split(X_split, y_split, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    X_split: 2D array (n_rows, n_features)\n",
    "    y_split: 1D array (n_rows,) integer labels\n",
    "    Returns:\n",
    "      X_seq: (n_sequences, window_size, n_features)\n",
    "      y_seq: (n_sequences, forecast_horizon) ints\n",
    "    \"\"\"\n",
    "    X_arr = np.asarray(X_split)\n",
    "    y_arr = np.asarray(y_split)\n",
    "\n",
    "    if X_arr.ndim != 2:\n",
    "        raise ValueError(f\"X_split must be 2D array, got shape {X_arr.shape}\")\n",
    "    if y_arr.ndim != 1:\n",
    "        raise ValueError(f\"y_split must be 1D array, got shape {y_arr.shape}\")\n",
    "    if X_arr.shape[0] != y_arr.shape[0]:\n",
    "        raise ValueError(f\"X and y must have same first-dimension length. X: {X_arr.shape[0]}, y: {y_arr.shape[0]}\")\n",
    "\n",
    "    n_rows = X_arr.shape[0]\n",
    "    last_start = n_rows - window_size - forecast_horizon  # inclusive max start index\n",
    "    if last_start < 0:\n",
    "        # Not enough rows to construct a single sequence\n",
    "        return np.empty((0, window_size, X_arr.shape[1])), np.empty((0, forecast_horizon), dtype=int)\n",
    "\n",
    "    n_sequences = last_start + 1\n",
    "    X_seq = np.empty((n_sequences, window_size, X_arr.shape[1]), dtype=X_arr.dtype)\n",
    "    y_seq = np.empty((n_sequences, forecast_horizon), dtype=int)\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        X_seq[i] = X_arr[i: i + window_size]\n",
    "        y_seq[i] = y_arr[i + window_size: i + window_size + forecast_horizon]\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "# --- Create sequences for each split ---\n",
    "X_train_seq, y_train_seq = create_sequences_from_split(\n",
    "    X_train_resampled,   # ✅ Use resampled data\n",
    "    y_train_resampled,   # ✅ Use resampled labels\n",
    "    WINDOW_SIZE,\n",
    "    FORECAST_HORIZON\n",
    ")\n",
    "X_val_seq, y_val_seq = create_sequences_from_split(\n",
    "    X_val_scaled,\n",
    "    y_val_labels,\n",
    "    WINDOW_SIZE,\n",
    "    FORECAST_HORIZON\n",
    ")\n",
    "X_test_seq, y_test_seq = create_sequences_from_split(\n",
    "    X_test_scaled,\n",
    "    y_test_labels,\n",
    "    WINDOW_SIZE,\n",
    "    FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "# --- One-hot encode multi-step labels ---\n",
    "# Determine number of classes from training labels (safe default: 3 classes {0,1,2})\n",
    "classes_in_train = np.unique(y_train_seq) if y_train_seq.size > 0 else np.array([0, 1, 2])\n",
    "n_classes = int(max(classes_in_train.max(), 2) + 1)  # ensures at least 3 classes (0..2)\n",
    "\n",
    "\n",
    "# Convert to categorical: result shape (n_sequences, FORECAST_HORIZON, n_classes)\n",
    "def one_hot_multi_step(y_seq, n_classes):\n",
    "    if y_seq.size == 0:\n",
    "        return np.empty((0, y_seq.shape[1], n_classes), dtype=np.float32)\n",
    "    # to_categorical works on flattened array, then reshape\n",
    "    flat = to_categorical(y_seq.ravel(), num_classes=n_classes)\n",
    "    return flat.reshape((y_seq.shape[0], y_seq.shape[1], n_classes))\n",
    "\n",
    "\n",
    "y_train_seq_cat = one_hot_multi_step(y_train_seq, n_classes)\n",
    "y_val_seq_cat = one_hot_multi_step(y_val_seq, n_classes)\n",
    "y_test_seq_cat = one_hot_multi_step(y_test_seq, n_classes)\n"
   ],
   "id": "a3d60357fcba0b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Part 7 — Build, Train and Evaluate the model\n",
    "# Assumes the following variables are already present in the namespace:\n",
    "# X_train_seq, y_train_seq, y_train_seq_cat\n",
    "# X_val_seq,   y_val_seq,   y_val_seq_cat\n",
    "# X_test_seq,  y_test_seq,  y_test_seq_cat\n",
    "# and constants: WINDOW_SIZE = 120, FORECAST_HORIZON = 5, FEATURES = ['OPEN','HIGH','LOW','CLOSE','TICKVOL']\n",
    "\n",
    "\n",
    "N_CLASSES = y_train_seq_cat.shape[-1]  # should be 3\n",
    "TIMESTEPS = y_train_seq_cat.shape[1]  # should be FORECAST_HORIZON\n",
    "\n",
    "# ---------------------------\n",
    "# Create timestep-aware sample weights to handle extreme imbalance\n",
    "# ---------------------------\n",
    "# Compute global class frequencies across all samples and timesteps in training set\n",
    "class_counts = np.sum(y_train_seq_cat, axis=(0, 1))  # shape (n_classes,)\n",
    "total_labels = np.sum(class_counts)\n",
    "print(\"Train class counts (sum across samples and timesteps):\", class_counts)\n",
    "\n",
    "# Avoid division by zero\n",
    "eps = 1e-8\n",
    "# Inverse-frequency weighting (normalized)\n",
    "inv_freq = (total_labels / (class_counts + eps))\n",
    "# normalize so that mean weight == 1 (keeps loss scale stable)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "print(\"Per-class inverse-frequency weights:\", inv_freq)\n",
    "\n",
    "\n",
    "# Build sample_weight arrays with shape (n_samples, timesteps)\n",
    "def build_timestep_sample_weights(y_cat, inv_freq_array):\n",
    "    # y_cat shape: (n_samples, timesteps, n_classes)\n",
    "    n_samples, timesteps, n_classes = y_cat.shape\n",
    "    sw = np.zeros((n_samples, timesteps), dtype=np.float32)\n",
    "    # argmax to get the true class index per sample/timestep\n",
    "    true_classes = np.argmax(y_cat, axis=-1)  # shape (n_samples, timesteps)\n",
    "    for c in range(n_classes):\n",
    "        sw[true_classes == c] = inv_freq_array[c]\n",
    "    return sw\n",
    "\n",
    "\n",
    "sample_weight_train = build_timestep_sample_weights(y_train_seq_cat, inv_freq)\n",
    "sample_weight_val = build_timestep_sample_weights(y_val_seq_cat, inv_freq)\n",
    "sample_weight_test = build_timestep_sample_weights(y_test_seq_cat, inv_freq)\n",
    "\n",
    "print(\"sample_weight_train shape:\", sample_weight_train.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Build the model\n",
    "# Seq2Seq-ish model: Encoder LSTM -> RepeatVector -> Decoder LSTM (return_sequences=True) -> TimeDistributed(Dense)\n",
    "# ---------------------------\n",
    "INPUT_SHAPE = X_train_seq.shape[1:]  # (WINDOW_SIZE, n_features)\n",
    "EMBED_DIM = 128\n",
    "ENC_UNITS = 128\n",
    "DEC_UNITS = 128\n",
    "DROPOUT = 0.2\n",
    "\n",
    "inputs = layers.Input(shape=INPUT_SHAPE, name='inputs')\n",
    "# optional masking if there are padded sequences; here probably not needed but harmless\n",
    "x = layers.Masking(mask_value=0.0)(inputs)\n",
    "# Encoder\n",
    "x = layers.Bidirectional(layers.LSTM(ENC_UNITS, return_sequences=False, dropout=DROPOUT), name='encoder_bi')(x)\n",
    "# Project to embedding\n",
    "x = layers.Dense(EMBED_DIM, activation='relu', name='encoder_dense')(x)\n",
    "# Repeat for forecast horizon\n",
    "x = layers.RepeatVector(TIMESTEPS, name='repeat_vector')(x)\n",
    "# Decoder\n",
    "x = layers.LSTM(DEC_UNITS, return_sequences=True, dropout=DROPOUT, name='decoder_lstm')(x)\n",
    "# Optional TimeDistributed intermediate dense\n",
    "x = layers.TimeDistributed(layers.Dense(64, activation='relu'), name='td_dense')(x)\n",
    "# Final classification per timestep\n",
    "outputs = layers.TimeDistributed(layers.Dense(N_CLASSES, activation='softmax'), name='td_softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name='seq2seq_reversal_classifier')\n",
    "model.summary()\n",
    "# ---------------------------\n",
    "# Compile\n",
    "# ---------------------------\n",
    "LR = 1e-3\n",
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# Callbacks\n",
    "# ---------------------------\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Fit\n",
    "# Note: sample_weight for sequence outputs should be shape (n_samples, timesteps)\n",
    "# ---------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq_cat,\n",
    "    validation_data=(X_val_seq, y_val_seq_cat, sample_weight_val),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nTRAINING COMPLETE!\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate on test set\n",
    "# ---------------------------\n",
    "print('\\nEvaluating on test set...')\n",
    "eval_results = model.evaluate(X_test_seq, y_test_seq_cat, sample_weight=sample_weight_test, verbose=2)\n",
    "print('Test loss/metrics:', eval_results)\n",
    "\n",
    "# ---------------------------\n",
    "# Detailed classification report (flatten timesteps)\n",
    "# ---------------------------\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq, batch_size=BATCH_SIZE)\n",
    "# y_pred_proba shape: (n_samples, timesteps, n_classes)\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1).reshape(-1)\n",
    "y_true = np.argmax(y_test_seq_cat, axis=-1).reshape(-1)\n",
    "\n",
    "print('\\nClassification report (flattened timesteps):')\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)"
   ],
   "id": "7e874016cf698b1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
