{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-23\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1-\n",
    "\n",
    "section: 3\n",
    "\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "2a4eff8556e58600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Part 3 — Chronological splitting (70% train, 15% val, 15% test)\n",
    "# --------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Safety checks\n",
    "assert 'df_model' in globals(), \"df_model is not defined. Please load df_model before running this cell.\"\n",
    "assert all(feat in df_model.columns for feat in FEATURES), f\"Not all FEATURES found in df_model columns: {FEATURES}\"\n",
    "assert 'Label' in df_model.columns, \"df_model must contain a 'Label' column.\"\n",
    "\n",
    "# Ensure chronological order by DATETIME (robustness)\n",
    "if 'DATETIME' in df_model.columns:\n",
    "    df_model = df_model.copy()\n",
    "    df_model['DATETIME'] = pd.to_datetime(df_model['DATETIME'])\n",
    "    df_model = df_model.sort_values('DATETIME', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Basic meta\n",
    "n_rows = len(df_model)\n",
    "print(f\"Total rows in df_model: {n_rows}\")\n",
    "\n",
    "# Chronological split boundaries\n",
    "train_end = int(np.floor(0.70 * n_rows))\n",
    "val_end = int(np.floor(0.85 * n_rows))  # 70% -> 85% -> 100%\n",
    "\n",
    "# Slice datasets (pure chronological, no shuffling)\n",
    "train_df = df_model.iloc[:train_end].reset_index(drop=True)\n",
    "val_df = df_model.iloc[train_end:val_end].reset_index(drop=True)\n",
    "test_df = df_model.iloc[val_end:].reset_index(drop=True)\n"
   ],
   "id": "ad510608238e2121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 4 — Scaling using only training data (StandardScaler)\n",
    "# At the end we print variables useful for Part 5 (Imbalance handling)\n",
    "# --- Prepare arrays of features for scaler and later sequence building ---\n",
    "\n",
    "X_train_raw = train_df[FEATURES].astype(float).copy()  # DataFrame\n",
    "X_val_raw = val_df[FEATURES].astype(float).copy()\n",
    "X_test_raw = test_df[FEATURES].astype(float).copy()\n",
    "\n",
    "# --- Fit scaler on training features ONLY ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw.values)  # fit on numpy array from training set only\n",
    "\n",
    "# Transform all splits using the training-fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train_raw.values)  # numpy array shape (n_train, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_raw.values)\n",
    "X_test_scaled = scaler.transform(X_test_raw.values)\n",
    "\n",
    "# If user later wants DataFrames back (with same columns), create them:\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=FEATURES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=FEATURES)\n",
    "\n",
    "# --- Label arrays for imbalance handling (these are per-row labels) ---\n",
    "y_train_labels = train_df['Label'].astype(int).values\n",
    "y_val_labels = val_df['Label'].astype(int).values\n",
    "y_test_labels = test_df['Label'].astype(int).values\n",
    "\n",
    "# --- Basic class distribution info (useful for imbalance handling) ---\n",
    "unique_classes = np.array(sorted(df_model['Label'].dropna().unique())).astype(int)\n",
    "\n",
    "train_class_counts = {int(c): int((y_train_labels == c).sum()) for c in unique_classes}\n",
    "val_class_counts = {int(c): int((y_val_labels == c).sum()) for c in unique_classes}\n",
    "test_class_counts = {int(c): int((y_test_labels == c).sum()) for c in unique_classes}\n",
    "\n",
    "train_total = len(y_train_labels)\n",
    "val_total = len(y_val_labels)\n",
    "test_total = len(y_test_labels)\n",
    "\n",
    "train_class_percent = {c: (count / train_total) * 100.0 for c, count in train_class_counts.items()}\n",
    "\n",
    "# --- Compute initial class weights (sklearn balanced weighting) on the training labels\n",
    "# This gives a starting point; given extreme imbalance you will likely combine resampling + weights.\n",
    "classes_for_weights = np.array(sorted(np.unique(y_train_labels)))\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes_for_weights, y=y_train_labels)\n",
    "class_weight_dict = {int(cls): float(w) for cls, w in zip(classes_for_weights, class_weights)}\n",
    "\n",
    "\n",
    "# For convenience, also expose them in the current namespace (no-op if already present)\n",
    "_train_idx = (0, train_end)\n",
    "_val_idx = (train_end, val_end)\n",
    "_test_idx = (val_end, n_rows)\n",
    "\n",
    "# End of this code cell — ready for Part 5.\n"
   ],
   "id": "3afdc96547207ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# PART 5 — IMBALANCE HANDLING\n",
    "# -----------------------------\n",
    "# Place parameters here so you can manage imbalance handling behavior easily.\n",
    "IMBALANCE_STRATEGY = \"combined\"  # options: \"none\", \"class_weight\", \"oversample\", \"undersample\", \"smote\", \"combined\"\n",
    "RANDOM_STATE = 42\n",
    "# Only used for undersampling the majority class (0). Fraction of majority to KEEP (0-1).\n",
    "MAJORITY_KEEP_FRAC = 0.5\n",
    "# After undersampling, SMOTE will upsample minority classes to reach this relative proportion of the\n",
    "# (new) majority class. e.g. minority_target_ratio = 0.5 -> each minority class will be ~50% of majority.\n",
    "MINORITY_TARGET_RATIO = 0.6\n",
    "\n",
    "# NOTE (important): resampling is applied ONLY to the TRAIN split. Validation and test splits remain unchanged.\n",
    "# Also: resampling breaks chronological ordering inside the training set. That is usually acceptable for models\n",
    "# that only learn patterns (not for models where the exact chronology of training samples matters). You already\n",
    "# performed chronological splitting; we keep val/test untouched.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Try to import imblearn; if not available, fallback to sklearn.utils.resample for simple oversampling/undersampling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare training arrays from existing variables\n",
    "# -----------------------------\n",
    "# Expectations (you said these exist earlier):\n",
    "# - train_df (pandas DataFrame) with a 'Label' column for training slice\n",
    "# - X_train_scaled, X_val_scaled, X_test_scaled: numpy arrays of scaled features (train/val/test)\n",
    "# - scaler: fitted scaler instance (kept for later parts)\n",
    "# If your variable names differ, adapt below accordingly.\n",
    "\n",
    "# create y_train_labels from train_df\n",
    "y_train_labels = train_df['Label'].astype(int).to_numpy()\n",
    "# make sure X_train_scaled is a numpy array\n",
    "X_train_scaled = np.asarray(X_train_scaled)\n",
    "X_val_scaled = np.asarray(X_val_scaled)\n",
    "X_test_scaled = np.asarray(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame view of scaled train features (useful later and requested)\n",
    "# FEATURES list is provided in your global hyperparameters\n",
    "try:\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "except Exception:\n",
    "    # if shape mismatch or column names not matching, fallback to generic column names\n",
    "    n_feats = X_train_scaled.shape[1]\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=[f\"f{i}\" for i in range(n_feats)])\n",
    "\n",
    "# Compute and print initial train class counts\n",
    "train_class_counts = dict(Counter(y_train_labels))\n",
    "print(\"Initial TRAIN class counts:\", train_class_counts)\n",
    "\n",
    "# Compute sklearn balanced class weights (reference)\n",
    "classes = np.unique(y_train_labels)\n",
    "class_weight_vals = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train_labels)\n",
    "class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weight_vals)}\n",
    "print(\"Initial computed class_weight (sklearn 'balanced'):\", class_weight_dict)\n",
    "\n",
    "# Prepare placeholders for resampled outputs\n",
    "X_train_resampled = X_train_scaled.copy()\n",
    "y_train_resampled = y_train_labels.copy()\n",
    "sample_weight_train_resampled = None\n",
    "\n",
    "# -----------------------------\n",
    "# Implement strategies\n",
    "# -----------------------------\n",
    "if IMBALANCE_STRATEGY == \"none\":\n",
    "    # Do nothing — just use the original training arrays\n",
    "    print(\"IMBALANCE_STRATEGY = 'none' -> no resampling performed. Using original training data.\")\n",
    "\n",
    "elif IMBALANCE_STRATEGY == \"class_weight\":\n",
    "    # Do not change the training set; use class weights at training time.\n",
    "    print(\"IMBALANCE_STRATEGY = 'class_weight' -> no resampling performed. Use class weights during model.fit().\")\n",
    "\n",
    "elif IMBALANCE_STRATEGY in (\"oversample\", \"undersample\", \"smote\", \"combined\"):\n",
    "    print(f\"IMBALANCE_STRATEGY = '{IMBALANCE_STRATEGY}' -> performing resampling on TRAIN only.\")\n",
    "    # If imblearn available, use its resamplers for better behavior. Otherwise fall back to manual resampling.\n",
    "    if IMBLEARN_AVAILABLE:\n",
    "        # Build index array so we can easily map back if needed\n",
    "        train_idx = np.arange(len(y_train_labels))\n",
    "\n",
    "        if IMBALANCE_STRATEGY == \"oversample\":\n",
    "            ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = ros.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"RandomOverSampler completed.\")\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"undersample\":\n",
    "            # Reduce majority class (0) to MAJORITY_KEEP_FRAC of its original count while keeping minorities intact.\n",
    "            # Create sampling_strategy dict for RandomUnderSampler\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "            sampling_strategy = {0: target_maj}\n",
    "            # keep other classes as-is (imblearn will remove entries only for classes in sampling_strategy)\n",
    "            rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"RandomUnderSampler completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"smote\":\n",
    "            # SMOTE can be unstable with extremely imbalanced data; we will first undersample majority a little, then SMOTE.\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "            # Step 1: undersample majority to make SMOTE viable\n",
    "            rus = RandomUnderSampler(sampling_strategy={0: target_maj}, random_state=RANDOM_STATE)\n",
    "            X_tmp, y_tmp = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            # Step 2: SMOTE to balance minorities relative to the new majority\n",
    "            # Build sampling_strategy for SMOTE: set each minority to MINORITY_TARGET_RATIO * target_maj\n",
    "            sampling_strategy_smote = {}\n",
    "            for cls, _ in Counter(y_tmp).items():\n",
    "                if cls == 0:\n",
    "                    continue\n",
    "                sampling_strategy_smote[int(cls)] = int(max(1, target_maj * MINORITY_TARGET_RATIO))\n",
    "            sm = SMOTE(sampling_strategy=sampling_strategy_smote, random_state=RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = sm.fit_resample(X_tmp, y_tmp)\n",
    "            print(\"SMOTE (with initial undersample) completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"combined\":\n",
    "            # Combined strategy: undersample majority to MAJORITY_KEEP_FRAC, then SMOTE minority classes up to\n",
    "            # MINORITY_TARGET_RATIO of the new majority class.\n",
    "            maj_count = train_class_counts.get(0, 0)\n",
    "            target_maj = max(1, int(maj_count * MAJORITY_KEEP_FRAC))\n",
    "\n",
    "            # 1) Undersample majority\n",
    "            rus = RandomUnderSampler(sampling_strategy={0: target_maj}, random_state=RANDOM_STATE)\n",
    "            X_tmp, y_tmp = rus.fit_resample(X_train_scaled, y_train_labels)\n",
    "            print(\"Step 1: Undersample majority done. Counts:\", dict(Counter(y_tmp)))\n",
    "\n",
    "            # 2) SMOTE -> target minority counts relative to new majority\n",
    "            sampling_strategy_smote = {}\n",
    "            for cls in np.unique(y_tmp):\n",
    "                if int(cls) == 0:\n",
    "                    continue\n",
    "                sampling_strategy_smote[int(cls)] = int(max(1, target_maj * MINORITY_TARGET_RATIO))\n",
    "\n",
    "            # If sampling_strategy_smote would result in fewer minority samples than already present, set to 'auto'\n",
    "            # to avoid trying to downsample with SMOTE (which cannot downsample).\n",
    "            # Build final smote strategy only for classes where desired > present\n",
    "            final_smote_strategy = {}\n",
    "            for cls, desired in sampling_strategy_smote.items():\n",
    "                present = Counter(y_tmp).get(cls, 0)\n",
    "                if desired > present:\n",
    "                    final_smote_strategy[cls] = desired\n",
    "\n",
    "            if len(final_smote_strategy) == 0:\n",
    "                # Nothing to SMOTE; keep undersampled version\n",
    "                X_train_resampled, y_train_resampled = X_tmp, y_tmp\n",
    "                print(\"No SMOTE required after undersampling (minorities already at/above target).\")\n",
    "            else:\n",
    "                sm = SMOTE(sampling_strategy=final_smote_strategy, random_state=RANDOM_STATE)\n",
    "                X_train_resampled, y_train_resampled = sm.fit_resample(X_tmp, y_tmp)\n",
    "                print(\"Step 2: SMOTE completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported IMBALANCE_STRATEGY with imblearn.\")\n",
    "\n",
    "    else:\n",
    "        # imblearn not available — fallback to simpler resampling using sklearn.utils.resample\n",
    "        print(\"imblearn not installed. Falling back to simple resampling with sklearn.utils.resample. \"\n",
    "              \"This is less flexible for SMOTE, so only basic oversample/undersample are available.\")\n",
    "        if IMBALANCE_STRATEGY == \"oversample\":\n",
    "            # naive oversample minority classes to match majority count (not ideal but workable)\n",
    "            df_train_tmp = pd.DataFrame(X_train_scaled)\n",
    "            df_train_tmp['Label'] = y_train_labels\n",
    "            majority_class = df_train_tmp[df_train_tmp['Label'] == 0]\n",
    "            majority_count = len(majority_class)\n",
    "            resampled_parts = [majority_class]\n",
    "            for cls in np.unique(y_train_labels):\n",
    "                if cls == 0:\n",
    "                    continue\n",
    "                cls_df = df_train_tmp[df_train_tmp['Label'] == cls]\n",
    "                upsampled = resample(cls_df, replace=True, n_samples=majority_count, random_state=RANDOM_STATE)\n",
    "                resampled_parts.append(upsampled)\n",
    "            df_res = pd.concat(resampled_parts).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "            y_train_resampled = df_res['Label'].to_numpy()\n",
    "            X_train_resampled = df_res.drop(columns=['Label']).to_numpy()\n",
    "            print(\"Fallback oversample completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        elif IMBALANCE_STRATEGY == \"undersample\":\n",
    "            df_train_tmp = pd.DataFrame(X_train_scaled)\n",
    "            df_train_tmp['Label'] = y_train_labels\n",
    "            majority_class = df_train_tmp[df_train_tmp['Label'] == 0]\n",
    "            target_maj = int(len(majority_class) * MAJORITY_KEEP_FRAC)\n",
    "            majority_down = resample(majority_class, replace=False, n_samples=target_maj, random_state=RANDOM_STATE)\n",
    "            others = df_train_tmp[df_train_tmp['Label'] != 0]\n",
    "            df_res = pd.concat([majority_down, others]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "            y_train_resampled = df_res['Label'].to_numpy()\n",
    "            X_train_resampled = df_res.drop(columns=['Label']).to_numpy()\n",
    "            print(\"Fallback undersample completed. New TRAIN counts:\", dict(Counter(y_train_resampled)))\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(\"Requested strategy requires imblearn but it's not installed. \"\n",
    "                               \"Install 'imblearn' to use SMOTE or combined strategies.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown IMBALANCE_STRATEGY: {IMBALANCE_STRATEGY}\")\n",
    "\n",
    "# -----------------------------\n",
    "# After resampling: compute sample weights (useful in model.fit if you want per-sample weighting)\n",
    "# -----------------------------\n",
    "# Compute class weights on the resampled training set (so the model still sees higher weight for minority if desired)\n",
    "resampled_classes = np.unique(y_train_resampled)\n",
    "resampled_class_weight_vals = compute_class_weight(class_weight=\"balanced\", classes=resampled_classes,\n",
    "                                                   y=y_train_resampled)\n",
    "resampled_class_weight_dict = {int(c): float(w) for c, w in zip(resampled_classes, resampled_class_weight_vals)}\n",
    "print(\"Resampled TRAIN class counts:\", dict(Counter(y_train_resampled)))\n",
    "print(\"Resampled computed class_weight (sklearn 'balanced'):\", resampled_class_weight_dict)\n",
    "\n",
    "# Build per-sample weights array for the resampled training set (useful if you want to pass sample_weight to model.fit)\n",
    "sample_weight_train_resampled = np.array([resampled_class_weight_dict[int(lbl)] for lbl in y_train_resampled],\n",
    "                                         dtype=float)\n",
    "\n"
   ],
   "id": "1ad0897eaa68dcc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 6 — Create sequences (WINDOW_SIZE -> X, FORECAST_HORIZON -> y)\n",
    "\n",
    "\n",
    "# --- Helper: sliding-window sequence creator for one split ---\n",
    "def create_sequences_from_split(X_split, y_split, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    X_split: 2D array (n_rows, n_features)\n",
    "    y_split: 1D array (n_rows,) integer labels\n",
    "    Returns:\n",
    "      X_seq: (n_sequences, window_size, n_features)\n",
    "      y_seq: (n_sequences, forecast_horizon) ints\n",
    "    \"\"\"\n",
    "    X_arr = np.asarray(X_split)\n",
    "    y_arr = np.asarray(y_split)\n",
    "\n",
    "    if X_arr.ndim != 2:\n",
    "        raise ValueError(f\"X_split must be 2D array, got shape {X_arr.shape}\")\n",
    "    if y_arr.ndim != 1:\n",
    "        raise ValueError(f\"y_split must be 1D array, got shape {y_arr.shape}\")\n",
    "    if X_arr.shape[0] != y_arr.shape[0]:\n",
    "        raise ValueError(f\"X and y must have same first-dimension length. X: {X_arr.shape[0]}, y: {y_arr.shape[0]}\")\n",
    "\n",
    "    n_rows = X_arr.shape[0]\n",
    "    last_start = n_rows - window_size - forecast_horizon  # inclusive max start index\n",
    "    if last_start < 0:\n",
    "        # Not enough rows to construct a single sequence\n",
    "        return np.empty((0, window_size, X_arr.shape[1])), np.empty((0, forecast_horizon), dtype=int)\n",
    "\n",
    "    n_sequences = last_start + 1\n",
    "    X_seq = np.empty((n_sequences, window_size, X_arr.shape[1]), dtype=X_arr.dtype)\n",
    "    y_seq = np.empty((n_sequences, forecast_horizon), dtype=int)\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        X_seq[i] = X_arr[i: i + window_size]\n",
    "        y_seq[i] = y_arr[i + window_size: i + window_size + forecast_horizon]\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "# --- Create sequences for each split ---\n",
    "X_train_seq, y_train_seq = create_sequences_from_split(X_train_scaled, y_train_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_val_seq, y_val_seq = create_sequences_from_split(X_val_scaled, y_val_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_test_seq, y_test_seq = create_sequences_from_split(X_test_scaled, y_test_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "# --- One-hot encode multi-step labels ---\n",
    "# Determine number of classes from training labels (safe default: 3 classes {0,1,2})\n",
    "classes_in_train = np.unique(y_train_seq) if y_train_seq.size > 0 else np.array([0, 1, 2])\n",
    "n_classes = int(max(classes_in_train.max(), 2) + 1)  # ensures at least 3 classes (0..2)\n",
    "\n",
    "\n",
    "# Convert to categorical: result shape (n_sequences, FORECAST_HORIZON, n_classes)\n",
    "def one_hot_multi_step(y_seq, n_classes):\n",
    "    if y_seq.size == 0:\n",
    "        return np.empty((0, y_seq.shape[1], n_classes), dtype=np.float32)\n",
    "    # to_categorical works on flattened array, then reshape\n",
    "    flat = to_categorical(y_seq.ravel(), num_classes=n_classes)\n",
    "    return flat.reshape((y_seq.shape[0], y_seq.shape[1], n_classes))\n",
    "\n",
    "\n",
    "y_train_seq_cat = one_hot_multi_step(y_train_seq, n_classes)\n",
    "y_val_seq_cat = one_hot_multi_step(y_val_seq, n_classes)\n",
    "y_test_seq_cat = one_hot_multi_step(y_test_seq, n_classes)\n",
    "\n",
    "\n",
    "# --- Useful diagnostics and prints required for Part 7 ---\n",
    "def seq_stats(X_seq, y_seq, name):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"X_{name}_seq.shape: {X_seq.shape}\")\n",
    "    print(f\"y_{name}_seq.shape: {y_seq.shape}\")\n",
    "    if y_seq.size > 0:\n",
    "        flattened = y_seq.ravel()\n",
    "        unique, counts = np.unique(flattened, return_counts=True)\n",
    "        dist = dict(zip([int(u) for u in unique], [int(c) for c in counts]))\n",
    "        total = flattened.size\n",
    "        print(f\"Label distribution across all forecast positions (counts): {dist}\")\n",
    "        print(\"Label distribution (percent):\", {int(u): round(c / total * 100, 4) for u, c in zip(unique, counts)})\n",
    "    else:\n",
    "        print(\"No sequences (empty).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "seq_stats(X_train_seq, y_train_seq, \"train\")\n",
    "seq_stats(X_val_seq, y_val_seq, \"val\")\n",
    "seq_stats(X_test_seq, y_test_seq, \"test\")\n",
    "\n"
   ],
   "id": "a3d60357fcba0b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 7 — Build, Train and Evaluate the model\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Basic checks\n",
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"y_train_seq_cat shape:\", y_train_seq_cat.shape)\n",
    "print(\"X_val_seq shape:\", X_val_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "\n",
    "N_CLASSES = y_train_seq_cat.shape[-1]  # should be 3\n",
    "TIMESTEPS = y_train_seq_cat.shape[1]  # should be FORECAST_HORIZON\n",
    "\n",
    "# ---------------------------\n",
    "# Create timestep-aware sample weights to handle extreme imbalance\n",
    "# ---------------------------\n",
    "# Compute global class frequencies across all samples and timesteps in training set\n",
    "class_counts = np.sum(y_train_seq_cat, axis=(0, 1))  # shape (n_classes,)\n",
    "total_labels = np.sum(class_counts)\n",
    "print(\"Train class counts (sum across samples and timesteps):\", class_counts)\n",
    "\n",
    "# Avoid division by zero\n",
    "eps = 1e-8\n",
    "# Inverse-frequency weighting (normalized)\n",
    "inv_freq = (total_labels / (class_counts + eps))\n",
    "# normalize so that mean weight == 1 (keeps loss scale stable)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "print(\"Per-class inverse-frequency weights:\", inv_freq)\n",
    "\n",
    "\n",
    "# Build sample_weight arrays with shape (n_samples, timesteps)\n",
    "def build_timestep_sample_weights(y_cat, inv_freq_array):\n",
    "    # y_cat shape: (n_samples, timesteps, n_classes)\n",
    "    n_samples, timesteps, n_classes = y_cat.shape\n",
    "    sw = np.zeros((n_samples, timesteps), dtype=np.float32)\n",
    "    # argmax to get the true class index per sample/timestep\n",
    "    true_classes = np.argmax(y_cat, axis=-1)  # shape (n_samples, timesteps)\n",
    "    for c in range(n_classes):\n",
    "        sw[true_classes == c] = inv_freq_array[c]\n",
    "    return sw\n",
    "\n",
    "\n",
    "sample_weight_train = build_timestep_sample_weights(y_train_seq_cat, inv_freq)\n",
    "sample_weight_val = build_timestep_sample_weights(y_val_seq_cat, inv_freq)\n",
    "sample_weight_test = build_timestep_sample_weights(y_test_seq_cat, inv_freq)\n",
    "\n",
    "print(\"sample_weight_train shape:\", sample_weight_train.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Build the model\n",
    "# Seq2Seq-ish model: Encoder LSTM -> RepeatVector -> Decoder LSTM (return_sequences=True) -> TimeDistributed(Dense)\n",
    "# ---------------------------\n",
    "INPUT_SHAPE = X_train_seq.shape[1:]  # (WINDOW_SIZE, n_features)\n",
    "EMBED_DIM = 128\n",
    "ENC_UNITS = 128\n",
    "DEC_UNITS = 128\n",
    "DROPOUT = 0.2\n",
    "\n",
    "inputs = layers.Input(shape=INPUT_SHAPE, name='inputs')\n",
    "# optional masking if there are padded sequences; here probably not needed but harmless\n",
    "x = layers.Masking(mask_value=0.0)(inputs)\n",
    "# Encoder\n",
    "x = layers.Bidirectional(layers.LSTM(ENC_UNITS, return_sequences=False, dropout=DROPOUT), name='encoder_bi')(x)\n",
    "# Project to embedding\n",
    "x = layers.Dense(EMBED_DIM, activation='relu', name='encoder_dense')(x)\n",
    "# Repeat for forecast horizon\n",
    "x = layers.RepeatVector(TIMESTEPS, name='repeat_vector')(x)\n",
    "# Decoder\n",
    "x = layers.LSTM(DEC_UNITS, return_sequences=True, dropout=DROPOUT, name='decoder_lstm')(x)\n",
    "# Optional TimeDistributed intermediate dense\n",
    "x = layers.TimeDistributed(layers.Dense(64, activation='relu'), name='td_dense')(x)\n",
    "# Final classification per timestep\n",
    "outputs = layers.TimeDistributed(layers.Dense(N_CLASSES, activation='softmax'), name='td_softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name='seq2seq_reversal_classifier')\n",
    "model.summary()"
   ],
   "id": "7e874016cf698b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Compile\n",
    "# ---------------------------\n",
    "LR = 1e-3\n",
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# Callbacks\n",
    "# ---------------------------\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Fit\n",
    "# Note: sample_weight for sequence outputs should be shape (n_samples, timesteps)\n",
    "# ---------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq_cat,\n",
    "    validation_data=(X_val_seq, y_val_seq_cat, sample_weight_val),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es, reduce_lr],\n",
    "    sample_weight=sample_weight_train,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nTRAINING COMPLETE!\")\n"
   ],
   "id": "c58fc5bdcd9716a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Evaluate on test set\n",
    "# ---------------------------\n",
    "print('\\nEvaluating on test set...')\n",
    "eval_results = model.evaluate(X_test_seq, y_test_seq_cat, sample_weight=sample_weight_test, verbose=2)\n",
    "print('Test loss/metrics:', eval_results)\n",
    "\n",
    "# ---------------------------\n",
    "# Detailed classification report (flatten timesteps)\n",
    "# ---------------------------\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq, batch_size=BATCH_SIZE)\n",
    "# y_pred_proba shape: (n_samples, timesteps, n_classes)\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1).reshape(-1)\n",
    "y_true = np.argmax(y_test_seq_cat, axis=-1).reshape(-1)\n",
    "\n",
    "print('\\nClassification report (flattened timesteps):')\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n"
   ],
   "id": "4c6d62ab188030b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timedelta\n",
    "N_FEATURES = len(FEATURES)  # 5\n",
    "# ============================================================================\n",
    "# PREDICTION SECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION ON UNSEEN DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# If given_time already exists, add 5 hours\n",
    "try:\n",
    "    dt = datetime.strptime(given_time, \"%Y.%m.%d %H:%M:%S\") + timedelta(hours=5)\n",
    "except NameError:\n",
    "    # First run: initialize given_time\n",
    "    dt = datetime.strptime(\"2025.08.13 21:00:00\", \"%Y.%m.%d %H:%M:%S\")\n",
    "\n",
    "# Store back as string\n",
    "given_time = dt.strftime(\"%Y.%m.%d %H:%M:%S\")\n",
    "print(f\"\\nGiven time: {given_time}\")"
   ],
   "id": "971f9353f0017d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Find the index of given_time in df (not df_model)\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "given_idx = df[df['DATETIME'] == given_time].index[0]\n",
    "\n",
    "print(f\"Given time index in df: {given_idx}\")\n",
    "\n",
    "# Extract 60 candles ending at given_time\n",
    "start_idx = given_idx - WINDOW_SIZE + 1\n",
    "end_idx = given_idx + 1\n",
    "\n",
    "input_df = df.iloc[start_idx:end_idx][['DATETIME'] + FEATURES].copy()\n",
    "print(f\"Input shape (before scaling): {input_df.shape}\")\n",
    "\n",
    "# Separate DATETIME from features for scaling\n",
    "input_candles = input_df.copy()  # Keep for visualization (has DATETIME)\n",
    "input_features_only = input_df[FEATURES]  # Only features for model\n",
    "\n",
    "# Scale using the same scaler from training (only the FEATURES columns)\n",
    "input_scaled = scaler.transform(input_features_only)\n",
    "input_scaled = input_scaled.reshape(1, WINDOW_SIZE, N_FEATURES)\n",
    "\n",
    "# Predict\n",
    "predictions_proba = model.predict(input_scaled, verbose=0)  # Shape: (1, 10, 3)\n",
    "predictions_proba = predictions_proba[0]  # Shape: (10, 3)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "# Create forecast datetimes (next 10 hours after given_time)\n",
    "given_datetime = pd.to_datetime(given_time)\n",
    "forecast_datetimes = [given_datetime + pd.Timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "# Create output DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': predicted_classes,\n",
    "    'prob_0': predictions_proba[:, 0],\n",
    "    'prob_1': predictions_proba[:, 1],\n",
    "    'prob_2': predictions_proba[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "predicted_df"
   ],
   "id": "d3c938f293d42d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d72e84c79eb4918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df018d2be311917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(2).copy()"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "9c0c6eb2f4c47e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "d5c3eaaba6bb40e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model with Comprehensive Report\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# 1- Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 2- Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 3- Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ],
   "id": "33c1a03d562a3c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 4- Save scaler (IMPORTANT - needed for predictions!)\n",
    "import joblib\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 5- Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# 6- Save full history as JSON so it can be reloaded later\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "print(f\"Full history object saved to: {history_json_path}\")"
   ],
   "id": "2d20b00ea8200f68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7 — Save Training Loss Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_plot_path = os.path.join(log_dir, \"training_loss.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Loss plot saved to: {loss_plot_path}\")"
   ],
   "id": "6301f264dfe54ebf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8 — Save Accuracy Plot\n",
    "acc_plot_path = os.path.join(log_dir, \"training_accuracy.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(acc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Accuracy plot saved to: {acc_plot_path}\")"
   ],
   "id": "707148790a4706c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "# Path to save\n",
    "cm_plot_path = os.path.join(log_dir, \"confusion_matrix.png\")\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=['0', '1', '2'],\n",
    "    yticklabels=['0', '1', '2']\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Flattened Horizon)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {cm_plot_path}\")\n"
   ],
   "id": "c2d10d34b0785e45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251210_062707.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251210_062707.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251210_062707_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9e1fb02288869c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
