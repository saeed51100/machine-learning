{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-25\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1-\n",
    "\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "2a4eff8556e58600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, RepeatVector, Dropout, \\\n",
    "    BatchNormalization\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "\n",
    "import datetime\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(\n",
    "        close,\n",
    "        high=None,\n",
    "        low=None,\n",
    "        smoothing_window=31,\n",
    "        polyorder=3,\n",
    "        base_prom_factor=0.02,\n",
    "        distance=3,\n",
    "        snap_window=5,\n",
    "        min_dev_pct=0.0015,  # 0.15% minimum leg size\n",
    "        min_dev_sigma=2.0,  # >= 2x local abs-return EMA\n",
    "        vol_window=100,  # EMA window for local volatility\n",
    "        verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Label reversal points with improved accuracy.\n",
    "\n",
    "    Returns labels array of length n where:\n",
    "    0 = none, 1 = valley, 2 = peak.\n",
    "\n",
    "    Tips:\n",
    "    - For best accuracy, pass high/low arrays from your OHLCV.\n",
    "      Example: label_reversal_points(df['CLOSE'], df['HIGH'], df['LOW'])\n",
    "    - Tune min_dev_pct / min_dev_sigma to be stricter or looser on swing size.\n",
    "    \"\"\"\n",
    "    close = np.asarray(close, dtype=float)\n",
    "    n = close.size\n",
    "    if n < 3:\n",
    "        return np.zeros(n, dtype=int)\n",
    "\n",
    "    # Interpolate NaNs if any\n",
    "    if np.isnan(close).any():\n",
    "        idx = np.arange(n)\n",
    "        good = ~np.isnan(close)\n",
    "        close = close.copy()\n",
    "        close[~good] = np.interp(idx[~good], idx[good], close[good])\n",
    "\n",
    "    # Helper: simple EMA for local abs-return volatility\n",
    "    def ema(x, span):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        alpha = 2.0 / (span + 1.0)\n",
    "        out = np.empty_like(x)\n",
    "        out[0] = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]\n",
    "        return out\n",
    "\n",
    "    # Local volatility in price terms via EMA of absolute returns\n",
    "    ret = np.zeros(n)\n",
    "    ret[1:] = np.abs(np.diff(close) / np.maximum(1e-12, close[:-1]))\n",
    "    vol_absret = ema(ret, vol_window)\n",
    "    local_vol_price = vol_absret * close  # convert to price units\n",
    "\n",
    "    # Smoothing to get robust candidates\n",
    "    win = smoothing_window\n",
    "    if win >= n:\n",
    "        win = n - 1 if (n - 1) % 2 == 1 else n - 2\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    smoothed = savgol_filter(close, win, polyorder)\n",
    "\n",
    "    # Base prominence threshold\n",
    "    global_std = np.std(close) or 1.0\n",
    "    prom = global_std * base_prom_factor\n",
    "\n",
    "    # Candidate peaks/valleys on smoothed\n",
    "    peak_idx, _ = find_peaks(smoothed, distance=distance, prominence=prom)\n",
    "    val_idx, _ = find_peaks(-smoothed, distance=distance, prominence=prom)\n",
    "\n",
    "    # Prominences for tie-breaking\n",
    "    peak_prom = peak_prominences(smoothed, peak_idx)[0] if peak_idx.size else np.array([])\n",
    "    val_prom = peak_prominences(-smoothed, val_idx)[0] if val_idx.size else np.array([])\n",
    "\n",
    "    # Combine\n",
    "    candidates = []\n",
    "    for i, p in enumerate(peak_idx):\n",
    "        candidates.append((int(p), 2, float(peak_prom[i]) if peak_prom.size else 0.0))\n",
    "    for i, v in enumerate(val_idx):\n",
    "        candidates.append((int(v), 1, float(val_prom[i]) if val_prom.size else 0.0))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    if not candidates:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "        # still mark edges for completeness\n",
    "        labels[0] = 1 if close[1] > close[0] else 2\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "        return labels\n",
    "\n",
    "    # Enforce alternation (remove weaker when two same-type neighbors)\n",
    "    def enforce_alternation(ext):\n",
    "        ext = ext[:]  # list of (idx, typ, prom)\n",
    "        while True:\n",
    "            removed = False\n",
    "            i = 0\n",
    "            while i < len(ext) - 1:\n",
    "                if ext[i][1] == ext[i + 1][1]:\n",
    "                    # drop the smaller prominence\n",
    "                    if ext[i][2] < ext[i + 1][2]:\n",
    "                        ext.pop(i)\n",
    "                    else:\n",
    "                        ext.pop(i + 1)\n",
    "                    removed = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            if not removed:\n",
    "                break\n",
    "        return ext\n",
    "\n",
    "    candidates = enforce_alternation(candidates)\n",
    "\n",
    "    # SNAP: move each extreme to the true local extremum on raw close (or HIGH/LOW)\n",
    "    def snap_index(idx, typ):\n",
    "        L = max(0, idx - snap_window)\n",
    "        R = min(n, idx + snap_window + 1)\n",
    "        if high is not None and low is not None:\n",
    "            if typ == 2:  # peak\n",
    "                j = np.argmax(np.asarray(high[L:R], dtype=float))\n",
    "            else:  # valley\n",
    "                j = np.argmin(np.asarray(low[L:R], dtype=float))\n",
    "        else:\n",
    "            if typ == 2:\n",
    "                j = np.argmax(close[L:R])\n",
    "            else:\n",
    "                j = np.argmin(close[L:R])\n",
    "        return L + int(j)\n",
    "\n",
    "    snapped = []\n",
    "    seen_at = {}  # avoid duplicate indices by keeping stronger prominence\n",
    "    for idx, typ, pr in candidates:\n",
    "        j = snap_index(idx, typ)\n",
    "        key = (j, typ)\n",
    "        if key not in seen_at or pr > seen_at[key][2]:\n",
    "            seen_at[key] = (j, typ, pr)\n",
    "    snapped = sorted(seen_at.values(), key=lambda x: x[0])\n",
    "\n",
    "    # Enforce alternation again after snapping\n",
    "    snapped = enforce_alternation(snapped)\n",
    "\n",
    "    # Filter micro-legs using adaptive threshold (min % move and sigma*local_vol)\n",
    "    pruned = []\n",
    "    for idx, typ, pr in snapped:\n",
    "        if not pruned:\n",
    "            pruned.append((idx, typ, pr))\n",
    "            continue\n",
    "        prev_idx, prev_typ, prev_pr = pruned[-1]\n",
    "        # time spacing\n",
    "        if idx - prev_idx < distance:\n",
    "            # keep the more prominent of the two\n",
    "            if pr > prev_pr:\n",
    "                pruned[-1] = (idx, typ, pr)\n",
    "            continue\n",
    "        leg = abs(close[idx] - close[prev_idx])\n",
    "        # thresholds at both ends\n",
    "        thr = max(min_dev_pct * close[prev_idx],\n",
    "                  min_dev_sigma * max(local_vol_price[prev_idx], 1e-12))\n",
    "        thr = max(thr, max(min_dev_pct * close[idx],\n",
    "                           min_dev_sigma * max(local_vol_price[idx], 1e-12)))\n",
    "        if leg >= thr:\n",
    "            pruned.append((idx, typ, pr))\n",
    "        else:\n",
    "            # too small swing → drop the later point\n",
    "            continue\n",
    "\n",
    "    # One more alternation pass (paranoid) and spacing check\n",
    "    pruned = enforce_alternation(pruned)\n",
    "    final_ext = []\n",
    "    for idx, typ, pr in pruned:\n",
    "        if final_ext and idx - final_ext[-1][0] < distance:\n",
    "            # keep stronger\n",
    "            if pr > final_ext[-1][2]:\n",
    "                final_ext[-1] = (idx, typ, pr)\n",
    "        else:\n",
    "            final_ext.append((idx, typ, pr))\n",
    "\n",
    "    # Build labels\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    for idx, typ, _ in final_ext:\n",
    "        labels[idx] = typ\n",
    "\n",
    "    # Mark edges as trend boundaries for continuity\n",
    "    if labels[0] == 0:\n",
    "        labels[0] = 1 if close[min(1, n - 1)] > close[0] else 2\n",
    "    if labels[-1] == 0 and n >= 2:\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "\n",
    "    if verbose:\n",
    "        c0 = int((labels == 0).sum())\n",
    "        c1 = int((labels == 1).sum())\n",
    "        c2 = int((labels == 2).sum())\n",
    "        print(f\"labels -> 0:{c0}  1:{c1}  2:{c2}  (extrema kept: {len(final_ext)})\")\n",
    "\n",
    "    return labels\n"
   ],
   "id": "cf03646179e62d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display label distribution in df_model\n",
    "label_counts = df_model['Label'].value_counts().sort_index()\n",
    "label_percentages = (df_model['Label'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "print(\"Label Distribution in df_model:\")\n",
    "print(\"-\" * 40)\n",
    "for label in sorted(df_model['Label'].unique()):\n",
    "    count = label_counts[label]\n",
    "    percentage = label_percentages[label]\n",
    "    print(f\"Class {label}: {count:,} rows ({percentage:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total rows: {len(df_model):,}\")\n"
   ],
   "id": "8e1d8ac369134288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "7a73456c9db9c4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Part 3 — Chronological splitting (70% train, 15% val, 15% test)\n",
    "# --------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Safety checks\n",
    "assert 'df_model' in globals(), \"df_model is not defined. Please load df_model before running this cell.\"\n",
    "assert all(feat in df_model.columns for feat in FEATURES), f\"Not all FEATURES found in df_model columns: {FEATURES}\"\n",
    "assert 'Label' in df_model.columns, \"df_model must contain a 'Label' column.\"\n",
    "\n",
    "# Ensure chronological order by DATETIME (robustness)\n",
    "if 'DATETIME' in df_model.columns:\n",
    "    df_model = df_model.copy()\n",
    "    df_model['DATETIME'] = pd.to_datetime(df_model['DATETIME'])\n",
    "    df_model = df_model.sort_values('DATETIME', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Basic meta\n",
    "n_rows = len(df_model)\n",
    "print(f\"Total rows in df_model: {n_rows}\")\n",
    "\n",
    "# Chronological split boundaries\n",
    "train_end = int(np.floor(0.70 * n_rows))\n",
    "val_end = int(np.floor(0.85 * n_rows))  # 70% -> 85% -> 100%\n",
    "\n",
    "# Slice datasets (pure chronological, no shuffling)\n",
    "train_df = df_model.iloc[:train_end].reset_index(drop=True)\n",
    "val_df = df_model.iloc[train_end:val_end].reset_index(drop=True)\n",
    "test_df = df_model.iloc[val_end:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train rows : {len(train_df)} (0:{train_end})\")\n",
    "print(f\"Val rows   : {len(val_df)} ({train_end}:{val_end})\")\n",
    "print(f\"Test rows  : {len(test_df)} ({val_end}:{n_rows})\")\n",
    "print(\"\")"
   ],
   "id": "ad510608238e2121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 4 — Scaling using only training data (StandardScaler)\n",
    "# At the end we print variables useful for Part 5 (Imbalance handling)\n",
    "# --- Prepare arrays of features for scaler and later sequence building ---\n",
    "\n",
    "X_train_raw = train_df[FEATURES].astype(float).copy()  # DataFrame\n",
    "X_val_raw = val_df[FEATURES].astype(float).copy()\n",
    "X_test_raw = test_df[FEATURES].astype(float).copy()\n",
    "\n",
    "# --- Fit scaler on training features ONLY ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw.values)  # fit on numpy array from training set only\n",
    "\n",
    "# Transform all splits using the training-fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train_raw.values)  # numpy array shape (n_train, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_raw.values)\n",
    "X_test_scaled = scaler.transform(X_test_raw.values)\n",
    "\n",
    "# If user later wants DataFrames back (with same columns), create them:\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=FEATURES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=FEATURES)\n",
    "\n",
    "# --- Label arrays for imbalance handling (these are per-row labels) ---\n",
    "y_train_labels = train_df['Label'].astype(int).values\n",
    "y_val_labels = val_df['Label'].astype(int).values\n",
    "y_test_labels = test_df['Label'].astype(int).values\n",
    "\n",
    "# --- Basic class distribution info (useful for imbalance handling) ---\n",
    "unique_classes = np.array(sorted(df_model['Label'].dropna().unique())).astype(int)\n",
    "\n",
    "train_class_counts = {int(c): int((y_train_labels == c).sum()) for c in unique_classes}\n",
    "val_class_counts = {int(c): int((y_val_labels == c).sum()) for c in unique_classes}\n",
    "test_class_counts = {int(c): int((y_test_labels == c).sum()) for c in unique_classes}\n",
    "\n",
    "train_total = len(y_train_labels)\n",
    "val_total = len(y_val_labels)\n",
    "test_total = len(y_test_labels)\n",
    "\n",
    "train_class_percent = {c: (count / train_total) * 100.0 for c, count in train_class_counts.items()}\n",
    "\n",
    "# --- Compute initial class weights (sklearn balanced weighting) on the training labels\n",
    "# This gives a starting point; given extreme imbalance you will likely combine resampling + weights.\n",
    "classes_for_weights = np.array(sorted(np.unique(y_train_labels)))\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes_for_weights, y=y_train_labels)\n",
    "class_weight_dict = {int(cls): float(w) for cls, w in zip(classes_for_weights, class_weights)}\n",
    "\n",
    "# --------------------------\n",
    "# PRINT the variables required for Part 5 (Imbalance handling)\n",
    "# These include: indices/splits, raw counts & percentages, and the scaler (fitted)\n",
    "# --------------------------\n",
    "print(\"\\n=== Variables & summaries for Part 5 (Imbalance handling) ===\\n\")\n",
    "\n",
    "print(\"1) Split indices (chronological slice positions):\")\n",
    "print(f\"   train_end index: {train_end}\")\n",
    "print(f\"   val_end   index: {val_end}\\n\")\n",
    "\n",
    "print(\"2) Shapes of splits:\")\n",
    "print(f\"   train_df shape: {train_df.shape}\")\n",
    "print(f\"   val_df   shape: {val_df.shape}\")\n",
    "print(f\"   test_df  shape: {test_df.shape}\\n\")\n",
    "\n",
    "print(\"3) Label distribution (counts) in TRAIN set:\")\n",
    "for c, cnt in train_class_counts.items():\n",
    "    pct = train_class_percent[c]\n",
    "    print(f\"   Class {c}: {cnt} samples ({pct:.6f}%)\")\n",
    "print(f\"   -> Total training samples = {train_total}\\n\")\n",
    "\n",
    "print(\"4) Label distribution (counts) in VAL set:\")\n",
    "for c, cnt in val_class_counts.items():\n",
    "    print(f\"   Class {c}: {cnt} samples\")\n",
    "print(f\"   -> Total validation samples = {val_total}\\n\")\n",
    "\n",
    "print(\"5) Label distribution (counts) in TEST set:\")\n",
    "for c, cnt in test_class_counts.items():\n",
    "    print(f\"   Class {c}: {cnt} samples\")\n",
    "print(f\"   -> Total test samples = {test_total}\\n\")\n",
    "\n",
    "print(\"6) Initial class_weight (sklearn 'balanced') computed from TRAIN labels:\")\n",
    "for c, w in class_weight_dict.items():\n",
    "    print(f\"   Class {c}: weight = {w:.6f}\")\n",
    "print(\n",
    "    \"   (These are a starting reference; with extreme imbalance you'll likely need resampling + specialized losses.)\\n\")\n",
    "\n",
    "print(\"7) Objects/arrays you will likely need for Part 5:\")\n",
    "print(\"   - y_train_labels           (numpy array of training labels)\")\n",
    "print(\"   - train_class_counts       (dict of per-class counts in train)\")\n",
    "print(\"   - class_weight_dict        (dict of computed weights from sklearn)\")\n",
    "print(\"   - X_train_scaled           (numpy array of scaled training features, shape = (n_train, n_features))\")\n",
    "print(\"   - X_val_scaled             (numpy array of scaled val features)\")\n",
    "print(\"   - X_test_scaled            (numpy array of scaled test features)\")\n",
    "print(\"   - X_train_scaled_df        (pandas DataFrame of scaled train features)\")\n",
    "print(\"   - scaler                  (fitted StandardScaler instance)\")\n",
    "\n",
    "# For convenience, also expose them in the current namespace (no-op if already present)\n",
    "_train_idx = (0, train_end)\n",
    "_val_idx = (train_end, val_end)\n",
    "_test_idx = (val_end, n_rows)\n",
    "\n",
    "# End of this code cell — ready for Part 5.\n"
   ],
   "id": "3afdc96547207ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 6 — Create sequences (WINDOW_SIZE -> X, FORECAST_HORIZON -> y)\n",
    "# Assumes the following variables already exist in the environment (from your Part 1-5):\n",
    "# WINDOW_SIZE = 120\n",
    "# FORECAST_HORIZON = 5\n",
    "# FEATURES = ['OPEN','HIGH','LOW','CLOSE','TICKVOL']\n",
    "# X_train_scaled, X_val_scaled, X_test_scaled    -> np.ndarray, shape: (n_rows_split, n_features)\n",
    "# y_train_labels, y_val_labels, y_test_labels    -> 1D array-like, shape: (n_rows_split,)\n",
    "#\n",
    "# Output variables created here (ready for Part 7):\n",
    "# X_train_seq, y_train_seq, y_train_seq_cat\n",
    "# X_val_seq,   y_val_seq,   y_val_seq_cat\n",
    "# X_test_seq,  y_test_seq,  y_test_seq_cat\n",
    "#\n",
    "# Notes:\n",
    "# - X_*_seq shapes -> (n_sequences, WINDOW_SIZE, n_features)\n",
    "# - y_*_seq shapes -> (n_sequences, FORECAST_HORIZON)  (integer labels 0/1/2)\n",
    "# - y_*_seq_cat shapes -> (n_sequences, FORECAST_HORIZON, n_classes) (one-hot encoded)\n",
    "# - We never cross split boundaries when creating sequences (each split processed independently).\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# --- Helper: sliding-window sequence creator for one split ---\n",
    "def create_sequences_from_split(X_split, y_split, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    X_split: 2D array (n_rows, n_features)\n",
    "    y_split: 1D array (n_rows,) integer labels\n",
    "    Returns:\n",
    "      X_seq: (n_sequences, window_size, n_features)\n",
    "      y_seq: (n_sequences, forecast_horizon) ints\n",
    "    \"\"\"\n",
    "    X_arr = np.asarray(X_split)\n",
    "    y_arr = np.asarray(y_split)\n",
    "\n",
    "    if X_arr.ndim != 2:\n",
    "        raise ValueError(f\"X_split must be 2D array, got shape {X_arr.shape}\")\n",
    "    if y_arr.ndim != 1:\n",
    "        raise ValueError(f\"y_split must be 1D array, got shape {y_arr.shape}\")\n",
    "    if X_arr.shape[0] != y_arr.shape[0]:\n",
    "        raise ValueError(f\"X and y must have same first-dimension length. X: {X_arr.shape[0]}, y: {y_arr.shape[0]}\")\n",
    "\n",
    "    n_rows = X_arr.shape[0]\n",
    "    last_start = n_rows - window_size - forecast_horizon  # inclusive max start index\n",
    "    if last_start < 0:\n",
    "        # Not enough rows to construct a single sequence\n",
    "        return np.empty((0, window_size, X_arr.shape[1])), np.empty((0, forecast_horizon), dtype=int)\n",
    "\n",
    "    n_sequences = last_start + 1\n",
    "    X_seq = np.empty((n_sequences, window_size, X_arr.shape[1]), dtype=X_arr.dtype)\n",
    "    y_seq = np.empty((n_sequences, forecast_horizon), dtype=int)\n",
    "\n",
    "    for i in range(n_sequences):\n",
    "        X_seq[i] = X_arr[i: i + window_size]\n",
    "        y_seq[i] = y_arr[i + window_size: i + window_size + forecast_horizon]\n",
    "\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "# --- Create sequences for each split ---\n",
    "X_train_seq, y_train_seq = create_sequences_from_split(X_train_scaled, y_train_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_val_seq, y_val_seq = create_sequences_from_split(X_val_scaled, y_val_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_test_seq, y_test_seq = create_sequences_from_split(X_test_scaled, y_test_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "\n",
    "# --- One-hot encode multi-step labels ---\n",
    "# Determine number of classes from training labels (safe default: 3 classes {0,1,2})\n",
    "classes_in_train = np.unique(y_train_seq) if y_train_seq.size > 0 else np.array([0, 1, 2])\n",
    "n_classes = int(max(classes_in_train.max(), 2) + 1)  # ensures at least 3 classes (0..2)\n",
    "\n",
    "\n",
    "# Convert to categorical: result shape (n_sequences, FORECAST_HORIZON, n_classes)\n",
    "def one_hot_multi_step(y_seq, n_classes):\n",
    "    if y_seq.size == 0:\n",
    "        return np.empty((0, y_seq.shape[1], n_classes), dtype=np.float32)\n",
    "    # to_categorical works on flattened array, then reshape\n",
    "    flat = to_categorical(y_seq.ravel(), num_classes=n_classes)\n",
    "    return flat.reshape((y_seq.shape[0], y_seq.shape[1], n_classes))\n",
    "\n",
    "\n",
    "y_train_seq_cat = one_hot_multi_step(y_train_seq, n_classes)\n",
    "y_val_seq_cat = one_hot_multi_step(y_val_seq, n_classes)\n",
    "y_test_seq_cat = one_hot_multi_step(y_test_seq, n_classes)\n",
    "\n",
    "\n",
    "# --- Useful diagnostics and prints required for Part 7 ---\n",
    "def seq_stats(X_seq, y_seq, name):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"X_{name}_seq.shape: {X_seq.shape}\")\n",
    "    print(f\"y_{name}_seq.shape: {y_seq.shape}\")\n",
    "    if y_seq.size > 0:\n",
    "        flattened = y_seq.ravel()\n",
    "        unique, counts = np.unique(flattened, return_counts=True)\n",
    "        dist = dict(zip([int(u) for u in unique], [int(c) for c in counts]))\n",
    "        total = flattened.size\n",
    "        print(f\"Label distribution across all forecast positions (counts): {dist}\")\n",
    "        print(\"Label distribution (percent):\", {int(u): round(c / total * 100, 4) for u, c in zip(unique, counts)})\n",
    "    else:\n",
    "        print(\"No sequences (empty).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "seq_stats(X_train_seq, y_train_seq, \"train\")\n",
    "seq_stats(X_val_seq, y_val_seq, \"val\")\n",
    "seq_stats(X_test_seq, y_test_seq, \"test\")\n",
    "\n",
    "print(\"n_classes (in training data / used for one-hot):\", n_classes)\n",
    "\n",
    "# Also print example shapes commonly referenced in Part 7:\n",
    "print(\"\\nVariables ready for Part 7 (Build / Train / Evaluate):\")\n",
    "print(\"X_train_seq :\", X_train_seq.shape)\n",
    "print(\"y_train_seq :\", y_train_seq.shape)\n",
    "print(\"y_train_seq_cat :\", y_train_seq_cat.shape)\n",
    "print(\"X_val_seq   :\", X_val_seq.shape)\n",
    "print(\"y_val_seq   :\", y_val_seq.shape)\n",
    "print(\"y_val_seq_cat   :\", y_val_seq_cat.shape)\n",
    "print(\"X_test_seq  :\", X_test_seq.shape)\n",
    "print(\"y_test_seq  :\", y_test_seq.shape)\n",
    "print(\"y_test_seq_cat  :\", y_test_seq_cat.shape)\n"
   ],
   "id": "a3d60357fcba0b3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Part 7 — Build, Train and Evaluate the model\n",
    "# Assumes the following variables are already present in the namespace:\n",
    "# X_train_seq, y_train_seq, y_train_seq_cat\n",
    "# X_val_seq,   y_val_seq,   y_val_seq_cat\n",
    "# X_test_seq,  y_test_seq,  y_test_seq_cat\n",
    "# and constants: WINDOW_SIZE = 120, FORECAST_HORIZON = 5, FEATURES = ['OPEN','HIGH','LOW','CLOSE','TICKVOL']\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Basic checks\n",
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"y_train_seq_cat shape:\", y_train_seq_cat.shape)\n",
    "print(\"X_val_seq shape:\", X_val_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "\n",
    "N_CLASSES = y_train_seq_cat.shape[-1]  # should be 3\n",
    "TIMESTEPS = y_train_seq_cat.shape[1]  # should be FORECAST_HORIZON\n",
    "\n",
    "# ---------------------------\n",
    "# Create timestep-aware sample weights to handle extreme imbalance\n",
    "# ---------------------------\n",
    "# Compute global class frequencies across all samples and timesteps in training set\n",
    "class_counts = np.sum(y_train_seq_cat, axis=(0, 1))  # shape (n_classes,)\n",
    "total_labels = np.sum(class_counts)\n",
    "print(\"Train class counts (sum across samples and timesteps):\", class_counts)\n",
    "\n",
    "# Avoid division by zero\n",
    "eps = 1e-8\n",
    "# Inverse-frequency weighting (normalized)\n",
    "inv_freq = (total_labels / (class_counts + eps))\n",
    "# normalize so that mean weight == 1 (keeps loss scale stable)\n",
    "inv_freq = inv_freq / np.mean(inv_freq)\n",
    "print(\"Per-class inverse-frequency weights:\", inv_freq)\n",
    "\n",
    "\n",
    "# Build sample_weight arrays with shape (n_samples, timesteps)\n",
    "def build_timestep_sample_weights(y_cat, inv_freq_array):\n",
    "    # y_cat shape: (n_samples, timesteps, n_classes)\n",
    "    n_samples, timesteps, n_classes = y_cat.shape\n",
    "    sw = np.zeros((n_samples, timesteps), dtype=np.float32)\n",
    "    # argmax to get the true class index per sample/timestep\n",
    "    true_classes = np.argmax(y_cat, axis=-1)  # shape (n_samples, timesteps)\n",
    "    for c in range(n_classes):\n",
    "        sw[true_classes == c] = inv_freq_array[c]\n",
    "    return sw\n",
    "\n",
    "\n",
    "sample_weight_train = build_timestep_sample_weights(y_train_seq_cat, inv_freq)\n",
    "sample_weight_val = build_timestep_sample_weights(y_val_seq_cat, inv_freq)\n",
    "sample_weight_test = build_timestep_sample_weights(y_test_seq_cat, inv_freq)\n",
    "\n",
    "print(\"sample_weight_train shape:\", sample_weight_train.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Build the model\n",
    "# Seq2Seq-ish model: Encoder LSTM -> RepeatVector -> Decoder LSTM (return_sequences=True) -> TimeDistributed(Dense)\n",
    "# ---------------------------\n",
    "INPUT_SHAPE = X_train_seq.shape[1:]  # (WINDOW_SIZE, n_features)\n",
    "EMBED_DIM = 128\n",
    "ENC_UNITS = 128\n",
    "DEC_UNITS = 128\n",
    "DROPOUT = 0.2\n",
    "\n",
    "inputs = layers.Input(shape=INPUT_SHAPE, name='inputs')\n",
    "# optional masking if there are padded sequences; here probably not needed but harmless\n",
    "x = layers.Masking(mask_value=0.0)(inputs)\n",
    "# Encoder\n",
    "x = layers.Bidirectional(layers.LSTM(ENC_UNITS, return_sequences=False, dropout=DROPOUT), name='encoder_bi')(x)\n",
    "# Project to embedding\n",
    "x = layers.Dense(EMBED_DIM, activation='relu', name='encoder_dense')(x)\n",
    "# Repeat for forecast horizon\n",
    "x = layers.RepeatVector(TIMESTEPS, name='repeat_vector')(x)\n",
    "# Decoder\n",
    "x = layers.LSTM(DEC_UNITS, return_sequences=True, dropout=DROPOUT, name='decoder_lstm')(x)\n",
    "# Optional TimeDistributed intermediate dense\n",
    "x = layers.TimeDistributed(layers.Dense(64, activation='relu'), name='td_dense')(x)\n",
    "# Final classification per timestep\n",
    "outputs = layers.TimeDistributed(layers.Dense(N_CLASSES, activation='softmax'), name='td_softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name='seq2seq_reversal_classifier')\n",
    "model.summary()"
   ],
   "id": "7e874016cf698b1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Compile\n",
    "# ---------------------------\n",
    "LR = 1e-3\n",
    "optimizer = optimizers.Adam(learning_rate=LR)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# Callbacks\n",
    "# ---------------------------\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Fit\n",
    "# Note: sample_weight for sequence outputs should be shape (n_samples, timesteps)\n",
    "# ---------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq_cat,\n",
    "    validation_data=(X_val_seq, y_val_seq_cat, sample_weight_val),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es, reduce_lr],\n",
    "    sample_weight=sample_weight_train,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTRAINING COMPLETE!\")\n"
   ],
   "id": "c58fc5bdcd9716a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Evaluate on test set\n",
    "# ---------------------------\n",
    "print('\\nEvaluating on test set...')\n",
    "eval_results = model.evaluate(X_test_seq, y_test_seq_cat, sample_weight=sample_weight_test, verbose=2)\n",
    "print('Test loss/metrics:', eval_results)\n",
    "\n",
    "# ---------------------------\n",
    "# Detailed classification report (flatten timesteps)\n",
    "# ---------------------------\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test_seq, batch_size=BATCH_SIZE)\n",
    "# y_pred_proba shape: (n_samples, timesteps, n_classes)\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1).reshape(-1)\n",
    "y_true = np.argmax(y_test_seq_cat, axis=-1).reshape(-1)\n",
    "\n",
    "print('\\nClassification report (flattened timesteps):')\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n"
   ],
   "id": "4c6d62ab188030b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timedelta\n",
    "N_FEATURES = len(FEATURES)  # 5\n",
    "# ============================================================================\n",
    "# PREDICTION SECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION ON UNSEEN DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# If given_time already exists, add 5 hours\n",
    "try:\n",
    "    dt = datetime.strptime(given_time, \"%Y.%m.%d %H:%M:%S\") + timedelta(hours=5)\n",
    "except NameError:\n",
    "    # First run: initialize given_time\n",
    "    dt = datetime.strptime(\"2025.08.13 21:00:00\", \"%Y.%m.%d %H:%M:%S\")\n",
    "\n",
    "# Store back as string\n",
    "given_time = dt.strftime(\"%Y.%m.%d %H:%M:%S\")\n",
    "print(f\"\\nGiven time: {given_time}\")"
   ],
   "id": "971f9353f0017d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Find the index of given_time in df (not df_model)\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "given_idx = df[df['DATETIME'] == given_time].index[0]\n",
    "\n",
    "print(f\"Given time index in df: {given_idx}\")\n",
    "\n",
    "# Extract 120 candles ending at given_time\n",
    "start_idx = given_idx - WINDOW_SIZE + 1\n",
    "end_idx = given_idx + 1\n",
    "\n",
    "input_df = df.iloc[start_idx:end_idx][['DATETIME'] + FEATURES].copy()\n",
    "print(f\"Input shape (before scaling): {input_df.shape}\")\n",
    "\n",
    "# Separate DATETIME from features for scaling\n",
    "input_candles = input_df.copy()  # Keep for visualization (has DATETIME)\n",
    "input_features_only = input_df[FEATURES]  # Only features for model\n",
    "\n",
    "# Scale using the same scaler from training (only the FEATURES columns)\n",
    "input_scaled = scaler.transform(input_features_only)\n",
    "input_scaled = input_scaled.reshape(1, WINDOW_SIZE, N_FEATURES)\n",
    "\n",
    "# Predict\n",
    "predictions_proba = model.predict(input_scaled, verbose=0)  # Shape: (1, 10, 3)\n",
    "predictions_proba = predictions_proba[0]  # Shape: (10, 3)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "# Create forecast datetimes (next 10 hours after given_time)\n",
    "given_datetime = pd.to_datetime(given_time)\n",
    "forecast_datetimes = [given_datetime + pd.Timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "# Create output DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': predicted_classes,\n",
    "    'prob_0': predictions_proba[:, 0],\n",
    "    'prob_1': predictions_proba[:, 1],\n",
    "    'prob_2': predictions_proba[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "predicted_df"
   ],
   "id": "d3c938f293d42d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d72e84c79eb4918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "df018d2be311917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(2).copy()"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "9c0c6eb2f4c47e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "d5c3eaaba6bb40e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model with Comprehensive Report\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# 1- Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 2- Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 3- Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ],
   "id": "33c1a03d562a3c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 4- Save scaler (IMPORTANT - needed for predictions!)\n",
    "import joblib\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 5- Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# 6- Save full history as JSON so it can be reloaded later\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "with open(history_json_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "print(f\"Full history object saved to: {history_json_path}\")"
   ],
   "id": "2d20b00ea8200f68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7 — Save Training Loss Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_plot_path = os.path.join(log_dir, \"training_loss.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Loss plot saved to: {loss_plot_path}\")"
   ],
   "id": "6301f264dfe54ebf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8 — Save Accuracy Plot\n",
    "acc_plot_path = os.path.join(log_dir, \"training_accuracy.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(acc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Accuracy plot saved to: {acc_plot_path}\")"
   ],
   "id": "707148790a4706c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion matrix (flattened)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('\\nConfusion matrix (flattened timesteps):')\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "# Path to save\n",
    "cm_plot_path = os.path.join(log_dir, \"confusion_matrix.png\")\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=['0', '1', '2'],\n",
    "    yticklabels=['0', '1', '2']\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Flattened Horizon)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Confusion matrix saved to: {cm_plot_path}\")\n"
   ],
   "id": "c2d10d34b0785e45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251216_183156.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251216_183156.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251216_183156_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9e1fb02288869c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
