{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8- PART 2 — CHRONOLOGICAL SPLITTING\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "LABEL_COL = 'Label'\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Compute split indices\n",
    "# ----------------------------\n",
    "n_total = len(df_model)\n",
    "\n",
    "train_end = int(n_total * TRAIN_RATIO)\n",
    "val_end = train_end + int(n_total * VAL_RATIO)\n",
    "\n",
    "# ----------------------------\n",
    "# Chronological split\n",
    "# ----------------------------\n",
    "df_train = df_model.iloc[:train_end].copy()\n",
    "df_val = df_model.iloc[train_end:val_end].copy()\n",
    "df_test = df_model.iloc[val_end:].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Separate features and labels\n",
    "# (to be used in PART 3: Scaling)\n",
    "# ----------------------------\n",
    "X_train_df = df_train[FEATURES].copy()\n",
    "y_train_df = df_train[LABEL_COL].copy()\n",
    "\n",
    "X_val_df = df_val[FEATURES].copy()\n",
    "y_val_df = df_val[LABEL_COL].copy()\n",
    "\n",
    "X_test_df = df_test[FEATURES].copy()\n",
    "y_test_df = df_test[LABEL_COL].copy()\n"
   ],
   "id": "ecdd55b73493676b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9- PART 3: FEATURE SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ----------------------------\n",
    "# Fit ONLY on training data\n",
    "# ----------------------------\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Transform validation & test\n",
    "# ----------------------------\n",
    "X_val_scaled = scaler.transform(X_val_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Convert labels to NumPy arrays\n",
    "# (important for sequence creation)\n",
    "# ----------------------------\n",
    "y_train = y_train_df.values\n",
    "y_val = y_val_df.values\n",
    "y_test = y_test_df.values\n",
    "\n",
    "# ----------------------------\n",
    "# Convert scaled features to NumPy arrays\n",
    "# ----------------------------\n",
    "X_train = np.asarray(X_train_scaled, dtype=np.float32)\n",
    "X_val = np.asarray(X_val_scaled, dtype=np.float32)\n",
    "X_test = np.asarray(X_test_scaled, dtype=np.float32)\n",
    "\n"
   ],
   "id": "53c784b9a8eb1763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 10- PART 4 — CREATE SEQUENCES (WINDOW_SIZE → X, FORECAST_HORIZON → y)\n",
    "\n",
    "WINDOW_SIZE = 120\n",
    "FORECAST_HORIZON = 5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sequence creation function (time-series safe, no shuffling)\n",
    "# ------------------------------------------------------------\n",
    "def create_sequences(X, y, window_size, forecast_horizon):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    max_start = len(X) - window_size - forecast_horizon + 1\n",
    "\n",
    "    for i in range(max_start):\n",
    "        X_seq.append(X[i: i + window_size])\n",
    "        y_seq.append(y[i + window_size: i + window_size + forecast_horizon])\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create sequences for train / validation / test\n",
    "# ------------------------------------------------------------\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    X_train, y_train, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_val_seq, y_val_seq = create_sequences(\n",
    "    X_val, y_val, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    X_test, y_test, WINDOW_SIZE, FORECAST_HORIZON\n",
    ")\n"
   ],
   "id": "eaf8da664ba1d14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- PART 5 — IMBALANCE HANDLING (Class-Weighted)\n",
    "\n",
    "# Rules enforced:\n",
    "#   - NO oversampling / undersampling / SMOTE\n",
    "#   - Class-weighted loss ONLY\n",
    "#   - Weights computed from y_train_seq ONLY\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# y_train_seq shape: (num_samples, FORECAST_HORIZON)\n",
    "# We must compute class weights from ALL future labels\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Flatten all forecast steps into one long label vector\n",
    "y_train_flat = y_train_seq.reshape(-1)\n",
    "\n",
    "# Unique classes (must be [0,1,2])\n",
    "classes = np.unique(y_train_flat)\n",
    "\n",
    "# Compute balanced class weights\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_flat\n",
    ")\n",
    "\n",
    "# Convert to Keras-compatible dict\n",
    "class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "print(\"=== CLASS DISTRIBUTION (TRAIN ONLY) ===\")\n",
    "unique, counts = np.unique(y_train_flat, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} samples\")\n",
    "\n",
    "print(\"\\n=== CLASS WEIGHTS (Keras compatible) ===\")\n",
    "for k, v in class_weights.items():\n",
    "    print(f\"Class {k}: {v:.4f}\")\n",
    "\n"
   ],
   "id": "dc90bc998db96a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 12- PART 6 — Build and Train the Model\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "\n",
    "class_weight_tensor = tf.constant(\n",
    "    [class_weights[0], class_weights[1], class_weights[2]],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Custom weighted sparse categorical cross-entropy\n",
    "# (supports multi-step sequence targets)\n",
    "# ============================================================\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def weighted_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: (batch, horizon)\n",
    "    y_pred: (batch, horizon, num_classes)\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Standard sparse categorical cross-entropy per timestep\n",
    "    scce = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=False\n",
    "    )  # shape: (batch, horizon)\n",
    "\n",
    "    # Gather class weights for each true label\n",
    "    weights = tf.gather(class_weight_tensor, y_true)  # (batch, horizon)\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_loss = scce * weights\n",
    "\n",
    "    return tf.reduce_mean(weighted_loss)\n"
   ],
   "id": "63bb67d52f9f988b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Architecture (Encoder → Sequence Classifier)\n",
    "\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE, len(FEATURES)))\n",
    "\n",
    "x = layers.LSTM(\n",
    "    128,\n",
    "    return_sequences=True,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(inputs)\n",
    "\n",
    "x = layers.LSTM(\n",
    "    64,\n",
    "    return_sequences=False,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2\n",
    ")(x)\n",
    "\n",
    "# Project to forecast horizon\n",
    "x = layers.Dense(FORECAST_HORIZON * 64, activation=\"relu\")(x)\n",
    "x = layers.Reshape((FORECAST_HORIZON, 64))(x)\n",
    "\n",
    "# Time-distributed classification head\n",
    "outputs = layers.TimeDistributed(\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    ")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)"
   ],
   "id": "57f665c22c5fa494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=weighted_sparse_categorical_crossentropy,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "2e8b1496e3f75e32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Callbacks\n",
    "\n",
    "cb_early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cb_reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "7f55b73cdd29ee6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[cb_early_stop, cb_reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\nTRAINING COMPLETE!\")"
   ],
   "id": "5c9625e3414766fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6036017d7bf93a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4c5bd4c62b298e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9ef84a945999a88",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
