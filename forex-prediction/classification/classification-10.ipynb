{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-10\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- https://chatgpt.com/c/690d9dcc-26cc-832f-8446-31080be617a7\n",
    "\n",
    "## next step:\n",
    "\n",
    "1- Improve labeling ( 3333 from 28-1111 )\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, RepeatVector\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(prices, window=6, threshold=0.0007):\n",
    "    \"\"\"\n",
    "    Labels trend reversals (1=Buy, 2=Sell) based on local mean shifts.\n",
    "    Smaller window & threshold increase sensitivity.\n",
    "    \"\"\"\n",
    "    prices = np.asarray(prices)\n",
    "    labels = [0] * len(prices)\n",
    "    prev_trend = 0  # 1 = up, -1 = down, 0 = unknown\n",
    "\n",
    "    for i in range(len(prices) - window):\n",
    "        past = prices[i:i + window // 2]\n",
    "        future = prices[i + window // 2:i + window]\n",
    "\n",
    "        past_mean = np.mean(past)\n",
    "        future_mean = np.mean(future)\n",
    "        change = (future_mean - past_mean) / past_mean\n",
    "\n",
    "        if change > threshold:\n",
    "            curr_trend = 1  # Uptrend\n",
    "        elif change < -threshold:\n",
    "            curr_trend = -1  # Downtrend\n",
    "        else:\n",
    "            curr_trend = 0  # No significant trend\n",
    "\n",
    "        # Detect reversal only when trend flips clearly\n",
    "        if prev_trend == -1 and curr_trend == 1:\n",
    "            labels[i + window // 2] = 1  # Buy\n",
    "        elif prev_trend == 1 and curr_trend == -1:\n",
    "            labels[i + window // 2] = 2  # Sell\n",
    "\n",
    "        if curr_trend != 0:\n",
    "            prev_trend = curr_trend\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values)"
   ],
   "id": "ae894c87b56a1558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_model['Label'].value_counts().sort_index())  # 0, 1, 2",
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_labeled_candles(df, n=190):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "9d9e40e665da9f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Imports & Hyperparams\n",
    "# --------------------------\n",
    "\n",
    "# Hyperparameters required by the user\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "FEATURES = ['OPEN','HIGH','LOW','CLOSE','TICKVOL']\n",
    "N_CLASSES = 3  # 0,1,2\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Assumptions: df_model exists\n",
    "# --------------------------\n",
    "# df_model must contain a DATETIME column (parseable), and Label column (0/1/2)\n",
    "# and the feature columns defined above. DATETIME is continuous hourly series.\n",
    "# Example: df_model['DATETIME'] dtype either datetime or string convertible to datetime.\n",
    "\n",
    "# If DATETIME is string, convert once:\n",
    "if not np.issubdtype(df_model['DATETIME'].dtype, np.datetime64):\n",
    "    # try parsing common formats including your example \"2025.07.24 11:00:00\"\n",
    "    df_model['DATETIME'] = pd.to_datetime(df_model['DATETIME'].astype(str).str.strip().str.replace('.', '-'),\n",
    "                                          errors='coerce', dayfirst=False)\n",
    "# Ensure sorted by DATETIME\n",
    "df_model = df_model.sort_values('DATETIME').reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# Prepare supervised sequences\n",
    "# --------------------------\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "n_rows = len(df_model)\n",
    "max_start = n_rows - WINDOW_SIZE - FORECAST_HORIZON + 1\n",
    "if max_start <= 0:\n",
    "    raise ValueError(\"df_model is too short for WINDOW_SIZE + FORECAST_HORIZON\")\n",
    "\n",
    "for start in range(max_start):\n",
    "    end = start + WINDOW_SIZE  # exclusive index for the window end\n",
    "    X_window = df_model.loc[start:end-1, FEATURES].values  # shape (WINDOW_SIZE, n_features)\n",
    "    y_window = df_model.loc[end:end+FORECAST_HORIZON-1, 'Label'].values  # length FORECAST_HORIZON\n",
    "    # Only keep windows where the y_window has valid labels (no NaNs)\n",
    "    if np.isnan(y_window).any():\n",
    "        continue\n",
    "    X_list.append(X_window)\n",
    "    y_list.append(y_window.astype(int))\n",
    "\n",
    "X = np.stack(X_list)  # shape (n_samples, WINDOW_SIZE, n_features)\n",
    "y = np.stack(y_list)  # shape (n_samples, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Prepared data shapes:\", X.shape, y.shape)\n",
    "\n",
    "# Convert y to one-hot for training with categorical_crossentropy\n",
    "y_onehot = np.array([to_categorical(sample, num_classes=N_CLASSES) for sample in y])  # (n_samples, H, C)\n",
    "\n",
    "# --------------------------\n",
    "# Train / Val split\n",
    "# --------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.15, random_state=SEED, shuffle=True)\n",
    "print(\"Train/Val shapes:\", X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "# --------------------------\n",
    "# Scaling features (fit on training only)\n",
    "# --------------------------\n",
    "# Fit scaler to training windows flattened (so scaler learns distribution per feature)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, len(FEATURES)))  # (n_samples * WINDOW_SIZE, n_features)\n",
    "\n",
    "def scale_windows(X_raw):\n",
    "    n_samples = X_raw.shape[0]\n",
    "    X_flat = X_raw.reshape(-1, len(FEATURES))\n",
    "    X_scaled_flat = scaler.transform(X_flat)\n",
    "    return X_scaled_flat.reshape(n_samples, WINDOW_SIZE, len(FEATURES))\n",
    "\n",
    "X_train_scaled = scale_windows(X_train)\n",
    "X_val_scaled = scale_windows(X_val)\n",
    "\n",
    "# --------------------------\n",
    "# Handle class imbalance (compute class weights then per-sample-per-timestep weights)\n",
    "# --------------------------\n",
    "# Flatten all target labels across train set to compute class frequency\n",
    "y_train_flat = np.argmax(y_train, axis=2).reshape(-1)  # flatten\n",
    "class_counts = np.bincount(y_train_flat, minlength=N_CLASSES).astype(float)\n",
    "class_freq = class_counts / class_counts.sum()\n",
    "# Inverse frequency (higher weight for rare classes)\n",
    "class_weights = {i: (1.0 / freq if freq > 0 else 1.0) for i, freq in enumerate(class_freq)}\n",
    "# Normalize weights to have mean 1\n",
    "mean_w = np.mean(list(class_weights.values()))\n",
    "class_weights = {k: (v / mean_w) for k, v in class_weights.items()}\n",
    "print(\"class_counts:\", class_counts, \"class_weights(normalized mean=1):\", class_weights)\n",
    "\n",
    "# Build sample_weight matrix: shape (n_samples, FORECAST_HORIZON)\n",
    "# Each timestep weight = class_weights[true_class]\n",
    "def build_sample_weights(y_onehot, class_weights_map):\n",
    "    y_idx = np.argmax(y_onehot, axis=2)  # (n_samples, H)\n",
    "    weights = np.vectorize(class_weights_map.get)(y_idx)\n",
    "    return weights.astype(np.float32)\n",
    "\n",
    "sample_weight_train = build_sample_weights(y_train, class_weights)\n",
    "sample_weight_val = build_sample_weights(y_val, class_weights)\n",
    "\n",
    "# --------------------------\n",
    "# Model: Encoder-Decoder LSTM -> TimeDistributed softmax\n",
    "# --------------------------\n",
    "n_features = len(FEATURES)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(WINDOW_SIZE, n_features), name='encoder_inputs')\n",
    "x = layers.Conv1D(filters=64, kernel_size=3, padding='causal', activation='relu')(encoder_inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)  # encoder output\n",
    "# Expand to decoder length\n",
    "x = layers.RepeatVector(FORECAST_HORIZON)(x)\n",
    "# Decoder\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x)\n",
    "decoder_outputs = layers.TimeDistributed(layers.Dense(N_CLASSES, activation='softmax'), name='decoder_outputs')(x)\n",
    "\n",
    "model = models.Model(encoder_inputs, decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "id": "e8b0e3f68b9fdc5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# Training callbacks\n",
    "# --------------------------\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "mc = callbacks.ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, save_format='keras')\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "# --------------------------\n",
    "# Train model\n",
    "# --------------------------\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val, sample_weight_val),\n",
    "    epochs=60,\n",
    "    batch_size=128,\n",
    "    callbacks=[es, mc, reduce_lr],\n",
    "    sample_weight=sample_weight_train,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# After training you have a model saved at best_model.keras (native Keras format).\n",
    "# You can also load with: model = tf.keras.models.load_model('best_model.keras')"
   ],
   "id": "473faffda7bf0631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# PREDICTION PIPELINE (for a single given_time)\n",
    "# --------------------------\n",
    "given_time = \"2025.07.24 11:00:00\"   # user-provided: note the space and dot-separated date\n",
    "# Parse given_time into datetime (try to match user's format)\n",
    "try:\n",
    "    # transform \"YYYY.MM.DD hh:mm:ss\" to a parseable format\n",
    "    gt = pd.to_datetime(given_time.replace('.', '-'), format=None, errors='coerce')\n",
    "    if pd.isna(gt):\n",
    "        # fallback generic parse\n",
    "        gt = pd.to_datetime(given_time, errors='coerce')\n",
    "except Exception:\n",
    "    gt = pd.to_datetime(given_time, errors='coerce')\n",
    "\n",
    "if pd.isna(gt):\n",
    "    raise ValueError(f\"given_time could not be parsed: {given_time}\")\n",
    "\n",
    "# Find the index of the row whose DATETIME equals given_time (we assumed DATETIME column matches)\n",
    "match_idx = df_model.index[df_model['DATETIME'] == gt]\n",
    "if len(match_idx) == 0:\n",
    "    # if exact match not found, try to find the last row <= given_time\n",
    "    match_idx = df_model.index[df_model['DATETIME'] <= gt]\n",
    "    if len(match_idx) == 0:\n",
    "        raise ValueError(f\"No row at or before given_time {gt} found in df_model['DATETIME']\")\n",
    "    idx = match_idx[-1]\n",
    "    # warn that we used the last available <= given_time\n",
    "    print(f\"Warning: exact given_time not found. Using last available index at {df_model.loc[idx,'DATETIME']}\")\n",
    "else:\n",
    "    idx = match_idx[0]\n",
    "\n",
    "# Determine start index for the 60-candle window (inclusive of idx)\n",
    "start_idx = idx - WINDOW_SIZE + 1\n",
    "if start_idx < 0:\n",
    "    raise ValueError(\"Not enough history before given_time to form a full WINDOW_SIZE input\")\n",
    "\n",
    "X_input_raw = df_model.loc[start_idx:idx, FEATURES].values  # shape (WINDOW_SIZE, n_features)\n",
    "if X_input_raw.shape[0] != WINDOW_SIZE:\n",
    "    raise ValueError(\"Input window length mismatch (not equal to WINDOW_SIZE)\")\n",
    "\n",
    "# Scale using the previously-fitted scaler\n",
    "X_input_scaled = scaler.transform(X_input_raw.reshape(-1, n_features)).reshape(1, WINDOW_SIZE, n_features)\n",
    "\n",
    "# Model prediction\n",
    "y_pred_prob = model.predict(X_input_scaled)  # shape (1, FORECAST_HORIZON, N_CLASSES)\n",
    "y_pred_prob = y_pred_prob[0]  # (FORECAST_HORIZON, N_CLASSES)\n",
    "y_pred_classes = np.argmax(y_pred_prob, axis=1)  # length FORECAST_HORIZON\n",
    "\n",
    "# Enforce \"realistic\" behavior: user requested that most steps be 0.\n",
    "# The model already learns imbalance, but we can optionally post-process:\n",
    "# Optionally suppress very low-confidence non-zero predictions:\n",
    "# If desired, uncomment the following lines to force a non-zero class only if prob > threshold\n",
    "#threshold = 0.50\n",
    "#for i in range(FORECAST_HORIZON):\n",
    "#    top_prob = np.max(y_pred_prob[i])\n",
    "#    top_class = np.argmax(y_pred_prob[i])\n",
    "#    if top_class != 0 and top_prob < threshold:\n",
    "#        y_pred_classes[i] = 0\n",
    "\n",
    "# Build forecast datetimes: next 10 hourly steps immediately after given_time\n",
    "first_forecast_dt = gt + pd.Timedelta(hours=1)\n",
    "forecast_datetimes = pd.date_range(start=first_forecast_dt, periods=FORECAST_HORIZON, freq='H')\n",
    "\n",
    "# Build predicted_df\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': y_pred_classes,\n",
    "    'prob_0': y_pred_prob[:, 0],\n",
    "    'prob_1': y_pred_prob[:, 1],\n",
    "    'prob_2': y_pred_prob[:, 2],\n",
    "})\n",
    "\n",
    "# final line to show predicted_df (Jupyter will display)\n",
    "predicted_df\n"
   ],
   "id": "47dbd04da25bd272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# --- 1. Historical window (last 4 real candles before forecast) ---\n",
    "# --- Find the starting index ---\n",
    "start_idx = df.index[df['DATETIME'] == pd.to_datetime(given_time)][0]\n",
    "\n",
    "# --- 1. Extract next n candles ---\n",
    "input_df = df.iloc[start_idx: start_idx + WINDOW_SIZE].copy()\n",
    "\n",
    "historical_df = input_df.tail(4).copy()\n",
    "historical_df"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = start_idx\n",
    "actual_future_end = start_idx + FORECAST_HORIZON\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "actual_future_df"
   ],
   "id": "7485efcf60503713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. Create predicted_df (forecast for next 10 hours) ---\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(\n",
    "    start=last_timestamp + pd.Timedelta(hours=1),\n",
    "    periods=FORECAST_HORIZON,\n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "c27312e46eb98ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "640f86f86378887c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# 11-4 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# 11-5 Save training loss plot\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 4. Evaluate on validation set (since no X_test/y_test defined)\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_acc = history.history['cat_acc'][-1]\n",
    "final_val_loss, final_val_acc = model.evaluate(X_val_scaled, y_val_cat, verbose=0)\n",
    "\n",
    "# 5. Save model summary and performance metrics\n",
    "summary_path = os.path.join(log_dir, 'model_log.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write('\\n')\n",
    "    f.write(f'Final Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Training Accuracy: {final_train_acc:.6f}\\n')\n",
    "    f.write(f'Final Validation Loss: {final_val_loss:.6f}\\n')\n",
    "    f.write(f'Final Validation Accuracy: {final_val_acc:.6f}\\n')"
   ],
   "id": "c60adc7617c16b75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = 'saved_models/model_20251106_214146.keras'\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={'loss_fn': focal_loss(), 'focal_loss': focal_loss()},\n",
    "    safe_mode=False\n",
    ")"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
