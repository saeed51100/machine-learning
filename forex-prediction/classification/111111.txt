I have written a time-series classification program in Python (Jupyter Notebook) and need help diagnosing why it produces poor results.
Below, I will share three parts for review.

1. Program Description

* The dataset contains a CLOSE price column and a Label column indicating price trend-reversal events with three classes:
    0: no reversal
    1: bullish reversal
    2: bearish reversal

    Labels 1 and 2 mark reversal points (peaks and valleys); all other rows are labeled 0, resulting in a highly imbalanced dataset.
    The modelâ€™s task is to learn patterns that indicate trend reversals and generalize them to unseen data.

* Prediction Task
    - Input: 120 consecutive hourly candles
    - Output: a sequence of 5 labels predicting reversal signals for the next 5 hours:
        0 = no signal
        1 = buy
        2 = sell

* Dataset Details

    - Original DataFrame (df): 140,000 rows, 10 columns
         (DATETIME, DATE, TIME, OPEN, HIGH, LOW, CLOSE, TICKVOL, VOL, SPREAD)
    - Timeframe: H1 (hourly Forex OHLCV data)
    - Data is continuous (holiday gaps forward-filled)
    - DATETIME is strictly chronological and used as the time index

* Training Setup
   - First 130,000 rows copied to df_model, with a Label column added
   - df_model is used for training, validation, and testing
   - Remaining 10,000 rows are completely unseen and reserved for real-world evaluation

* Label Distribution (df_model)
    Class 0: ~98.55%
    Class 1: ~0.73%
    Class 2: ~0.73%

* Data Splitting Constraints
    - Strict chronological splitting only

    - Either:
       - 70% train / 15% validation / 15% test, or
       - walk-forward validation
    - No shuffling allowed

2. Program Summary
    - (Approximately 300 lines of code)

3. Training Results
    * Final Performance
        - Training Loss: 1.0216
        - Test Loss: 1.2550
        - Test Accuracy: 0.2839

Confusion Matrix (Flattened Timesteps)
    [[30067 10205 66982]
     [   39   172   739]
     [   55   142   749]]