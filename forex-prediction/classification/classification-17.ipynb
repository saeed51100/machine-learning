{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-16\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- https://claude.ai/chat/f3657f31-a334-4ad9-904d-6ca86b00103c\n",
    "\n",
    "2- improve save and load 2 and reports\n",
    "\n",
    "## next step:\n",
    "\n",
    "1-\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, RepeatVector, Dropout, \\\n",
    "    BatchNormalization\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter, find_peaks, peak_prominences\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(\n",
    "        close,\n",
    "        high=None,\n",
    "        low=None,\n",
    "        smoothing_window=31,\n",
    "        polyorder=3,\n",
    "        base_prom_factor=0.02,\n",
    "        distance=3,\n",
    "        snap_window=5,\n",
    "        min_dev_pct=0.0015,  # 0.15% minimum leg size\n",
    "        min_dev_sigma=2.0,  # >= 2x local abs-return EMA\n",
    "        vol_window=100,  # EMA window for local volatility\n",
    "        verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Label reversal points with improved accuracy.\n",
    "\n",
    "    Returns labels array of length n where:\n",
    "    0 = none, 1 = valley, 2 = peak.\n",
    "\n",
    "    Tips:\n",
    "    - For best accuracy, pass high/low arrays from your OHLCV.\n",
    "      Example: label_reversal_points(df['CLOSE'], df['HIGH'], df['LOW'])\n",
    "    - Tune min_dev_pct / min_dev_sigma to be stricter or looser on swing size.\n",
    "    \"\"\"\n",
    "    close = np.asarray(close, dtype=float)\n",
    "    n = close.size\n",
    "    if n < 3:\n",
    "        return np.zeros(n, dtype=int)\n",
    "\n",
    "    # Interpolate NaNs if any\n",
    "    if np.isnan(close).any():\n",
    "        idx = np.arange(n)\n",
    "        good = ~np.isnan(close)\n",
    "        close = close.copy()\n",
    "        close[~good] = np.interp(idx[~good], idx[good], close[good])\n",
    "\n",
    "    # Helper: simple EMA for local abs-return volatility\n",
    "    def ema(x, span):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        alpha = 2.0 / (span + 1.0)\n",
    "        out = np.empty_like(x)\n",
    "        out[0] = x[0]\n",
    "        for i in range(1, len(x)):\n",
    "            out[i] = alpha * x[i] + (1 - alpha) * out[i - 1]\n",
    "        return out\n",
    "\n",
    "    # Local volatility in price terms via EMA of absolute returns\n",
    "    ret = np.zeros(n)\n",
    "    ret[1:] = np.abs(np.diff(close) / np.maximum(1e-12, close[:-1]))\n",
    "    vol_absret = ema(ret, vol_window)\n",
    "    local_vol_price = vol_absret * close  # convert to price units\n",
    "\n",
    "    # Smoothing to get robust candidates\n",
    "    win = smoothing_window\n",
    "    if win >= n:\n",
    "        win = n - 1 if (n - 1) % 2 == 1 else n - 2\n",
    "    if win % 2 == 0:\n",
    "        win += 1\n",
    "    smoothed = savgol_filter(close, win, polyorder)\n",
    "\n",
    "    # Base prominence threshold\n",
    "    global_std = np.std(close) or 1.0\n",
    "    prom = global_std * base_prom_factor\n",
    "\n",
    "    # Candidate peaks/valleys on smoothed\n",
    "    peak_idx, _ = find_peaks(smoothed, distance=distance, prominence=prom)\n",
    "    val_idx, _ = find_peaks(-smoothed, distance=distance, prominence=prom)\n",
    "\n",
    "    # Prominences for tie-breaking\n",
    "    peak_prom = peak_prominences(smoothed, peak_idx)[0] if peak_idx.size else np.array([])\n",
    "    val_prom = peak_prominences(-smoothed, val_idx)[0] if val_idx.size else np.array([])\n",
    "\n",
    "    # Combine\n",
    "    candidates = []\n",
    "    for i, p in enumerate(peak_idx):\n",
    "        candidates.append((int(p), 2, float(peak_prom[i]) if peak_prom.size else 0.0))\n",
    "    for i, v in enumerate(val_idx):\n",
    "        candidates.append((int(v), 1, float(val_prom[i]) if val_prom.size else 0.0))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "\n",
    "    if not candidates:\n",
    "        labels = np.zeros(n, dtype=int)\n",
    "        # still mark edges for completeness\n",
    "        labels[0] = 1 if close[1] > close[0] else 2\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "        return labels\n",
    "\n",
    "    # Enforce alternation (remove weaker when two same-type neighbors)\n",
    "    def enforce_alternation(ext):\n",
    "        ext = ext[:]  # list of (idx, typ, prom)\n",
    "        while True:\n",
    "            removed = False\n",
    "            i = 0\n",
    "            while i < len(ext) - 1:\n",
    "                if ext[i][1] == ext[i + 1][1]:\n",
    "                    # drop the smaller prominence\n",
    "                    if ext[i][2] < ext[i + 1][2]:\n",
    "                        ext.pop(i)\n",
    "                    else:\n",
    "                        ext.pop(i + 1)\n",
    "                    removed = True\n",
    "                else:\n",
    "                    i += 1\n",
    "            if not removed:\n",
    "                break\n",
    "        return ext\n",
    "\n",
    "    candidates = enforce_alternation(candidates)\n",
    "\n",
    "    # SNAP: move each extreme to the true local extremum on raw close (or HIGH/LOW)\n",
    "    def snap_index(idx, typ):\n",
    "        L = max(0, idx - snap_window)\n",
    "        R = min(n, idx + snap_window + 1)\n",
    "        if high is not None and low is not None:\n",
    "            if typ == 2:  # peak\n",
    "                j = np.argmax(np.asarray(high[L:R], dtype=float))\n",
    "            else:  # valley\n",
    "                j = np.argmin(np.asarray(low[L:R], dtype=float))\n",
    "        else:\n",
    "            if typ == 2:\n",
    "                j = np.argmax(close[L:R])\n",
    "            else:\n",
    "                j = np.argmin(close[L:R])\n",
    "        return L + int(j)\n",
    "\n",
    "    snapped = []\n",
    "    seen_at = {}  # avoid duplicate indices by keeping stronger prominence\n",
    "    for idx, typ, pr in candidates:\n",
    "        j = snap_index(idx, typ)\n",
    "        key = (j, typ)\n",
    "        if key not in seen_at or pr > seen_at[key][2]:\n",
    "            seen_at[key] = (j, typ, pr)\n",
    "    snapped = sorted(seen_at.values(), key=lambda x: x[0])\n",
    "\n",
    "    # Enforce alternation again after snapping\n",
    "    snapped = enforce_alternation(snapped)\n",
    "\n",
    "    # Filter micro-legs using adaptive threshold (min % move and sigma*local_vol)\n",
    "    pruned = []\n",
    "    for idx, typ, pr in snapped:\n",
    "        if not pruned:\n",
    "            pruned.append((idx, typ, pr))\n",
    "            continue\n",
    "        prev_idx, prev_typ, prev_pr = pruned[-1]\n",
    "        # time spacing\n",
    "        if idx - prev_idx < distance:\n",
    "            # keep the more prominent of the two\n",
    "            if pr > prev_pr:\n",
    "                pruned[-1] = (idx, typ, pr)\n",
    "            continue\n",
    "        leg = abs(close[idx] - close[prev_idx])\n",
    "        # thresholds at both ends\n",
    "        thr = max(min_dev_pct * close[prev_idx],\n",
    "                  min_dev_sigma * max(local_vol_price[prev_idx], 1e-12))\n",
    "        thr = max(thr, max(min_dev_pct * close[idx],\n",
    "                           min_dev_sigma * max(local_vol_price[idx], 1e-12)))\n",
    "        if leg >= thr:\n",
    "            pruned.append((idx, typ, pr))\n",
    "        else:\n",
    "            # too small swing â†’ drop the later point\n",
    "            continue\n",
    "\n",
    "    # One more alternation pass (paranoid) and spacing check\n",
    "    pruned = enforce_alternation(pruned)\n",
    "    final_ext = []\n",
    "    for idx, typ, pr in pruned:\n",
    "        if final_ext and idx - final_ext[-1][0] < distance:\n",
    "            # keep stronger\n",
    "            if pr > final_ext[-1][2]:\n",
    "                final_ext[-1] = (idx, typ, pr)\n",
    "        else:\n",
    "            final_ext.append((idx, typ, pr))\n",
    "\n",
    "    # Build labels\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    for idx, typ, _ in final_ext:\n",
    "        labels[idx] = typ\n",
    "\n",
    "    # Mark edges as trend boundaries for continuity\n",
    "    if labels[0] == 0:\n",
    "        labels[0] = 1 if close[min(1, n - 1)] > close[0] else 2\n",
    "    if labels[-1] == 0 and n >= 2:\n",
    "        labels[-1] = 1 if close[-1] > close[-2] else 2\n",
    "\n",
    "    if verbose:\n",
    "        c0 = int((labels == 0).sum())\n",
    "        c1 = int((labels == 1).sum())\n",
    "        c2 = int((labels == 2).sum())\n",
    "        print(f\"labels -> 0:{c0}  1:{c1}  2:{c2}  (extrema kept: {len(final_ext)})\")\n",
    "\n",
    "    return labels\n"
   ],
   "id": "cf03646179e62d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# baseline (close-only)\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values, verbose=True)\n",
    "\n",
    "# inspect counts\n",
    "print(df_model['Label'].value_counts())"
   ],
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display label distribution in df_model\n",
    "label_counts = df_model['Label'].value_counts().sort_index()\n",
    "label_percentages = (df_model['Label'].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "print(\"Label Distribution in df_model:\")\n",
    "print(\"-\" * 40)\n",
    "for label in sorted(df_model['Label'].unique()):\n",
    "    count = label_counts[label]\n",
    "    percentage = label_percentages[label]\n",
    "    print(f\"Class {label}: {count:,} rows ({percentage:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total rows: {len(df_model):,}\")\n"
   ],
   "id": "8e1d8ac369134288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_labeled_candles(df_model, n=1000):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df_model.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "f29bcb88e0fcade0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# CONSTANTS\n",
    "# ============================================================================\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "N_FEATURES = len(FEATURES)\n",
    "N_CLASSES = 3"
   ],
   "id": "7ef8f174a24aafce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_sequences(data, labels, window_size, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences for multi-step classification.\n",
    "    Returns X with shape (n_samples, window_size, n_features)\n",
    "    and y with shape (n_samples, forecast_horizon, n_classes) for multi-label output\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - window_size - forecast_horizon + 1):\n",
    "        # Input: 60 consecutive candles\n",
    "        X.append(data[i:i + window_size])\n",
    "\n",
    "        # Output: next 10 labels (one-hot encoded)\n",
    "        future_labels = labels[i + window_size:i + window_size + forecast_horizon]\n",
    "        y.append(future_labels)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def prepare_data(df_model, features=FEATURES):\n",
    "    \"\"\"\n",
    "    Prepare training, validation, and test sets with chronological splitting.\n",
    "    \"\"\"\n",
    "    # Extract features and labels\n",
    "    feature_data = df_model[features].values\n",
    "    labels = df_model['Label'].values\n",
    "\n",
    "    # Normalize features using RobustScaler (better for outliers)\n",
    "    scaler = RobustScaler()\n",
    "    feature_data_scaled = scaler.fit_transform(feature_data)\n",
    "\n",
    "    # Chronological split: 70% train, 15% val, 15% test\n",
    "    n_total = len(df_model)\n",
    "    train_end = int(n_total * 0.70)\n",
    "    val_end = int(n_total * 0.85)\n",
    "\n",
    "    train_data = feature_data_scaled[:train_end]\n",
    "    train_labels = labels[:train_end]\n",
    "\n",
    "    val_data = feature_data_scaled[train_end:val_end]\n",
    "    val_labels = labels[train_end:val_end]\n",
    "\n",
    "    test_data = feature_data_scaled[val_end:]\n",
    "    test_labels = labels[val_end:]\n",
    "\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, train_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "    X_val, y_val = create_sequences(val_data, val_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "    X_test, y_test = create_sequences(test_data, test_labels, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "    print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler"
   ],
   "id": "5c6798b1103bf067",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "def build_reversal_model(window_size=WINDOW_SIZE, n_features=N_FEATURES,\n",
    "                         forecast_horizon=FORECAST_HORIZON, n_classes=N_CLASSES):\n",
    "    \"\"\"\n",
    "    Build a hybrid CNN-LSTM model for multi-step time series classification.\n",
    "\n",
    "    Architecture rationale:\n",
    "    - CNN layers: Extract local patterns in price action\n",
    "    - LSTM layers: Capture temporal dependencies\n",
    "    - Attention: Focus on important time steps\n",
    "    - Multi-output: Predict all 10 steps simultaneously\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = layers.Input(shape=(window_size, n_features))\n",
    "\n",
    "    # Temporal Convolutional layers for local pattern extraction\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Bidirectional LSTM for temporal dependencies\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer: forecast_horizon * n_classes neurons\n",
    "    # Reshape to (batch, forecast_horizon, n_classes)\n",
    "    x = layers.Dense(forecast_horizon * n_classes)(x)\n",
    "    outputs = layers.Reshape((forecast_horizon, n_classes))(x)\n",
    "    outputs = layers.Activation('softmax', name='output')(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ],
   "id": "2d9293804a3dd300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, class_weights=None):\n",
    "    \"\"\"\n",
    "    Train the reversal classification model.\n",
    "    \"\"\"\n",
    "    # Build model\n",
    "    model = build_reversal_model()\n",
    "\n",
    "    # Calculate class weights for imbalanced data\n",
    "    # Flatten labels for class weight calculation\n",
    "    labels_flat = y_train.flatten()\n",
    "    classes = np.unique(labels_flat)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=labels_flat)\n",
    "    class_weight_dict = {i: weights[i] for i in range(len(weights))}\n",
    "\n",
    "    print(f\"\\nClass weights: {class_weight_dict}\")\n",
    "\n",
    "    # Convert y to one-hot encoding for training\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=N_CLASSES)\n",
    "    y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=N_CLASSES)\n",
    "\n",
    "    # Compile model with focal loss to handle class imbalance\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',  # For multi-class classification\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train_onehot,\n",
    "        validation_data=(X_val, y_val_onehot),\n",
    "        epochs=2,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history"
   ],
   "id": "681517acfc6bc0d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL SECTION - MAIN TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FOREX TREND REVERSAL CLASSIFIER - MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data (assuming df_model is already loaded)\n",
    "print(\"\\n[1/3] Preparing data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler = prepare_data(df_model)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n[2/3] Training model...\")\n",
    "model, history = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n[3/3] Evaluating on test set...\")\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=N_CLASSES)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "d6fabfd744c1cb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# PREDICTION SECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION ON UNSEEN DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "given_time = \"2025.08.13 21:00:00\"\n",
    "print(f\"\\nGiven time: {given_time}\")\n",
    "\n",
    "# Find the index of given_time in df (not df_model)\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "given_idx = df[df['DATETIME'] == given_time].index[0]\n",
    "\n",
    "print(f\"Given time index in df: {given_idx}\")\n",
    "\n",
    "# Extract 60 candles ending at given_time\n",
    "start_idx = given_idx - WINDOW_SIZE + 1\n",
    "end_idx = given_idx + 1\n",
    "\n",
    "input_df = df.iloc[start_idx:end_idx][['DATETIME'] + FEATURES].copy()\n",
    "print(f\"Input shape (before scaling): {input_df.shape}\")\n",
    "\n",
    "# Separate DATETIME from features for scaling\n",
    "input_candles = input_df.copy()  # Keep for visualization (has DATETIME)\n",
    "input_features_only = input_df[FEATURES].values  # Only features for model\n",
    "\n",
    "# Scale using the same scaler from training (only the FEATURES columns)\n",
    "input_scaled = scaler.transform(input_features_only)\n",
    "input_scaled = input_scaled.reshape(1, WINDOW_SIZE, N_FEATURES)\n",
    "\n",
    "# Predict\n",
    "predictions_proba = model.predict(input_scaled, verbose=0)  # Shape: (1, 10, 3)\n",
    "predictions_proba = predictions_proba[0]  # Shape: (10, 3)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions_proba, axis=1)\n",
    "\n",
    "# Create forecast datetimes (next 10 hours after given_time)\n",
    "given_datetime = pd.to_datetime(given_time)\n",
    "forecast_datetimes = [given_datetime + pd.Timedelta(hours=i + 1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "# Create output DataFrame\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': predicted_classes,\n",
    "    'prob_0': predictions_proba[:, 0],\n",
    "    'prob_1': predictions_proba[:, 1],\n",
    "    'prob_2': predictions_proba[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "predicted_df"
   ],
   "id": "c14b8cbd611fa3f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ecb76553b6c2927c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "437f948703b12210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87e662f3d2138b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "historical_df = input_df.tail(4).copy()"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "historical_df",
   "id": "40247f9f71a52d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = given_idx + 1\n",
    "actual_future_end = given_idx + FORECAST_HORIZON + 1\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "\n"
   ],
   "id": "d897696834d52398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actual_future_df",
   "id": "59e6a63085d42a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. Create predicted_df (forecast for next 10 hours) ---\n",
    "last_timestamp = pd.to_datetime(df.loc[given_idx, 'DATETIME'])\n",
    "datetime_index = pd.date_range(\n",
    "    start=last_timestamp + pd.Timedelta(hours=1),\n",
    "    periods=FORECAST_HORIZON,\n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "c27312e46eb98ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "640f86f86378887c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model with Comprehensive Report\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "print(f\"\\n[SAVING MODEL]\")\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# 11-4 Save scaler (IMPORTANT - needed for predictions!)\n",
    "import joblib\n",
    "\n",
    "scaler_path = os.path.join('saved_models', f'scaler_{timestamp}.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 11-5 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# 11-6 Save training loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-7 Save accuracy plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_accuracy.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-8 Evaluate on validation set\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=N_CLASSES)\n",
    "eval_results = model.evaluate(X_val, y_val_onehot, verbose=0)\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_loss = eval_results[0]\n",
    "final_val_acc = eval_results[1]\n",
    "\n",
    "# 11-9 Generate detailed predictions for per-class analysis\n",
    "print(\"\\n[GENERATING DETAILED METRICS]\")\n",
    "y_val_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_val_pred = np.argmax(y_val_pred_proba, axis=-1)\n",
    "\n",
    "# Flatten predictions and true labels for sklearn metrics\n",
    "y_val_pred_flat = y_val_pred.flatten()\n",
    "y_val_true_flat = y_val.flatten()\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(\n",
    "    y_val_true_flat,\n",
    "    y_val_pred_flat,\n",
    "    target_names=['Class 0 (No Signal)', 'Class 1 (Buy)', 'Class 2 (Sell)'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_val_true_flat, y_val_pred_flat)\n",
    "\n",
    "# Calculate class distributions\n",
    "train_class_dist = np.bincount(y_train.flatten()) / len(y_train.flatten()) * 100\n",
    "val_class_dist = np.bincount(y_val.flatten()) / len(y_val.flatten()) * 100\n",
    "test_class_dist = np.bincount(y_test.flatten()) / len(y_test.flatten()) * 100\n",
    "\n",
    "# Get class weights used during training\n",
    "labels_flat = y_train.flatten()\n",
    "classes = np.unique(labels_flat)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = compute_class_weight('balanced', classes=classes, y=labels_flat)\n",
    "class_weight_dict = {i: weights[i] for i in range(len(weights))}\n",
    "\n",
    "# Calculate training time from history\n",
    "epochs_trained = len(history.history['loss'])\n",
    "\n",
    "# 11-10 Save confusion matrix plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(log_dir, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 11-11 Save per-class performance plot\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_val_true_flat, y_val_pred_flat, average=None\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "metrics = [precision, recall, f1]\n",
    "metric_names = ['Precision', 'Recall', 'F1-Score']\n",
    "colors_bar = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    axes[idx].bar(['Class 0', 'Class 1', 'Class 2'], metric, color=colors_bar)\n",
    "    axes[idx].set_title(f'{name} by Class')\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].set_ylabel(name)\n",
    "    for i, v in enumerate(metric):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(log_dir, 'per_class_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 11-12 Create comprehensive report\n",
    "comprehensive_report = f\"\"\"\n",
    "{'=' * 80}\n",
    "MODEL TRAINING REPORT\n",
    "{'=' * 80}\n",
    "Timestamp: {timestamp}\n",
    "Model Path: {model_path}\n",
    "Scaler Path: {scaler_path}\n",
    "\n",
    "{'=' * 80}\n",
    "DATA CONFIGURATION\n",
    "{'=' * 80}\n",
    "Total Samples in df_model: {len(df_model):,}\n",
    "\n",
    "Training Samples: {len(X_train):,} ({len(X_train) / len(df_model) * 100:.1f}%)\n",
    "Validation Samples: {len(X_val):,} ({len(X_val) / len(df_model) * 100:.1f}%)\n",
    "Test Samples: {len(X_test):,} ({len(X_test) / len(df_model) * 100:.1f}%)\n",
    "\n",
    "CLASS DISTRIBUTION:\n",
    "Training Set:\n",
    "  - Class 0 (No Signal): {train_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {train_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {train_class_dist[2]:.2f}%\n",
    "\n",
    "Validation Set:\n",
    "  - Class 0 (No Signal): {val_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {val_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {val_class_dist[2]:.2f}%\n",
    "\n",
    "Test Set:\n",
    "  - Class 0 (No Signal): {test_class_dist[0]:.2f}%\n",
    "  - Class 1 (Buy Reversal): {test_class_dist[1]:.2f}%\n",
    "  - Class 2 (Sell Reversal): {test_class_dist[2]:.2f}%\n",
    "\n",
    "FEATURE CONFIGURATION:\n",
    "Features Used: {', '.join(FEATURES)}\n",
    "Window Size: {WINDOW_SIZE} hours\n",
    "Forecast Horizon: {FORECAST_HORIZON} hours\n",
    "\n",
    "{'=' * 80}\n",
    "TRAINING CONFIGURATION\n",
    "{'=' * 80}\n",
    "Optimizer: Adam (initial lr=0.001)\n",
    "Loss Function: Categorical Crossentropy\n",
    "Batch Size: 64\n",
    "Early Stopping: patience=15, monitor=val_loss\n",
    "Reduce LR: patience=7, factor=0.5\n",
    "\n",
    "CLASS WEIGHTS (for handling imbalance):\n",
    "  Class 0: {class_weight_dict[0]:.4f}\n",
    "  Class 1: {class_weight_dict[1]:.4f}\n",
    "  Class 2: {class_weight_dict[2]:.4f}\n",
    "\n",
    "TRAINING PROGRESS:\n",
    "Epochs Trained: {epochs_trained} / 100\n",
    "Best Validation Loss Epoch: {np.argmin(history.history['val_loss']) + 1}\n",
    "\n",
    "{'=' * 80}\n",
    "MODEL ARCHITECTURE\n",
    "{'=' * 80}\n",
    "Total Parameters: {model.count_params():,}\n",
    "Trainable Parameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}\n",
    "\n",
    "{'=' * 80}\n",
    "OVERALL METRICS\n",
    "{'=' * 80}\n",
    "Final Training Loss: {final_train_loss:.6f}\n",
    "Final Training Accuracy: {final_train_acc:.6f}\n",
    "Final Validation Loss: {final_val_loss:.6f}\n",
    "Final Validation Accuracy: {final_val_acc:.6f}\n",
    "\n",
    "{'=' * 80}\n",
    "PER-CLASS PERFORMANCE (Validation Set)\n",
    "{'=' * 80}\n",
    "{class_report}\n",
    "\n",
    "{'=' * 80}\n",
    "CONFUSION MATRIX (Validation Set)\n",
    "{'=' * 80}\n",
    "                Predicted\n",
    "              Class 0  Class 1  Class 2\n",
    "Actual Class 0  {cm[0][0]:6d}    {cm[0][1]:6d}    {cm[0][2]:6d}\n",
    "       Class 1  {cm[1][0]:6d}    {cm[1][1]:6d}    {cm[1][2]:6d}\n",
    "       Class 2  {cm[2][0]:6d}    {cm[2][1]:6d}    {cm[2][2]:6d}\n",
    "\n",
    "{'=' * 80}\n",
    "MINORITY CLASS ANALYSIS\n",
    "{'=' * 80}\n",
    "Class 1 (Buy Reversal):\n",
    "  Total Instances: {support[1]}\n",
    "  Correctly Predicted: {cm[1][1]}\n",
    "  Missed (False Negatives): {cm[1][0] + cm[1][2]}\n",
    "  False Positives: {cm[0][1] + cm[2][1]}\n",
    "\n",
    "Class 2 (Sell Reversal):\n",
    "  Total Instances: {support[2]}\n",
    "  Correctly Predicted: {cm[2][2]}\n",
    "  Missed (False Negatives): {cm[2][0] + cm[2][1]}\n",
    "  False Positives: {cm[0][2] + cm[1][2]}\n",
    "\n",
    "{'=' * 80}\n",
    "FILES SAVED\n",
    "{'=' * 80}\n",
    "- Model: {model_filename}\n",
    "- Scaler: scaler_{timestamp}.pkl\n",
    "- Training History: training_history.csv\n",
    "- Training Loss Plot: training_loss.png\n",
    "- Training Accuracy Plot: training_accuracy.png\n",
    "- Confusion Matrix: confusion_matrix.png\n",
    "- Per-Class Metrics: per_class_metrics.png\n",
    "- This Report: comprehensive_report.txt\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\"\n",
    "\n",
    "# 11-13 Save comprehensive report\n",
    "report_path = os.path.join(log_dir, 'comprehensive_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(comprehensive_report)\n",
    "\n",
    "# Also save model summary separately\n",
    "summary_path = os.path.join(log_dir, 'model_architecture.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Print the comprehensive report to console\n",
    "print(comprehensive_report)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"[SAVE COMPLETE]\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"All files saved in: {log_dir}\")\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - Comprehensive Report: {report_path}\")\n",
    "print(f\"  - Model Architecture: {summary_path}\")\n",
    "print(f\"  - Confusion Matrix: {os.path.join(log_dir, 'confusion_matrix.png')}\")\n",
    "print(f\"  - Per-Class Metrics: {os.path.join(log_dir, 'per_class_metrics.png')}\")"
   ],
   "id": "7673e991882f6874",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load model\n",
    "model_path = 'saved_models/model_20251124_144549.keras'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# 2- Load scaler\n",
    "scaler_path = 'saved_models/scaler_20251124_144549.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# 3- Load history JSON\n",
    "log_dir = 'saved_models/model_20251124_144549_logs'\n",
    "history_json_path = os.path.join(log_dir, 'history.json')\n",
    "\n",
    "with open(history_json_path, 'r') as f:\n",
    "    history_dict = json.load(f)\n",
    "\n",
    "\n",
    "# create history-like object\n",
    "class ReloadedHistory:\n",
    "    def __init__(self, hdict):\n",
    "        self.history = hdict\n",
    "\n",
    "\n",
    "history = ReloadedHistory(history_dict)\n",
    "\n",
    "# Now you can access history just like before\n",
    "print(history.history.keys())\n",
    "print(history.history['loss'][:5])\n"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
