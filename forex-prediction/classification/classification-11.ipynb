{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# classification-11\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- https://chatgpt.com/c/690db8bc-b820-8330-ba21-c6c793d573f9\n",
    "\n",
    "## next step:\n",
    "\n",
    "1- Improve labeling ( 3333 from 28-1111 )\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Reshape, TimeDistributed, Lambda, RepeatVector, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Input, layers, models, callbacks, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "id": "e76513e71e49aa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# ensure strings and strip any weird whitespace\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df['TIME'] = df['TIME'].astype(str).str.strip()\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], dayfirst=False, errors='coerce')\n",
    "if df['DATETIME'].isna().any():\n",
    "    raise ValueError(\"Some DATETIME values could not be parsed. Check date/time formats.\")\n",
    "\n",
    "# set DATETIME as index for reindexing\n",
    "df = df.set_index('DATETIME').sort_index()\n",
    "\n",
    "# --------------------------\n",
    "# Create continuous hourly index & fill weekend gaps\n",
    "# --------------------------\n",
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Reindex to full hourly range so weekends/missing hours appear as NaN rows\n",
    "df = df.reindex(full_index)\n",
    "\n",
    "# Fill strategy:\n",
    "# - Prices: forward-fill last known price across weekend gap (common approach for modeling continuity).\n",
    "# - TICKVOL / VOL: set missing to 0 (no ticks during weekend).\n",
    "# - SPREAD: forward-fill last known.\n",
    "# Alternative: you could leave NaNs and drop sequences that cross weekends (safer but reduces data).\n",
    "df[['OPEN', 'HIGH', 'LOW', 'CLOSE']] = df[['OPEN', 'HIGH', 'LOW', 'CLOSE']].ffill()\n",
    "df['SPREAD'] = df['SPREAD'].ffill()\n",
    "df['TICKVOL'] = df['TICKVOL'].fillna(0)\n",
    "df['VOL'] = df['VOL'].fillna(0)\n",
    "\n",
    "# Reset index to make DATETIME a regular column again\n",
    "df = df.reset_index().rename(columns={'index': 'DATETIME'})"
   ],
   "id": "7754644750a0b8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "e7923b89f6b69488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example: choose the start and end rows\n",
    "start_row = 32200\n",
    "end_row = 33000\n",
    "\n",
    "# Select the range and make a copy to avoid SettingWithCopyWarning\n",
    "subset = df.iloc[start_row:end_row + 1].copy()\n",
    "\n",
    "# Ensure DATETIME is datetime type\n",
    "subset['DATETIME'] = pd.to_datetime(subset['DATETIME'])\n",
    "\n",
    "# Plot CLOSE price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(subset['DATETIME'], subset['CLOSE'], linewidth=1.0, color='blue')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(f\"Price Chart from Row {start_row} to {end_row}\", fontsize=14)\n",
    "plt.xlabel(\"Datetime\", fontsize=12)\n",
    "plt.ylabel(\"Close Price\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9fe95d0a7b4893b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 33000  # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500  # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df) - mm].reset_index(drop=True)"
   ],
   "id": "477c8b58b48e9dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def label_reversal_points(prices, window=6, threshold=0.0007):\n",
    "    \"\"\"\n",
    "    Labels trend reversals (1=Buy, 2=Sell) based on local mean shifts.\n",
    "    Smaller window & threshold increase sensitivity.\n",
    "    \"\"\"\n",
    "    prices = np.asarray(prices)\n",
    "    labels = [0] * len(prices)\n",
    "    prev_trend = 0  # 1 = up, -1 = down, 0 = unknown\n",
    "\n",
    "    for i in range(len(prices) - window):\n",
    "        past = prices[i:i + window // 2]\n",
    "        future = prices[i + window // 2:i + window]\n",
    "\n",
    "        past_mean = np.mean(past)\n",
    "        future_mean = np.mean(future)\n",
    "        change = (future_mean - past_mean) / past_mean\n",
    "\n",
    "        if change > threshold:\n",
    "            curr_trend = 1  # Uptrend\n",
    "        elif change < -threshold:\n",
    "            curr_trend = -1  # Downtrend\n",
    "        else:\n",
    "            curr_trend = 0  # No significant trend\n",
    "\n",
    "        # Detect reversal only when trend flips clearly\n",
    "        if prev_trend == -1 and curr_trend == 1:\n",
    "            labels[i + window // 2] = 1  # Buy\n",
    "        elif prev_trend == 1 and curr_trend == -1:\n",
    "            labels[i + window // 2] = 2  # Sell\n",
    "\n",
    "        if curr_trend != 0:\n",
    "            prev_trend = curr_trend\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "df_model['Label'] = label_reversal_points(df_model['CLOSE'].values)"
   ],
   "id": "ae894c87b56a1558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_model['Label'].value_counts().sort_index())  # 0, 1, 2",
   "id": "918427444f51dceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_labeled_candles(df, n=190):\n",
    "    \"\"\"\n",
    "    Plots the last n candles with BUY/SELL labels based on the 'Label' column.\n",
    "    Assumes df already has a 'DATETIME' column.\n",
    "    \"\"\"\n",
    "    # Drop NaN rows (e.g., weekend gaps)\n",
    "    df_plot = df.dropna(subset=['CLOSE']).tail(n).copy()\n",
    "\n",
    "    # Ensure DATETIME is a datetime column (optional safeguard)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_plot['DATETIME']):\n",
    "        df_plot['DATETIME'] = pd.to_datetime(df_plot['DATETIME'])\n",
    "\n",
    "    # === Plot Close Price ===\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(df_plot['DATETIME'], df_plot['CLOSE'], label='Close Price', color='black', linewidth=1.5)\n",
    "\n",
    "    # === Plot BUY (1) and SELL (2) signals ===\n",
    "    for _, row in df_plot.iterrows():\n",
    "        if row['Label'] == 1:  # BUY\n",
    "            plt.axvline(x=row['DATETIME'], color='green', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'BUY', color='green', ha='center', va='bottom', fontsize=9)\n",
    "        elif row['Label'] == 2:  # SELL\n",
    "            plt.axvline(x=row['DATETIME'], color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(row['DATETIME'], row['CLOSE'], 'SELL', color='red', ha='center', va='top', fontsize=9)\n",
    "\n",
    "    # === Aesthetics ===\n",
    "    plt.title(f'Last {n} Candles with Trend Reversal Labels')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "8cc50615fd7d5aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_labeled_candles(df_model)",
   "id": "9d9e40e665da9f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Forex time-series classification: train + predict ===\n",
    "# Assumptions:\n",
    "# - df_model: pandas.DataFrame with continuous hourly rows (~130k) and columns:\n",
    "#   ['DATETIME', 'DATE','TIME','OPEN','HIGH','LOW','CLOSE','TICKVOL','VOL','SPREAD','Label']\n",
    "# - df: another DataFrame with same structure (used for prediction input)\n",
    "# - Both have a DATETIME column parseable by pd.to_datetime\n",
    "# - FEATURES to use: ['OPEN','HIGH','LOW','CLOSE','TICKVOL']\n",
    "#\n",
    "# Final output (last expression) is `predicted_df` with 10 rows:\n",
    "#   DATETIME, forecast_class, prob_0, prob_1, prob_2\n",
    "\n",
    "# --------------------------\n",
    "# User hyperparameters\n",
    "# --------------------------\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "FEATURES = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "N_CLASSES = 3  # 0=no signal,1=buy,2=sell\n",
    "\n",
    "# Training hyperparams (tweak as needed)\n",
    "VALIDATION_SPLIT = 0.1\n",
    "MODEL_SAVE_PATH = \"best_model_seq2seq.keras\"\n",
    "\n",
    "# --------------------------\n",
    "# 0) Basic checks & prepare datetimes\n",
    "# --------------------------\n",
    "# Ensure DATETIME column is datetime dtype\n",
    "for dfi in (df_model, df):\n",
    "    if not np.issubdtype(dfi['DATETIME'].dtype, np.datetime64):\n",
    "        dfi['DATETIME'] = pd.to_datetime(dfi['DATETIME'], infer_datetime_format=True, dayfirst=False)\n",
    "\n",
    "# Ensure feature columns exist\n",
    "missing = [c for c in FEATURES if c not in df_model.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing feature columns in df_model: {missing}\")\n",
    "\n",
    "# --------------------------\n",
    "# 1) Create sliding windows X and targets y from df_model\n",
    "#    For each time t (index i) where i >= WINDOW_SIZE-1 and i+FORECAST_HORIZON < len(df_model),\n",
    "#    X sample uses rows [i-WINDOW_SIZE+1 ... i] (WINDOW_SIZE rows)\n",
    "#    Targets are Label at i+1 ... i+FORECAST_HORIZON (FORECAST_HORIZON labels)\n",
    "# --------------------------\n",
    "labels = df_model['Label'].astype(int).values\n",
    "data_values = df_model[FEATURES].values  # shape (T, n_features)\n",
    "T = len(df_model)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# We can form samples for i in [WINDOW_SIZE-1, T-FORECAST_HORIZON-1]\n",
    "start_idx = WINDOW_SIZE - 1\n",
    "end_idx = T - FORECAST_HORIZON - 1\n",
    "\n",
    "for i in range(start_idx, end_idx + 1):\n",
    "    x_window = data_values[i - WINDOW_SIZE + 1 : i + 1]  # shape (WINDOW_SIZE, n_features)\n",
    "    y_horizon = labels[i + 1 : i + 1 + FORECAST_HORIZON]  # shape (FORECAST_HORIZON,)\n",
    "    # sanity: ensure length\n",
    "    if x_window.shape[0] != WINDOW_SIZE or y_horizon.shape[0] != FORECAST_HORIZON:\n",
    "        continue\n",
    "    X_list.append(x_window)\n",
    "    y_list.append(y_horizon)\n",
    "\n",
    "X = np.stack(X_list, axis=0)  # shape (N, WINDOW_SIZE, n_features)\n",
    "y = np.stack(y_list, axis=0)  # shape (N, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Prepared dataset shapes:\", X.shape, y.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Train / validation split\n",
    "# --------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SPLIT, shuffle=True, random_state=42)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Scaling: fit scaler on training X (flatten time axis)\n",
    "# --------------------------\n",
    "# We'll fit a StandardScaler on the feature columns across all timesteps in the training set\n",
    "scaler = StandardScaler()\n",
    "# reshape to (N*WINDOW_SIZE, n_features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[2])\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "# transform both train and val\n",
    "def scale_X(X_in):\n",
    "    n_samples, win, n_feat = X_in.shape\n",
    "    flat = X_in.reshape(-1, n_feat)\n",
    "    flat_scaled = scaler.transform(flat)\n",
    "    return flat_scaled.reshape(n_samples, win, n_feat)\n",
    "\n",
    "X_train_scaled = scale_X(X_train)\n",
    "X_val_scaled = scale_X(X_val)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Convert targets to one-hot (for categorical crossentropy)\n",
    "# --------------------------\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=N_CLASSES)  # shape (N, H, C)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=N_CLASSES)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Handle class imbalance: compute class weights from flattened labels across training set\n",
    "#    We'll compute weights per class and then create a sample_weight matrix of shape (N, H)\n",
    "# --------------------------\n",
    "y_train_flat = y_train.ravel()\n",
    "class_counts = np.bincount(y_train_flat, minlength=N_CLASSES)\n",
    "total = y_train_flat.shape[0]\n",
    "# Avoid divide-by-zero\n",
    "class_freq = np.maximum(class_counts, 1)\n",
    "class_weights = total / (class_freq * N_CLASSES)  # inverse-frequency normalized\n",
    "print(\"Class counts (train):\", class_counts)\n",
    "print(\"Computed class_weights:\", class_weights)\n",
    "\n",
    "# sample_weight matrix shape (N_samples, FORECAST_HORIZON)\n",
    "# map each label to its class weight\n",
    "sample_weight_train = np.vectorize(lambda lbl: class_weights[lbl])(y_train)\n",
    "sample_weight_val = np.vectorize(lambda lbl: class_weights[lbl])(y_val)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Build Seq2Seq-style model (encoder -> RepeatVector -> decoder LSTM with return_sequences)\n",
    "#    Output: TimeDistributed Dense softmax with FORECAST_HORIZON time steps\n",
    "# --------------------------\n",
    "n_features = X_train_scaled.shape[2]\n",
    "latent_units = 128\n",
    "drop_rate = 0.2\n",
    "\n",
    "inp = Input(shape=(WINDOW_SIZE, n_features), name=\"encoder_input\")\n",
    "x = LSTM(latent_units, return_sequences=False, name=\"encoder_lstm\")(inp)\n",
    "x = Dropout(drop_rate)(x)\n",
    "x = BatchNormalization()(x)\n",
    "# Repeat\n",
    "x = RepeatVector(FORECAST_HORIZON)(x)\n",
    "# Decoder\n",
    "x = LSTM(128, return_sequences=True, name=\"decoder_lstm\")(x)\n",
    "x = Dropout(drop_rate)(x)\n",
    "x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "out = TimeDistributed(Dense(N_CLASSES, activation='softmax'), name='out')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ],
   "id": "be4fd7571253a7cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# 7) Training callbacks\n",
    "# --------------------------\n",
    "es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)\n",
    "mc = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Note: Keras accepts sample_weight shaped (samples, timesteps) for temporal weighting\n",
    "# We'll provide sample_weight_train and sample_weight_val which are (N, FORECAST_HORIZON)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat, sample_weight_val),\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    callbacks=[es, mc],\n",
    "    sample_weight=sample_weight_train,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final scaler + model if desired\n",
    "model.save(\"final_model.keras\")  # recommended native Keras format"
   ],
   "id": "d26e72415d5a814e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# PREDICTION SECTION\n",
    "# --------------------------\n",
    "# Specification:\n",
    "# - Use df (not df_model) to select 60 unseen candles ending at given_time\n",
    "# - given_time = \"2025.08.13 21:00:00\" (user-provided format)\n",
    "# - prepare the input, scale it, run model.predict, get probs and classes, build predicted_df\n",
    "# --------------------------\n",
    "\n",
    "given_time = \"2025.08.13 21:00:00\"\n",
    "given_time_dt = pd.to_datetime(given_time, infer_datetime_format=True)\n",
    "\n",
    "# find the index in df where DATETIME == given_time_dt\n",
    "if not np.issubdtype(df['DATETIME'].dtype, np.datetime64):\n",
    "    df['DATETIME'] = pd.to_datetime(df['DATETIME'], infer_datetime_format=True)\n",
    "\n",
    "matches = df.index[df['DATETIME'] == given_time_dt].tolist()\n",
    "if len(matches) == 0:\n",
    "    raise ValueError(f\"given_time {given_time_dt} not found exactly in df['DATETIME']. If index is different, adjust or ensure times match.\")\n",
    "idx = matches[0]\n",
    "\n",
    "# Ensure we have at least WINDOW_SIZE rows ending at idx (inclusive)\n",
    "if idx - (WINDOW_SIZE - 1) < 0:\n",
    "    raise ValueError(f\"Not enough history before given_time index {idx} to build a window of size {WINDOW_SIZE}.\")\n",
    "\n",
    "input_window_df = df.iloc[idx - (WINDOW_SIZE - 1) : idx + 1].copy()  # inclusive end\n",
    "# sanity check\n",
    "assert len(input_window_df) == WINDOW_SIZE\n",
    "\n",
    "# select features and scale\n",
    "X_input = input_window_df[FEATURES].values.reshape(1, WINDOW_SIZE, n_features)  # shape (1,60,n_feat)\n",
    "# check feature count\n",
    "if X_input.shape[2] != n_features:\n",
    "    raise ValueError(f\"Feature dimension mismatch: model expects {n_features} features, but X_input has {X_input.shape[2]}\")\n",
    "\n",
    "# scale with previously fitted scaler\n",
    "X_input_flat = X_input.reshape(-1, n_features)\n",
    "X_input_scaled_flat = scaler.transform(X_input_flat)\n",
    "X_input_scaled = X_input_scaled_flat.reshape(1, WINDOW_SIZE, n_features)\n",
    "\n",
    "# Run prediction\n",
    "y_pred_prob = model.predict(X_input_scaled)  # shape (1, FORECAST_HORIZON, N_CLASSES)\n",
    "y_pred_prob = np.squeeze(y_pred_prob, axis=0)  # (FORECAST_HORIZON, N_CLASSES)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)  # (FORECAST_HORIZON,)\n",
    "\n",
    "# Build forecast DATETIME list: next 10 hours immediately after given_time\n",
    "forecast_datetimes = [given_time_dt + pd.Timedelta(hours=i+1) for i in range(FORECAST_HORIZON)]\n",
    "\n",
    "predicted_df = pd.DataFrame({\n",
    "    'DATETIME': forecast_datetimes,\n",
    "    'forecast_class': y_pred_class,\n",
    "    'prob_0': y_pred_prob[:, 0],\n",
    "    'prob_1': y_pred_prob[:, 1],\n",
    "    'prob_2': y_pred_prob[:, 2],\n",
    "})\n",
    "\n",
    "# Final display/return: predicted_df is the last expression (as required)\n",
    "predicted_df"
   ],
   "id": "4e87c5307b245bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67f1c7b291c63436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1afae1a06dc9be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "e31209f9cef74d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------------------\n",
    "# === Visualization Block ===\n",
    "# --------------------------\n",
    "\n",
    "# --- 1. Historical window (last 4 real candles before forecast) ---\n",
    "# --- Find the starting index ---\n",
    "start_idx = df.index[df['DATETIME'] == pd.to_datetime(given_time)][0]\n",
    "\n",
    "# --- 1. Extract next n candles ---\n",
    "input_df = df.iloc[start_idx: start_idx + WINDOW_SIZE].copy()\n",
    "\n",
    "historical_df = input_df.tail(4).copy()\n",
    "historical_df"
   ],
   "id": "fc3ffb5a765ae1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 2. Actual future 10 candles  ---\n",
    "# Since input_df ends at index (start_idx - 1), actual_future_df starts right after that.\n",
    "actual_future_start = start_idx\n",
    "actual_future_end = start_idx + FORECAST_HORIZON\n",
    "actual_future_df = df.iloc[actual_future_start - 1:actual_future_end].copy()\n",
    "actual_future_df"
   ],
   "id": "7485efcf60503713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. Create predicted_df (forecast for next 10 hours) ---\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(\n",
    "    start=last_timestamp + pd.Timedelta(hours=1),\n",
    "    periods=FORECAST_HORIZON,\n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Add text labels for clarity ---\n",
    "predicted_df['label'] = predicted_df['forecast_class'].map({1: 'buy', 2: 'sell'}).fillna('')\n",
    "\n",
    "\n",
    "# --- 5. Plot title & output settings ---\n",
    "plot_title = 'Actual vs Predicted Forex Trend Reversals'\n",
    "output_plot_path = None  # e.g., 'forecast_plot.png'\n",
    "\n"
   ],
   "id": "c27312e46eb98ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6. Import your plotting utility ---\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils_2\n",
    "\n",
    "# --- 7. Plot all series ---\n",
    "forex_plot_utils_2.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")\n"
   ],
   "id": "640f86f86378887c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 11- Save Model\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# 11-4 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# 11-5 Save training loss plot\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 4. Evaluate on validation set (since no X_test/y_test defined)\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_train_acc = history.history['cat_acc'][-1]\n",
    "final_val_loss, final_val_acc = model.evaluate(X_val_scaled, y_val_cat, verbose=0)\n",
    "\n",
    "# 5. Save model summary and performance metrics\n",
    "summary_path = os.path.join(log_dir, 'model_log.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write('\\n')\n",
    "    f.write(f'Final Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Training Accuracy: {final_train_acc:.6f}\\n')\n",
    "    f.write(f'Final Validation Loss: {final_val_loss:.6f}\\n')\n",
    "    f.write(f'Final Validation Accuracy: {final_val_acc:.6f}\\n')"
   ],
   "id": "c60adc7617c16b75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = 'saved_models/model_20251106_214146.keras'\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={'loss_fn': focal_loss(), 'focal_loss': focal_loss()},\n",
    "    safe_mode=False\n",
    ")"
   ],
   "id": "71f8ce11032e711c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
