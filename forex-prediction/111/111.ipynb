{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# regression-02\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1- remove unnecessary rows for training model.\n"
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:30:19.747550Z",
     "start_time": "2025-10-13T14:30:15.831503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 18:00:16.517292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-13 18:00:16.637464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:30:22.400433Z",
     "start_time": "2025-10-13T14:30:22.258774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "\n",
    "df = pd.read_csv('XAGUSD-197001010000--H1-rates.csv', sep='\\t')\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])\n",
    "\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Sort data chronologically by DATETIME\n",
    "df.sort_values(by='DATETIME', inplace=True)\n",
    "\n",
    "# Reset index to ensure clean row order\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ],
   "id": "27be8804efc0b24c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:30:26.454360Z",
     "start_time": "2025-10-13T14:30:26.440787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify how many rows to remove for model\n",
    "nn = 1000   # Delete the first nn rows that do not follow the one-hour timeframe.\n",
    "mm = 500   # Remove mm last row that the model should not see.\n",
    "\n",
    "# Delete first nn and last mm rows\n",
    "df_model = df.iloc[nn:len(df)-mm].reset_index(drop=True)"
   ],
   "id": "fdfb1892b0cb90ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:30:29.340390Z",
     "start_time": "2025-10-13T14:30:29.304558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select features to scale\n",
    "features = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_model[features])\n",
    "\n",
    "close_scaler = MinMaxScaler()\n",
    "scaled_close = close_scaler.fit_transform(df_model[['CLOSE']])"
   ],
   "id": "3813efa7b7b96154",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:30:30.560309Z",
     "start_time": "2025-10-13T14:30:30.555279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Parameters ---\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10"
   ],
   "id": "f3103d27aa971ac",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:31:42.198171Z",
     "start_time": "2025-10-13T14:30:57.736533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare sequences\n",
    "def create_sequences(features, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features) - horizon):\n",
    "        X.append(features[i - window:i])\n",
    "        y.append(target[i:i + horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled, scaled_close, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Build basic LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(FORECAST_HORIZON)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train\n",
    "mc = callbacks.ModelCheckpoint(filepath='mcp_saved_model.keras', monitor='val_loss', save_best_only=True)\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    callbacks=[mc, es],\n",
    "    validation_split=0.2,\n",
    ")"
   ],
   "id": "cb7841301ce30d5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001B[1m1969/1969\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 11ms/step - loss: 7.9693e-04 - mae: 0.0081 - val_loss: 5.0394e-05 - val_mae: 0.0046\n",
      "Epoch 2/2\n",
      "\u001B[1m1969/1969\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 11ms/step - loss: 5.1252e-05 - mae: 0.0044 - val_loss: 4.4375e-05 - val_mae: 0.0044\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
