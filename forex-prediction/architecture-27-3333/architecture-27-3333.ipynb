{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-27-3333\n",
    "\n",
    "## What's new:\n",
    "\n",
    "1-\n"
   ],
   "id": "fb927f0482914677"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:53:39.256407Z",
     "start_time": "2025-08-15T13:53:39.251464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Reshape, TimeDistributed, Lambda, LayerNormalization, \\\n",
    "    Bidirectional\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import talib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n"
   ],
   "id": "4abc13163ee2d424",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:53:40.953613Z",
     "start_time": "2025-08-15T13:53:40.881529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1- Load and Scaling Features\n",
    "# Load and preprocess\n",
    "df = pd.read_csv('datasets-27-3333/XAGUSD-H1-rates.csv', sep='\\t')\n",
    "\n",
    "# Rename columns for easier access\n",
    "df.rename(columns={\n",
    "    '<DATE>': 'DATE',\n",
    "    '<TIME>': 'TIME',\n",
    "    '<OPEN>': 'OPEN',\n",
    "    '<HIGH>': 'HIGH',\n",
    "    '<LOW>': 'LOW',\n",
    "    '<CLOSE>': 'CLOSE',\n",
    "    '<TICKVOL>': 'TICKVOL',\n",
    "    '<VOL>': 'VOL',\n",
    "    '<SPREAD>': 'SPREAD'\n",
    "}, inplace=True)\n",
    "\n",
    "# Optional: Combine DATE and TIME into a single datetime column\n",
    "df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Sort data chronologically by DATETIME\n",
    "df.sort_values(by='DATETIME', inplace=True)\n",
    "\n",
    "# Reset index to ensure clean row order\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select features to scale\n",
    "feature_cols = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL']\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])"
   ],
   "id": "4c9a5a36c8be002e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:53:46.075836Z",
     "start_time": "2025-08-15T13:53:45.569383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2- Label trend reversals\n",
    "def label_reversal_points(prices, window=8, threshold=0.002):\n",
    "    prices = np.asarray(prices)\n",
    "    labels = [0] * len(prices)\n",
    "    prev_trend = 0  # 1 = up, -1 = down, 0 = unknown\n",
    "\n",
    "    for i in range(len(prices) - window):\n",
    "        past = prices[i:i + window // 2]\n",
    "        future = prices[i + window // 2:i + window]\n",
    "\n",
    "        past_mean = np.mean(past)\n",
    "        future_mean = np.mean(future)\n",
    "        change = (future_mean - past_mean) / past_mean\n",
    "\n",
    "        if change > threshold:\n",
    "            curr_trend = 1  # Uptrend\n",
    "        elif change < -threshold:\n",
    "            curr_trend = -1  # Downtrend\n",
    "        else:\n",
    "            curr_trend = 0  # No significant trend\n",
    "\n",
    "        # Detect a reversal (trend direction changed)\n",
    "        if prev_trend == -1 and curr_trend == 1:\n",
    "            labels[i + window // 2] = 1  # Buy signal at start of uptrend\n",
    "        elif prev_trend == 1 and curr_trend == -1:\n",
    "            labels[i + window // 2] = 2  # Sell signal at start of downtrend\n",
    "\n",
    "        # Update previous trend only if there is a new clear trend\n",
    "        if curr_trend != 0:\n",
    "            prev_trend = curr_trend\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "df['Label'] = label_reversal_points(df['CLOSE'].values)"
   ],
   "id": "ce5f9c1ecf96cced",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:54:31.133557Z",
     "start_time": "2025-08-15T13:53:49.118767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3- Create sequences\n",
    "SEQ_LEN = 60  # past candles for input\n",
    "FORECAST_HORIZON = 10  # predict next 10 candles\n",
    "NUM_CLASSES = 3  # 0 = no signal, 1 = buy, 2 = sell\n",
    "X, y = [], []\n",
    "for i in range(len(df) - SEQ_LEN - FORECAST_HORIZON + 1):\n",
    "    seq_x = df[feature_cols].iloc[i: i + SEQ_LEN].values\n",
    "    seq_y = df['Label'].iloc[i + SEQ_LEN: i + SEQ_LEN + FORECAST_HORIZON].values\n",
    "    X.append(seq_x)\n",
    "    y.append(seq_y)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ],
   "id": "7aaf04c111873e14",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:54:36.502169Z",
     "start_time": "2025-08-15T13:54:35.611200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4- One-hot encode labels for each timestep\n",
    "y_onehot = np.array([to_categorical(seq, num_classes=NUM_CLASSES) for seq in y])"
   ],
   "id": "f5c675eddb4bc802",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:54:41.586042Z",
     "start_time": "2025-08-15T13:54:41.579609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5- Train-test split\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y_onehot[:split], y_onehot[split:]\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)"
   ],
   "id": "9b787dab0031e409",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (44011, 60, 5)\n",
      "y_train: (44011, 10, 3)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:54:44.844004Z",
     "start_time": "2025-08-15T13:54:44.797951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6- Handle Class Imbalance\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Flatten labels to compute weights\n",
    "y_flat = np.argmax(y_train, axis=-1).flatten()\n",
    "class_weights_values = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(NUM_CLASSES),\n",
    "    y=y_flat\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_values))\n",
    "print(\"Class weights:\", class_weights)"
   ],
   "id": "ba0012e61d11b0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.3769036369817907), 1: np.float64(5.766640461215933), 2: np.float64(5.767320569773689)}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:54:51.398431Z",
     "start_time": "2025-08-15T13:54:50.149213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7- Build LSTM Classification Model\n",
    "# ==============================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
    "\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=False, input_shape=(SEQ_LEN, len(feature_cols))),\n",
    "    Dropout(0.3),\n",
    "    RepeatVector(FORECAST_HORIZON),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    TimeDistributed(Dense(NUM_CLASSES, activation='softmax'))\n",
    "])\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Convert your class_weights dict to a tensor\n",
    "weights = np.array([class_weights[i] for i in range(NUM_CLASSES)], dtype=np.float32)\n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    weights_tensor = K.constant(weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Avoid log(0)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # Calculate loss for each class and apply weights\n",
    "        loss = y_true * K.log(y_pred) * weights_tensor\n",
    "        loss = -K.sum(loss, axis=-1)  # sum over classes\n",
    "        return loss\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=weighted_categorical_crossentropy(weights),\n",
    "    metrics=['accuracy', 'mae']\n",
    ")\n"
   ],
   "id": "6474407f3f5a9c11",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755266090.461708    4011 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2317 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/saeed/repositories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T13:56:55.459593Z",
     "start_time": "2025-08-15T13:54:56.905151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8- Fit model with EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "rc = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, rc],\n",
    "    verbose=1\n",
    ")\n",
    "# shuffle=False,  # Important: keep time order! ?????"
   ],
   "id": "f189dd95c6660684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755266099.191149    4081 cuda_dnn.cc:529] Loaded cuDNN version 91001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 9ms/step - accuracy: 0.2898 - loss: 1.1002 - mae: 0.4440 - val_accuracy: 0.5441 - val_loss: 1.0583 - val_mae: 0.4418 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 8ms/step - accuracy: 0.3336 - loss: 1.0959 - mae: 0.4438 - val_accuracy: 0.6717 - val_loss: 1.0581 - val_mae: 0.4415 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.3759 - loss: 1.0962 - mae: 0.4439 - val_accuracy: 0.8912 - val_loss: 1.0579 - val_mae: 0.4347 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 8ms/step - accuracy: 0.3438 - loss: 1.0981 - mae: 0.4441 - val_accuracy: 0.8905 - val_loss: 1.0579 - val_mae: 0.4356 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 8ms/step - accuracy: 0.3894 - loss: 1.1015 - mae: 0.4443 - val_accuracy: 0.8912 - val_loss: 1.0581 - val_mae: 0.4321 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.4194 - loss: 1.0985 - mae: 0.4437\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.4194 - loss: 1.0985 - mae: 0.4438 - val_accuracy: 0.8912 - val_loss: 1.0580 - val_mae: 0.4366 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.4201 - loss: 1.1022 - mae: 0.4446 - val_accuracy: 0.8912 - val_loss: 1.0579 - val_mae: 0.4366 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.4461 - loss: 1.0974 - mae: 0.4441 - val_accuracy: 0.8894 - val_loss: 1.0580 - val_mae: 0.4404 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.4291 - loss: 1.0962 - mae: 0.4442\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.4291 - loss: 1.0962 - mae: 0.4442 - val_accuracy: 0.8912 - val_loss: 1.0578 - val_mae: 0.4369 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.4618 - loss: 1.0978 - mae: 0.4445 - val_accuracy: 0.8912 - val_loss: 1.0578 - val_mae: 0.4375 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.4932 - loss: 1.0959 - mae: 0.4434 - val_accuracy: 0.8899 - val_loss: 1.0578 - val_mae: 0.4381 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.4638 - loss: 1.0946 - mae: 0.4438 - val_accuracy: 0.8817 - val_loss: 1.0578 - val_mae: 0.4383 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.4187 - loss: 1.0958 - mae: 0.4442 - val_accuracy: 0.8870 - val_loss: 1.0577 - val_mae: 0.4371 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m684/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.4040 - loss: 1.0949 - mae: 0.4441\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.4038 - loss: 1.0949 - mae: 0.4441 - val_accuracy: 0.6356 - val_loss: 1.0582 - val_mae: 0.4381 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.3532 - loss: 1.0964 - mae: 0.4444 - val_accuracy: 0.8754 - val_loss: 1.0578 - val_mae: 0.4368 - learning_rate: 1.2500e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.3705 - loss: 1.0936 - mae: 0.4437 - val_accuracy: 0.8593 - val_loss: 1.0579 - val_mae: 0.4377 - learning_rate: 1.2500e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.3889 - loss: 1.0938 - mae: 0.4434\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.3889 - loss: 1.0938 - mae: 0.4434 - val_accuracy: 0.8199 - val_loss: 1.0581 - val_mae: 0.4376 - learning_rate: 1.2500e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m688/688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 10ms/step - accuracy: 0.3582 - loss: 1.0980 - mae: 0.4441 - val_accuracy: 0.8605 - val_loss: 1.0579 - val_mae: 0.4375 - learning_rate: 6.2500e-05\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T14:00:59.686415Z",
     "start_time": "2025-08-15T14:00:58.193556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 11- Save Model\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 11-1 Create timestamp and paths\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# 11-2 Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 11-3 Save model\n",
    "model.save(model_path)\n",
    "\n",
    "# 11-4 Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# 11-5 Save training loss plot\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# 11-6 Save model summary and final performance\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    # Model architecture\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    # Final metrics\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "\n",
    "    # Use y_test directly (already one-hot encoded from Step 4)\n",
    "    final_test_loss, final_test_accuracy, final_test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Accuracy: {final_test_accuracy:.6f}\\n')\n",
    "    f.write(f'Final Test MAE: {final_test_mae:.6f}\\n')\n",
    "\n",
    "print(f\"✅ Model and logs saved in: {log_dir}\")"
   ],
   "id": "c60adc7617c16b75",
   "outputs": [
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and logs saved in: saved_models/model_20250815_173058_logs\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
