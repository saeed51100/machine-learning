{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# architecture-08 ( Add Multi-Head Attention ) - v01\n",
    "## Train is ok and predict with error\n",
    "\n",
    "What's new:\n",
    "\n",
    "1- Remove correlated indicators."
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:10:14.156192Z",
     "start_time": "2025-06-12T09:10:14.147527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import talib\n",
    "from tensorflow.keras import layers, models, callbacks"
   ],
   "id": "c980d7d41b42c555",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:10:16.539253Z",
     "start_time": "2025-06-12T09:10:16.456110Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('datasets-08/XAGUSD-H1-rates.csv', sep='\\t').dropna()",
   "id": "27be8804efc0b24c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:10:18.418902Z",
     "start_time": "2025-06-12T09:10:18.371256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def add_ta_features(df):\n",
    "    df['rsi'] = talib.RSI(df['<CLOSE>'], timeperiod=14)\n",
    "    macd, macd_signal, macd_hist = talib.MACD(df['<CLOSE>'])\n",
    "    df['macd'] = macd\n",
    "    df['MACD_signal'] = macd_signal\n",
    "    df['MACD_hist'] = macd_hist\n",
    "    df['ema_10'] = talib.EMA(df['<CLOSE>'], timeperiod=10)\n",
    "    df['ema_50'] = talib.EMA(df['<CLOSE>'], timeperiod=50)\n",
    "    df['atr'] = talib.ATR(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    df['adx'] = talib.ADX(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "df = add_ta_features(df)"
   ],
   "id": "ac2c2ed535f2da44",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:10:21.422953Z",
     "start_time": "2025-06-12T09:10:21.400084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scaling Features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(\n",
    "    df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>', 'rsi', 'macd', 'ema_50', 'atr', 'adx']])"
   ],
   "id": "3813efa7b7b96154",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:13:57.091551Z",
     "start_time": "2025-06-12T09:13:57.085651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare sequences\n",
    "def create_sequences(features, target, window, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features) - horizon):\n",
    "        X.append(features[i - window:i])\n",
    "        y.append(target[i:i + horizon])\n",
    "    return np.array(X), np.array(y)"
   ],
   "id": "ba73f79603cf9f44",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:13:58.871049Z",
     "start_time": "2025-06-12T09:13:58.866185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 10\n",
    "NUM_HEADS = 4  # You can try 2, 4, 8 depending on GPU and dataset\n",
    "KEY_DIM = 16  # Size of each attention head"
   ],
   "id": "78dcd2e9ae0c85d1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:14:01.571065Z",
     "start_time": "2025-06-12T09:14:01.091628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "close_scaler = MinMaxScaler()\n",
    "scaled_close = close_scaler.fit_transform(df[['<CLOSE>']])\n",
    "X, y = create_sequences(scaled, scaled_close, WINDOW_SIZE, FORECAST_HORIZON)"
   ],
   "id": "27484974f06818b5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:14:03.834564Z",
     "start_time": "2025-06-12T09:14:03.693408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ],
   "id": "e5d74f22728f847b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:35:28.158783Z",
     "start_time": "2025-06-12T09:18:58.402299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_attention_model(input_shape, forecast_horizon, num_heads=4, key_dim=16):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Ensure projection matches MHA output dimension\n",
    "    projection_dim = num_heads * key_dim\n",
    "    x = layers.Dense(projection_dim, activation='relu')(inputs)  # Now shape = (None, 60, 64)\n",
    "\n",
    "    # Multi-head self-attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = layers.LayerNormalization(epsilon=1e-6)(attn_output + x)  # Shapes now match\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = layers.Dense(64, activation='relu')(attn_output)\n",
    "    ffn = layers.Dense(projection_dim, activation='relu')(ffn)\n",
    "    ffn_output = layers.LayerNormalization(epsilon=1e-6)(ffn + attn_output)\n",
    "\n",
    "    # Global average pooling and output\n",
    "    pooled = layers.GlobalAveragePooling1D()(ffn_output)\n",
    "    outputs = layers.Dense(forecast_horizon)(pooled)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_attention_model(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    forecast_horizon=FORECAST_HORIZON\n",
    ")\n",
    "\n",
    "# Train\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[es],\n",
    "    validation_split=0.2,\n",
    ")"
   ],
   "id": "cb7841301ce30d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 12:48:58.781964: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 84422400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1099/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 33ms/step - loss: 0.0370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 12:49:40.226698: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 21108000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 38ms/step - loss: 0.0369 - val_loss: 0.0019\n",
      "Epoch 2/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 37ms/step - loss: 4.1641e-04 - val_loss: 9.4167e-04\n",
      "Epoch 3/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 37ms/step - loss: 3.0143e-04 - val_loss: 8.7968e-04\n",
      "Epoch 4/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 38ms/step - loss: 2.8175e-04 - val_loss: 7.1624e-04\n",
      "Epoch 5/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 42ms/step - loss: 2.8983e-04 - val_loss: 7.0635e-04\n",
      "Epoch 6/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 45ms/step - loss: 2.5149e-04 - val_loss: 0.0013\n",
      "Epoch 7/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 46ms/step - loss: 2.4237e-04 - val_loss: 5.7689e-04\n",
      "Epoch 8/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 45ms/step - loss: 2.3083e-04 - val_loss: 0.0010\n",
      "Epoch 9/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 2.1528e-04 - val_loss: 0.0015\n",
      "Epoch 10/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 2.0153e-04 - val_loss: 5.1806e-04\n",
      "Epoch 11/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 46ms/step - loss: 1.9181e-04 - val_loss: 5.6539e-04\n",
      "Epoch 12/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 2.0379e-04 - val_loss: 7.7363e-04\n",
      "Epoch 13/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 1.9531e-04 - val_loss: 4.5860e-04\n",
      "Epoch 14/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 47ms/step - loss: 1.7741e-04 - val_loss: 0.0012\n",
      "Epoch 15/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 1.7879e-04 - val_loss: 4.3811e-04\n",
      "Epoch 16/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 1.6083e-04 - val_loss: 7.5805e-04\n",
      "Epoch 17/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 47ms/step - loss: 1.5158e-04 - val_loss: 8.8370e-04\n",
      "Epoch 18/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 46ms/step - loss: 1.5522e-04 - val_loss: 6.3673e-04\n",
      "Epoch 19/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m54s\u001B[0m 49ms/step - loss: 1.5599e-04 - val_loss: 0.0010\n",
      "Epoch 20/50\n",
      "\u001B[1m1100/1100\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 48ms/step - loss: 1.3633e-04 - val_loss: 8.5552e-04\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:53:46.233257Z",
     "start_time": "2025-06-12T09:53:46.004042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load the last 60 rows from a separate CSV file for prediction ===\n",
    "input_df = pd.read_csv('datasets-08/new-data-for-test/rows-60-from-20240503/rows-60-from-20240503.csv',\n",
    "                       sep='\\t').dropna()\n",
    "input_df = add_ta_features(input_df)\n",
    "input_scaled = scaler.transform(\n",
    "    input_df[['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>', 'rsi', 'macd', 'ema_50', 'atr', 'adx']])\n",
    "input_sequence = np.expand_dims(input_scaled, axis=0)  # shape: (1, 60, 5)\n",
    "\n",
    "# === Predict the next 10 candles + Inverse scale ===\n",
    "pred = model.predict(input_sequence)\n",
    "prediction = close_scaler.inverse_transform(pred)"
   ],
   "id": "7ad768ead6620c73",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 60, 10), found shape=(1, 11, 10)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m input_sequence = np.expand_dims(input_scaled, axis=\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# shape: (1, 60, 5)\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# === Predict the next 10 candles + Inverse scale ===\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m pred = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_sequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m prediction = close_scaler.inverse_transform(pred)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repozitories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repozitories/machine-learning/forex-prediction/envs/lib/python3.11/site-packages/keras/src/layers/input_spec.py:245\u001B[39m, in \u001B[36massert_input_compatibility\u001B[39m\u001B[34m(input_spec, inputs, layer_name)\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    244\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m spec_dim != dim:\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    246\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m of layer \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m is \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    247\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mincompatible with the layer: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    248\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    249\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    250\u001B[39m         )\n",
      "\u001B[31mValueError\u001B[39m: Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 60, 10), found shape=(1, 11, 10)"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# plot section",
   "id": "6456b78cc1b88128"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, '../utils')\n",
    "import forex_plot_utils\n",
    "import os\n",
    "\n",
    "# PARAMETERS\n",
    "csv1_path = 'datasets-08/new-data-for-test/rows-60-from-20240503/latest-4-for-history.csv'\n",
    "csv3_path = 'datasets-08/new-data-for-test/rows-60-from-20240503/after.csv'\n",
    "plot_title = 'Actual vs Predicted Forex Closing Prices'\n",
    "output_plot_path = None  # e.g., 'output.png'\n",
    "\n",
    "# LOAD DATA FROM CSVS\n",
    "historical_df = forex_plot_utils.load_csv_with_datetime(csv1_path) if os.path.exists(csv1_path) else None\n",
    "actual_future_df = forex_plot_utils.load_csv_with_datetime(csv3_path) if os.path.exists(csv3_path) else None\n",
    "\n",
    "# LOAD DATA FROM PREDICTION\n",
    "\n",
    "# Combine <DATE> and <TIME> columns into a datetime\n",
    "input_df['DATETIME'] = pd.to_datetime(input_df['<DATE>'] + ' ' + input_df['<TIME>'])\n",
    "\n",
    "last_timestamp = input_df['DATETIME'].iloc[-1]\n",
    "datetime_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=len(prediction[0]), freq='h')\n",
    "\n",
    "# Create DataFrame\n",
    "predicted_df = pd.DataFrame({'DATETIME': datetime_index, '<CLOSE>': prediction[0]})"
   ],
   "id": "d4d7751243eff18e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PLOT\n",
    "forex_plot_utils.plot_all_series(\n",
    "    historical_df=historical_df,\n",
    "    predicted_df=predicted_df,\n",
    "    actual_future_df=actual_future_df,\n",
    "    title=plot_title,\n",
    "    output_path=output_plot_path\n",
    ")"
   ],
   "id": "a6e7b86736ad5b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Create timestamp and paths ===\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'model_{timestamp}.keras'\n",
    "model_path = os.path.join('saved_models', model_filename)\n",
    "\n",
    "# Directory to hold logs and extras\n",
    "log_dir = os.path.join('saved_models', f'model_{timestamp}_logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# === Save model ===\n",
    "model.save(model_path)\n",
    "\n",
    "# === Save training history ===\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(log_dir, 'training_history.csv'), index=False)\n",
    "\n",
    "# === Save training loss plot ===\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(log_dir, 'training_loss.png'))\n",
    "plt.close()\n",
    "\n",
    "# === Save model summary and final performance ===\n",
    "with open(os.path.join(log_dir, 'model_log.txt'), 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    f.write(f'\\nFinal Training Loss: {final_train_loss:.6f}\\n')\n",
    "    f.write(f'Final Test Loss: {final_test_loss:.6f}\\n')\n"
   ],
   "id": "2bbbacfc2fb3329a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
