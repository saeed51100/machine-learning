{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ta-lib-006 ( Feature‑Engineering + Sequence Models)\n",
    "https://chatgpt.com/c/680b5d6e-3f64-800a-9f2b-c08e35b0d0e8\n",
    "What's new:\n",
    "\n",
    "1- Change logic from classification to regression."
   ],
   "id": "67a9ecb3258e8606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import talib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots inline\n",
    "%matplotlib inline"
   ],
   "id": "ff3be02c3ae9ecbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "DATA_PATH = \"../datasets/XAGUSD-H1-rates.csv\"   # Path to your downloaded MT5 CSV\n",
    "SEQUENCE_LENGTH = 30                # Number of past candles per sample\n",
    "PREDICT_HORIZON  = 5                # How many candles ahead to detect a trend change\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50"
   ],
   "id": "d71b0513bdaf89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load & Inspect Data\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t')"
   ],
   "id": "d8371facd08687f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine <DATE> and <TIME> into single datetime index\n",
    "df['DATETIME'] = pd.to_datetime(df['<DATE>'] + ' ' + df['<TIME>'])\n",
    "df.set_index('DATETIME', inplace=True)\n",
    "df.drop(columns=['<DATE>', '<TIME>'], inplace=True)\n",
    "df.head()"
   ],
   "id": "d904ddb06d873898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute TA indicators and append to dataframe.\n",
    "# e.g. RSI, ATR, MACD Histogram\n",
    "df['rsi'] = talib.RSI(df['<CLOSE>'], timeperiod=14)\n",
    "macd, macd_sig, macd_hist = talib.MACD(df['<CLOSE>'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "df['macd_hist'] = macd_hist\n",
    "df['atr'] = talib.ATR(df['<HIGH>'], df['<LOW>'], df['<CLOSE>'], timeperiod=14)\n",
    "df.dropna(inplace=True) # Remove missing values."
   ],
   "id": "173e7a431130ca4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Engineering\n",
    "feature_cols = ['<CLOSE>', 'rsi', 'macd_hist', 'atr']\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[feature_cols])"
   ],
   "id": "13f88411fa165acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Target Engineering (Multi-Step Future Prices)\n",
    "target_col = '<CLOSE>'  # Predict future CLOSE prices"
   ],
   "id": "b17e65787bcc264a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sequence Creation\n",
    "X, y = [], []\n",
    "for i in range(len(df) - SEQUENCE_LENGTH - PREDICT_HORIZON + 1):\n",
    "    seq_x = scaled[i : i+SEQUENCE_LENGTH]\n",
    "    future_y = df[target_col].iloc[i+SEQUENCE_LENGTH : i+SEQUENCE_LENGTH+PREDICT_HORIZON].values\n",
    "    X.append(seq_x)\n",
    "    y.append(future_y)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (samples, 30, 4)\n",
    "print(\"y shape:\", y.shape)  # (samples, 5)"
   ],
   "id": "384b988ae91b9a93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test :\", X_test.shape, y_test.shape)"
   ],
   "id": "cb9abf4559ce8eba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build LSTM Regression Model\n",
    "def build_regression_model(input_shape, output_horizon):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(output_horizon)  # Linear output: predicting raw future prices\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss='mse',                # Mean Squared Error for regression\n",
    "        optimizer='adam',\n",
    "        metrics=['mae']             # Mean Absolute Error as additional metric\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_regression_model((SEQUENCE_LENGTH, len(feature_cols)), PREDICT_HORIZON)\n",
    "model.summary()\n"
   ],
   "id": "e396b4d1219f5142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es]\n",
    ")"
   ],
   "id": "61b57fd3c90d4900",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:25:01.286300Z",
     "start_time": "2025-04-26T17:25:00.229084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {loss:.4f}  Test MAE: {mae:.4f}\")\n"
   ],
   "id": "1c59b4e9fe41b0a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m344/344\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - loss: 0.0223 - mae: 0.1021\n",
      "Test Loss (MSE): 0.0232  Test MAE: 0.1035\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T17:25:04.088083Z",
     "start_time": "2025-04-26T17:25:03.712496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- New Code Starts Here ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load recent real candles for prediction\n",
    "recent_candles = pd.read_csv(\"../datasets/new-data-for-test/rows-30-from-20240503.csv\", sep='\\t')\n",
    "\n",
    "# Combine <DATE> and <TIME> into single datetime index\n",
    "recent_candles['DATETIME'] = pd.to_datetime(recent_candles['<DATE>'] + ' ' + recent_candles['<TIME>'])\n",
    "recent_candles.set_index('DATETIME', inplace=True)\n",
    "recent_candles.drop(columns=['<DATE>', '<TIME>'], inplace=True)\n",
    "\n",
    "\n",
    "# Compute TA indicators and append to dataframe.\n",
    "# e.g. RSI, ATR, MACD Histogram\n",
    "recent_candles['rsi'] = talib.RSI(recent_candles['<CLOSE>'], timeperiod=14)\n",
    "macd, macd_sig, macd_hist = talib.MACD(recent_candles['<CLOSE>'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "recent_candles['macd_hist'] = macd_hist\n",
    "recent_candles['atr'] = talib.ATR(recent_candles['<HIGH>'], recent_candles['<LOW>'], recent_candles['<CLOSE>'], timeperiod=14)\n",
    "recent_candles.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Select the latest SEQUENCE_LENGTH candles\n",
    "input_candles = recent_candles.tail(SEQUENCE_LENGTH)\n",
    "\n",
    "# Scale the input using the same scaler from training\n",
    "input_features = input_candles[feature_cols]\n",
    "input_scaled = scaler.transform(input_features)\n",
    "input_scaled = np.expand_dims(input_scaled, axis=0)  # (1, 30, 4)\n",
    "\n",
    "# Predict future prices\n",
    "predicted_future_prices = model.predict(input_scaled)[0]  # shape = (PREDICT_HORIZON,)\n",
    "print(\"Predicted Future Prices:\", predicted_future_prices)\n",
    "\n",
    "# Prepare for plotting\n",
    "# Get the last real CLOSE price\n",
    "last_real_close = input_candles['<CLOSE>'].iloc[-1]\n",
    "\n",
    "# Build X axis (time)\n",
    "real_times = input_candles.index\n",
    "future_times = pd.date_range(start=real_times[-1] + pd.Timedelta(hours=1), periods=PREDICT_HORIZON, freq='H')\n",
    "\n",
    "# Plot real CLOSE prices\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(real_times, input_candles['<CLOSE>'], label='Real Close Prices (Input)', marker='o')\n",
    "\n",
    "# Plot predicted future prices\n",
    "plt.plot(future_times, predicted_future_prices, label='Predicted Future Close Prices', marker='x', linestyle='--', color='red')\n",
    "\n",
    "# Decorations\n",
    "plt.title(f\"Prediction of Next {PREDICT_HORIZON} Future Candles\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "d4be8ba9ccc67a1f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Scale the input using the same scaler from training\u001B[39;00m\n\u001B[32m     27\u001B[39m input_features = input_candles[feature_cols]\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m input_scaled = \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m input_scaled = np.expand_dims(input_scaled, axis=\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# (1, 30, 4)\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Predict future prices\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/tensorflow-course/start-tensorflow/envs/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/tensorflow-course/start-tensorflow/envs/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1045\u001B[39m, in \u001B[36mStandardScaler.transform\u001B[39m\u001B[34m(self, X, copy)\u001B[39m\n\u001B[32m   1042\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m   1044\u001B[39m copy = copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.copy\n\u001B[32m-> \u001B[39m\u001B[32m1045\u001B[39m X = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1047\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1048\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1049\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sparse.issparse(X):\n\u001B[32m   1056\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.with_mean:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/tensorflow-course/start-tensorflow/envs/lib/python3.11/site-packages/sklearn/base.py:633\u001B[39m, in \u001B[36mBaseEstimator._validate_data\u001B[39m\u001B[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[39m\n\u001B[32m    631\u001B[39m         out = X, y\n\u001B[32m    632\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m--> \u001B[39m\u001B[32m633\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m    635\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repositories/machine-learning/tensorflow-course/start-tensorflow/envs/lib/python3.11/site-packages/sklearn/utils/validation.py:1087\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1085\u001B[39m     n_samples = _num_samples(array)\n\u001B[32m   1086\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_samples < ensure_min_samples:\n\u001B[32m-> \u001B[39m\u001B[32m1087\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1088\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m) while a\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1089\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1090\u001B[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001B[32m   1091\u001B[39m         )\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_features > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m array.ndim == \u001B[32m2\u001B[39m:\n\u001B[32m   1094\u001B[39m     n_features = array.shape[\u001B[32m1\u001B[39m]\n",
      "\u001B[31mValueError\u001B[39m: Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
